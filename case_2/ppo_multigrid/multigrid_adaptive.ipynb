{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_2_multigrid_adaptive'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 1e-4,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.15,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[70,70,50], log_std_init=-1.7),\n",
    "                verbose = 1,\n",
    "                target_kl =0.1,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/coarse_grid_functions.py:51: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1mFirst-class function type feature is experimental\u001b[0m\u001b[0m\n",
      "  for j in range(len(p_1)-1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f23d0a93dd8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f23d0a93320>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.679    |\n",
      "| time/              |          |\n",
      "|    fps             | 137      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.679     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 731       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0707188 |\n",
      "|    clip_fraction        | 0.445     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 5.85      |\n",
      "|    explained_variance   | -0.64     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0213   |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -0.0366   |\n",
      "|    std                  | 0.183     |\n",
      "|    value_loss           | 0.0156    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 748        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03175534 |\n",
      "|    clip_fraction        | 0.444      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.83       |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.029     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00377    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035791736 |\n",
      "|    clip_fraction        | 0.456       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.87        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0215      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 740         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031586558 |\n",
      "|    clip_fraction        | 0.471       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.91        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00767    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.704       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 763         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038644783 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0755     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00233     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.709       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 782         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042874157 |\n",
      "|    clip_fraction        | 0.494       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.96        |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0524     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00205     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.715       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 785         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042074747 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.01        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0504     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00189     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.712       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046469264 |\n",
      "|    clip_fraction        | 0.488       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.06        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0448     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00191     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.717    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 774      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.049085 |\n",
      "|    clip_fraction        | 0.493    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 6.11     |\n",
      "|    explained_variance   | 0.961    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0788  |\n",
      "|    n_updates            | 180      |\n",
      "|    policy_gradient_loss | -0.0467  |\n",
      "|    std                  | 0.181    |\n",
      "|    value_loss           | 0.00163  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.72       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 782        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04640566 |\n",
      "|    clip_fraction        | 0.504      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.15       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0777    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00173    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.718      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04995772 |\n",
      "|    clip_fraction        | 0.487      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.18       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0723    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00166    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.721       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055400204 |\n",
      "|    clip_fraction        | 0.512       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.23        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0479     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00158     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.728      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 746        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05184882 |\n",
      "|    clip_fraction        | 0.496      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.27       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0524    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0431    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00167    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.732      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 771        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05387343 |\n",
      "|    clip_fraction        | 0.515      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.31       |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0763    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0442    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00167    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.738      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 745        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05276501 |\n",
      "|    clip_fraction        | 0.507      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.36       |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0248    |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00155    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.738       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 757         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052379556 |\n",
      "|    clip_fraction        | 0.511       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.38        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0798     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00153     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.745      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05409802 |\n",
      "|    clip_fraction        | 0.51       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.46       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0583    |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00146    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.746       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 743         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054557253 |\n",
      "|    clip_fraction        | 0.514       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.53        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0715     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00157     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.747      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04908336 |\n",
      "|    clip_fraction        | 0.52       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.58       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0321    |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0428    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00149    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.742       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 777         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050528366 |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.64        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00145     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.748      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 791        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06235838 |\n",
      "|    clip_fraction        | 0.542      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.6        |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0886    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0444    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.748      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 768        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06051864 |\n",
      "|    clip_fraction        | 0.547      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.6        |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0171    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00149    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.76       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 789        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05880314 |\n",
      "|    clip_fraction        | 0.536      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.6        |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0926    |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0422    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.766      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04857253 |\n",
      "|    clip_fraction        | 0.532      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.64       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0724    |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0436    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.0015     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.771      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 765        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07528596 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.65       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0489    |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.771       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 762         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061827272 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.64        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0699     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00144     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.773      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 755        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06836298 |\n",
      "|    clip_fraction        | 0.545      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.65       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0383    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0394    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00146    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.775      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05624243 |\n",
      "|    clip_fraction        | 0.534      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.69       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0764    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0359    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.773       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 799         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056877136 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.74        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0635     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.776       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 765         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061973475 |\n",
      "|    clip_fraction        | 0.565       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.76        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0565     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07079506 |\n",
      "|    clip_fraction        | 0.573      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.84       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0771    |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.78     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 760      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.064237 |\n",
      "|    clip_fraction        | 0.559    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 6.92     |\n",
      "|    explained_variance   | 0.971    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0718  |\n",
      "|    n_updates            | 640      |\n",
      "|    policy_gradient_loss | -0.0414  |\n",
      "|    std                  | 0.174    |\n",
      "|    value_loss           | 0.00137  |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.777       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 778         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065170564 |\n",
      "|    clip_fraction        | 0.575       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.96        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.065      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00138     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.778       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060446948 |\n",
      "|    clip_fraction        | 0.567       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.98        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0461     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.781      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06369358 |\n",
      "|    clip_fraction        | 0.563      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.05       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.014     |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.783      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 766        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06640868 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.14       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0575    |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0395    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.783       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 801         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065608464 |\n",
      "|    clip_fraction        | 0.579       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.22        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.038      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 0.00133     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.783      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07560213 |\n",
      "|    clip_fraction        | 0.575      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.27       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0451    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0384    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.786       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 781         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069437705 |\n",
      "|    clip_fraction        | 0.578       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.35        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00325    |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00133     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 758        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07151332 |\n",
      "|    clip_fraction        | 0.571      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.41       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0336    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0347    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 781        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06294056 |\n",
      "|    clip_fraction        | 0.579      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.45       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0117    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0348    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.791       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 783         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071153745 |\n",
      "|    clip_fraction        | 0.568       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.5         |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.794      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 749        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07178465 |\n",
      "|    clip_fraction        | 0.579      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.59       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.066     |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.796      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 795        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06554037 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.66       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0734    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0335    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 792        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07847029 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.78       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0585    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0362    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.796      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 763        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06510444 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.86       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0454    |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0356    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07782781 |\n",
      "|    clip_fraction        | 0.59       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.91       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0488    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0381    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00132    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.798       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 770         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061063003 |\n",
      "|    clip_fraction        | 0.576       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.99        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0656     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 0.00131     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.798      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07259835 |\n",
      "|    clip_fraction        | 0.587      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.09       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0453    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0353    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.799      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 804        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07358284 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.25       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0552    |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0332    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.801       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064116515 |\n",
      "|    clip_fraction        | 0.586       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0407      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.803      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 790        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07665865 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.46       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0403    |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0364    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.804      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 789        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08085267 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.54       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0338    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.804      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07606259 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.63       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0576    |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0311    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.806       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 759         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072046354 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.74        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00567     |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.00108     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 749        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08402011 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.83       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0942    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0346    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.808       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 786         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068703726 |\n",
      "|    clip_fraction        | 0.597       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.94        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0173     |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.159       |\n",
      "|    value_loss           | 0.00107     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 768        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08006798 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.04       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.000989   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07962311 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.13       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000755   |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0317    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000951   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.811       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 805         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064977705 |\n",
      "|    clip_fraction        | 0.596       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.21        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0745     |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 0.000974    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 792        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07093988 |\n",
      "|    clip_fraction        | 0.592      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.28       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0852    |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0271    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.000937   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.812       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 770         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084386334 |\n",
      "|    clip_fraction        | 0.605       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.37        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0512     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 0.000869    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 800        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07447239 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.44       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00909    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0274    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000836   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08865658 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.5        |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0423    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0274    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000826   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 796        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07281404 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.56       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0728    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000867   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.815       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 781         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083601676 |\n",
      "|    clip_fraction        | 0.592       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.67        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00136    |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.000843    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.816       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 760         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072260655 |\n",
      "|    clip_fraction        | 0.601       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.72        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0823     |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.000846    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 783        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09999265 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.83       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0186     |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000853   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 758        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08094476 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.92       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0468    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000868   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.818     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 771       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0800878 |\n",
      "|    clip_fraction        | 0.615     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.99      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0538   |\n",
      "|    n_updates            | 1400      |\n",
      "|    policy_gradient_loss | -0.0258   |\n",
      "|    std                  | 0.152     |\n",
      "|    value_loss           | 0.000854  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 798         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086556196 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0212     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    std                  | 0.152       |\n",
      "|    value_loss           | 0.000774    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 750        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08638317 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0756    |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000837   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 798         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091239884 |\n",
      "|    clip_fraction        | 0.613       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000851    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.82      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0784907 |\n",
      "|    clip_fraction        | 0.604     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.2      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0808   |\n",
      "|    n_updates            | 1480      |\n",
      "|    policy_gradient_loss | -0.0264   |\n",
      "|    std                  | 0.15      |\n",
      "|    value_loss           | 0.00081   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 778         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081656635 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0383     |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000784    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 789        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07281746 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.092     |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0242    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000778   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07756476 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000834   |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000861   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 785        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07710326 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000222   |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000777   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.822       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 793         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082571015 |\n",
      "|    clip_fraction        | 0.6         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.6        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0713     |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000768    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09877135 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0825    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.024     |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000732   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06918831 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0685    |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000692   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 765         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090791225 |\n",
      "|    clip_fraction        | 0.618       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0536     |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.000722    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08319668 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0869     |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000754   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "seed 1: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f23d0678fd0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f23d0aaad30>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.842     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 149       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 17        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0894752 |\n",
      "|    clip_fraction        | 0.601     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11        |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00128  |\n",
      "|    n_updates            | 1680      |\n",
      "|    policy_gradient_loss | -0.0211   |\n",
      "|    std                  | 0.146     |\n",
      "|    value_loss           | 0.000713  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.841    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 600      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.120362 |\n",
      "|    clip_fraction        | 0.623    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 11       |\n",
      "|    explained_variance   | 0.971    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.0156   |\n",
      "|    n_updates            | 1700     |\n",
      "|    policy_gradient_loss | -0.0207  |\n",
      "|    std                  | 0.146    |\n",
      "|    value_loss           | 0.00115  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07896442 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0591    |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 618        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07789459 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.039     |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 617        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08911327 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0674    |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 616         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090339765 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.2        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.000598    |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07178559 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0152    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 616        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09420937 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.104      |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08148356 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0408    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 620        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07602552 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00489    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 640        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08014289 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000204  |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08000388 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.034     |\n",
      "|    n_updates            | 1900       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.841       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 621         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.067546204 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.5        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0684     |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.00123     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.841     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 633       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0885105 |\n",
      "|    clip_fraction        | 0.633     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.6      |\n",
      "|    explained_variance   | 0.979     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0198    |\n",
      "|    n_updates            | 1940      |\n",
      "|    policy_gradient_loss | -0.0251   |\n",
      "|    std                  | 0.142     |\n",
      "|    value_loss           | 0.00123   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 612        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10220375 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0329     |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0237    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 611        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08320764 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.03      |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0195    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09917094 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0733    |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 618        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10466786 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0403    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 612        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09037805 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0649    |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0216    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108941935 |\n",
      "|    clip_fraction        | 0.627       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12          |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0534     |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.00114     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11103816 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0296     |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 623        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10244737 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.027     |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09818645 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0434    |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 607        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10027685 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.026     |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0207    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07722817 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0141     |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 599        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10249814 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0422    |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "seed 1: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 31 x 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f23d0a7c1d0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f23d0796630>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 118        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09950431 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0264    |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117442206 |\n",
      "|    clip_fraction        | 0.629       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0303     |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00149     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09303221 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0697    |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0225    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10065824 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0463    |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0207    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10225745 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0732    |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00136    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11497315 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0555    |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00143    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 351         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093061306 |\n",
      "|    clip_fraction        | 0.635       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00996    |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00141     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10095106 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0515    |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.0014     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088516854 |\n",
      "|    clip_fraction        | 0.634       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0553     |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098430574 |\n",
      "|    clip_fraction        | 0.645       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00797     |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00133     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11015715 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0276    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09398239 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0632    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095093735 |\n",
      "|    clip_fraction        | 0.637       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0276     |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10491518 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.049     |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00136    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10674901 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0351    |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09430946 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0764     |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10548501 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0158    |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09562679 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0606    |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08588356 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00591    |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08611304 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0736     |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 354         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104977034 |\n",
      "|    clip_fraction        | 0.621       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00692    |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.00112     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090276256 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0539     |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09937219 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0542    |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 350       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1138612 |\n",
      "|    clip_fraction        | 0.631     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.2      |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.08     |\n",
      "|    n_updates            | 2660      |\n",
      "|    policy_gradient_loss | -0.0176   |\n",
      "|    std                  | 0.131     |\n",
      "|    value_loss           | 0.00113   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09213556 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0384     |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10847256 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0295    |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f23d06939b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f23d0aaa6d8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.671      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 134        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 19         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12149398 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00203   |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 711         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052540995 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.9         |\n",
      "|    explained_variance   | -0.0716     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0194     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0117      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.693      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 662        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04818082 |\n",
      "|    clip_fraction        | 0.445      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.91       |\n",
      "|    explained_variance   | 0.832      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0226    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0411    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00364    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 700        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03540184 |\n",
      "|    clip_fraction        | 0.463      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.93       |\n",
      "|    explained_variance   | 0.924      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0323    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0453    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.0029     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 706         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039474703 |\n",
      "|    clip_fraction        | 0.453       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0444     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.0028      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 697        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03461511 |\n",
      "|    clip_fraction        | 0.461      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.97       |\n",
      "|    explained_variance   | 0.939      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0736    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00233    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.712     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 719       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0384032 |\n",
      "|    clip_fraction        | 0.47      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6         |\n",
      "|    explained_variance   | 0.949     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0586   |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | -0.0456   |\n",
      "|    std                  | 0.182     |\n",
      "|    value_loss           | 0.00203   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.713       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 722         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042181153 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.02        |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0214     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00183     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.723      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 720        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04004362 |\n",
      "|    clip_fraction        | 0.484      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.07       |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0714    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0459    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00179    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.725       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 703         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049447075 |\n",
      "|    clip_fraction        | 0.493       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.12        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00648     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00165     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 666         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031107241 |\n",
      "|    clip_fraction        | 0.498       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.16        |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0419     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00157     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.728       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 715         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046225674 |\n",
      "|    clip_fraction        | 0.486       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.21        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0337     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00151     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.731       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 702         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053673506 |\n",
      "|    clip_fraction        | 0.486       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.24        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0741     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00146     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.731       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 732         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036493424 |\n",
      "|    clip_fraction        | 0.488       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.27        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0895     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0436     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00158     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.732       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 694         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057083078 |\n",
      "|    clip_fraction        | 0.51        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.25        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0255      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00148     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 709         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046996646 |\n",
      "|    clip_fraction        | 0.501       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.26        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0409     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.0014      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.731       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 701         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056324076 |\n",
      "|    clip_fraction        | 0.51        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.3         |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0171     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00142     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 718        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05161085 |\n",
      "|    clip_fraction        | 0.517      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.34       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0528    |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0429    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00141    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 728         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046237253 |\n",
      "|    clip_fraction        | 0.516       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.37        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.043      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0425     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00143     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.729       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 753         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053354573 |\n",
      "|    clip_fraction        | 0.534       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.41        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0609     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.736       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 710         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049200125 |\n",
      "|    clip_fraction        | 0.533       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.43        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.737       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 690         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053538214 |\n",
      "|    clip_fraction        | 0.521       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.47        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0774     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0448     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.737      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 702        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05754757 |\n",
      "|    clip_fraction        | 0.537      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.51       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0498    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0443    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.744       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 697         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060478866 |\n",
      "|    clip_fraction        | 0.528       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.57        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0425     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00138     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.742       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 690         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051982533 |\n",
      "|    clip_fraction        | 0.533       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.6         |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0598     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00136     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.747       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 719         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052179508 |\n",
      "|    clip_fraction        | 0.526       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.6         |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00503     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00138     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.744      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 698        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06456201 |\n",
      "|    clip_fraction        | 0.542      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.65       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0679    |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.745       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 730         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061597742 |\n",
      "|    clip_fraction        | 0.55        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.73        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0711     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00153     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.748       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 723         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044842355 |\n",
      "|    clip_fraction        | 0.563       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.79        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0281      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0467     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.754      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 721        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07242744 |\n",
      "|    clip_fraction        | 0.574      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.86       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0254    |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0462    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00129    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.757       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 707         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054085374 |\n",
      "|    clip_fraction        | 0.538       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.88        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0418     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0394     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00131     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.758       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 704         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052268445 |\n",
      "|    clip_fraction        | 0.548       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.93        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0633     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.761      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 698        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06873323 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.95       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0857    |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.761      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 702        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06926413 |\n",
      "|    clip_fraction        | 0.556      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.92       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0969    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0424    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.763       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 704         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.073275015 |\n",
      "|    clip_fraction        | 0.576       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.93        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0684     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00118     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.769       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 754         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068933085 |\n",
      "|    clip_fraction        | 0.572       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7           |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0548     |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 708         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058051504 |\n",
      "|    clip_fraction        | 0.574       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.03        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0559     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.774      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 714        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06999611 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.05       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0491    |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0376    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.775       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 728         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075781025 |\n",
      "|    clip_fraction        | 0.576       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.12        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0757     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    std                  | 0.173       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.778       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 735         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057983406 |\n",
      "|    clip_fraction        | 0.583       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.19        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.783      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 710        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06620696 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.29       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0788    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.041     |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00136    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.786     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 689       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0702068 |\n",
      "|    clip_fraction        | 0.586     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.36      |\n",
      "|    explained_variance   | 0.97      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0876   |\n",
      "|    n_updates            | 820       |\n",
      "|    policy_gradient_loss | -0.0397   |\n",
      "|    std                  | 0.171     |\n",
      "|    value_loss           | 0.00134   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.786      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 725        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05944202 |\n",
      "|    clip_fraction        | 0.587      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.42       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0257    |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0359    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 704        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08025626 |\n",
      "|    clip_fraction        | 0.583      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.48       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0758    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0349    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 684        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06441654 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.57       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00604    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0367    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 702        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08018204 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.61       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0576    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.039     |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.793      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 718        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08121494 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.69       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0252    |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0354    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.794      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 750        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07474008 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.82       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0551    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0337    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.795      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 722        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06673454 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.96       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0222    |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0358    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.795      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 701        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07334892 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.07       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0329    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0329    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 735        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07458758 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.11       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0376     |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.000942   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 696        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07457799 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.18       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0615     |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.000978   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.799      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 713        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06751354 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.27       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0489    |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0307    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.000969   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.801      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 710        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05998656 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.41       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0731    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.000911   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.804      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 709        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08040826 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.5        |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0156     |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0289    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.000862   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.806      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 722        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07572769 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.58       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0308    |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.000913   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.808      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 733        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06735216 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.71       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0158    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.000928   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 733        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07389768 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.84       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0809    |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.000848   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.81      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 717       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0721579 |\n",
      "|    clip_fraction        | 0.589     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.96      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0457   |\n",
      "|    n_updates            | 1160      |\n",
      "|    policy_gradient_loss | -0.0276   |\n",
      "|    std                  | 0.159     |\n",
      "|    value_loss           | 0.000837  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 722        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06479712 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.07       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.122      |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000917   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 732        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07912575 |\n",
      "|    clip_fraction        | 0.592      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.15       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0269    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000842   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.812       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 732         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070950285 |\n",
      "|    clip_fraction        | 0.599       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.19        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0749     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 0.00094     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.813      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 724        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07366645 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.29       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0218    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000865   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 708        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07480247 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.39       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0503    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00082    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 718        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07723531 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.43       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0435    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00075    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 734        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07527916 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.51       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0193    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000776   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 724        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06936828 |\n",
      "|    clip_fraction        | 0.587      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.53       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0324    |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.0008     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.816      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 745        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07632315 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.6        |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0667     |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000764   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.816      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 737        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07640413 |\n",
      "|    clip_fraction        | 0.593      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.69       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0344     |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000708   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 707        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05611039 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.74       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0101     |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000681   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 748        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07033519 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.77       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0648    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000646   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 693        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07102599 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.8        |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0301     |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00067    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 704        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06902157 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.88       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.03      |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00067    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 731        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09122075 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.96       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0275    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000606   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.822     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 717       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0759146 |\n",
      "|    clip_fraction        | 0.598     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10        |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00482  |\n",
      "|    n_updates            | 1480      |\n",
      "|    policy_gradient_loss | -0.019    |\n",
      "|    std                  | 0.151     |\n",
      "|    value_loss           | 0.000664  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.822       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 722         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064518355 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000638    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 769        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07935315 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0234    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000594   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 710        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07234187 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0789    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0227    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000665   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 710         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064042054 |\n",
      "|    clip_fraction        | 0.61        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.4        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 0.00059     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 716        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08750352 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0109     |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000616   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 724        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06739133 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0816    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000564   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.829     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 721       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0730402 |\n",
      "|    clip_fraction        | 0.609     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.6      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0135    |\n",
      "|    n_updates            | 1620      |\n",
      "|    policy_gradient_loss | -0.0173   |\n",
      "|    std                  | 0.147     |\n",
      "|    value_loss           | 0.000601  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 744         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087128915 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0367     |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    std                  | 0.147       |\n",
      "|    value_loss           | 0.000584    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 710        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06924448 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00378   |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000603   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 724        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07408793 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0387    |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000519   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 733         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061184037 |\n",
      "|    clip_fraction        | 0.605       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11          |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0187     |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 0.000534    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 707        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10523143 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.064     |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.00051    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 715        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07882668 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00719    |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000586   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 688        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07672254 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00139    |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000504   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 742         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.096666336 |\n",
      "|    clip_fraction        | 0.621       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.3        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0276     |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000503    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 730        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09564488 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00868    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00048    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 729        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09048167 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0556    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000388   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.836       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 726         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080318056 |\n",
      "|    clip_fraction        | 0.611       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.00881    |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.000423    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 696        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10271826 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00041    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 726        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08446936 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.057     |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000416   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.837     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 702       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0993552 |\n",
      "|    clip_fraction        | 0.612     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.8      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0562   |\n",
      "|    n_updates            | 1900      |\n",
      "|    policy_gradient_loss | -0.0136   |\n",
      "|    std                  | 0.139     |\n",
      "|    value_loss           | 0.000437  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 747         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083745345 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0253     |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000375    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 713        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08242915 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.033     |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000383   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "seed 2: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f23d0f0feb8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f23d0802198>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093663886 |\n",
      "|    clip_fraction        | 0.62        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.1        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00453    |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000341    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 551        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15305337 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00719   |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000673   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 594        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08737948 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00576   |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000656   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 579        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07283892 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0247    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000581   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 585        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07360482 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0431    |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000609   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 569        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09128594 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0122     |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000624   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 583        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07690664 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0238     |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000605   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 566        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10893071 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0317    |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000526   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 591        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09447921 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0761    |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000536   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 590        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07501002 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0368    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.00771   |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000511   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.852     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 583       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0704095 |\n",
      "|    clip_fraction        | 0.621     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.7      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.021     |\n",
      "|    n_updates            | 2160      |\n",
      "|    policy_gradient_loss | -0.0107   |\n",
      "|    std                  | 0.134     |\n",
      "|    value_loss           | 0.000544  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 598         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074866906 |\n",
      "|    clip_fraction        | 0.623       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.011      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.000567    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 584         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079780504 |\n",
      "|    clip_fraction        | 0.629       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0279     |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000549    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 595        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09337392 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0261    |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000513   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 588        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08709037 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0456     |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000477   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 599        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10279093 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0484     |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000479   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 593         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080517806 |\n",
      "|    clip_fraction        | 0.628       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00387     |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.000515    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 591         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.099334955 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0511     |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.00985    |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 0.000461    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 576        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10053706 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00771    |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000473   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 585        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10306032 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00233   |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000524   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 596       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0954187 |\n",
      "|    clip_fraction        | 0.626     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.3      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0574   |\n",
      "|    n_updates            | 2360      |\n",
      "|    policy_gradient_loss | -0.00942  |\n",
      "|    std                  | 0.13      |\n",
      "|    value_loss           | 0.000504  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 590        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07183729 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0207    |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000459   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 576        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09764932 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0525    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.00958   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000485   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 571        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09361023 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0648    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000542   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 578        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09689679 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0184     |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.00893   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000458   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 568        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08206781 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0193     |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000437   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "seed 2: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 31 x 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f23d0aaa5c0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f23c44ac208>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 117        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10284491 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.037      |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.00607   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000458   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 328        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15138367 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0186    |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000704   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08327727 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0131    |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000703   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.859       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106857136 |\n",
      "|    clip_fraction        | 0.646       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00511     |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000646    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.859     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 345       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0836695 |\n",
      "|    clip_fraction        | 0.63      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.8      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0284   |\n",
      "|    n_updates            | 2560      |\n",
      "|    policy_gradient_loss | -0.00699  |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000636  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08710202 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00831   |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000643   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.859       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110716775 |\n",
      "|    clip_fraction        | 0.636       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.9        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0174     |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000658    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 339         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089687206 |\n",
      "|    clip_fraction        | 0.642       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14          |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.03       |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.00987    |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000698    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 339         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.118440226 |\n",
      "|    clip_fraction        | 0.645       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14          |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0459     |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000639    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08446461 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0142     |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.00227   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000681   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 337        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10644545 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0491    |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000662   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.115637936 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.000515   |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.000565    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09964489 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0114    |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000626   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11020877 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0394     |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000591   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10684119 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000117   |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.00981   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000588   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10019015 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0311    |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000645   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10622301 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0429    |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.00888   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000604   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 344         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088519245 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.4        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0181     |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    std                  | 0.124       |\n",
      "|    value_loss           | 0.000595    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10073544 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0153    |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.00986   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000619   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 336        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11699257 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0247     |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0099    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000608   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.86      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 341       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1132489 |\n",
      "|    clip_fraction        | 0.644     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.5      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.047     |\n",
      "|    n_updates            | 2880      |\n",
      "|    policy_gradient_loss | -0.00852  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000662  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112450644 |\n",
      "|    clip_fraction        | 0.63        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.017      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.00896    |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000604    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10844269 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0141     |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.00849   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000603   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10227449 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0171    |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000627   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117998004 |\n",
      "|    clip_fraction        | 0.636       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.015       |\n",
      "|    n_updates            | 2960        |\n",
      "|    policy_gradient_loss | -0.0078     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000622    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11011499 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0425    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0094    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000586   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlgAAAH0CAYAAADhUFPUAAAAAXNSR0IArs4c6QAAIABJREFUeF7snQd4VkXa/u80CAESSui9d5EiVRAUUEEFVtfCf0EUUWyLDcsqCEpR+BZYUfpHEUEQV1EBFQTpAtKkgxQpIZRQQgkkpPyvGb43hLT3vGdOz32ui2vdZM7M8/zumXnvzJl3TlBaWloaeJEACZAACZAACZAACRhGIIgGyzCWrIgESIAESIAESIAEJAEaLHYEEiABEiABEiABEjCYAA2WwUBZHQmQAAmQAAmQAAnQYLEPkAAJkAAJkAAJkIDBBGiwDAbK6kiABEiABEiABEiABot9gARIgARIgARIgAQMJkCDZTBQVkcCJEACJEACJEACNFjsAyRAAiRAAiRAAiRgMAEaLIOBsjoSIAESIAESIAESoMFiHyABEiABEiABEiABgwnQYBkMlNWRAAmQAAmQAAmQAA0W+wAJkAAJkAAJkAAJGEyABstgoKyOBEiABEiABEiABGiw2AdIgARIgARIgARIwGACNFgGA2V1JEACJEACJEACJECDxT5AAiRAAiRAAiRAAgYToMEyGCirIwESIAESIAESIAEaLPYBEiABEiABEiABEjCYAA2WwUBZHQmQAAmQAAmQAAnQYLEPkAAJkAAJkAAJkIDBBGiwDAbK6kiABEiABEiABEiABot9gARIgARIgARIgAQMJkCDZTBQVkcCJEACJEACJEACNFjsAyRAAiRAAiRAAiRgMAEaLIOBsjoSIAESIAESIAESoMFiHyABEiABEiABEiABgwnQYBkMlNWRAAmQAAmQAAmQAA0W+wAJkAAJkAAJkAAJGEyABstgoKyOBEiABEiABEiABGiw2AdIgARIgARIgARIwGACNFgGA2V1JEACJEACJEACJECDxT5AAiRAAiRAAiRAAgYToMEyGCirIwESIAESIAESIAEaLPYBEiABEiABEiABEjCYAA2WwUBZHQmQAAmQAAmQAAnQYLEPkAAJkAAJkAAJkIDBBGiwDAbK6kiABEiABEiABEiABot9gARIgARIgARIgAQMJkCDZTBQVkcCJEACJEACJEACNFjsAyRAAiRAAiRAAiRgMAEaLIOBsjoSIAESIAESIAESoMFiHyABEiABEiABEiABgwnQYBkMlNWRAAmQAAmQAAmQAA0W+wAJkAAJkAAJkAAJGEyABstgoKyOBEiABEiABEiABGiw2AdIgARIgARIgARIwGACNFgGA2V1JEACJEACJEACJECDxT5AAiRAAiRAAiRAAgYToMEyGCirIwESIAESIAESIAEaLPYBEiABEiABEiABEjCYAA2WwUBZHQmQAAmQAAmQAAnQYLEPkAAJkAAJkAAJkIDBBGiwDAbK6kiABEiABEiABEiABot9gARIgARIgARIgAQMJkCDZTBQVkcCJEACJEACJEACNFjsAyRAAiRAAiRAAiRgMAEaLIOBsjoSIAESIAESIAESoMFiHyABEiABEiABEiABgwnQYBkMlNWRAAmQAAmQAAmQAA0W+wAJkAAJkAAJkAAJGEyABstgoKyOBEiABEiABEiABGiw2AdIgARIgARIgARIwGACNFgGA2V1JEACJEACJEACJECDxT5AAiRAAiRAAiRAAgYToMEyGCirIwESIAESIAESIAEaLPYBEiABEiABEiABEjCYAA2WwUBZHQmQAAmQAAmQAAnQYLEPkAAJkAAJkAAJkIDBBGiwDAbK6kiABEiABEiABEiABot9gARIgARIgARIgAQMJkCDZTBQVkcCJEACJEACJEACNFjsAyRAAiRAAiRAAiRgMAEaLIOBsjoSIAESIAESIAESoMFiHyABEiABEiABEiABgwnQYBkMlNWRAAmQAAmQAAmQAA0W+wAJkAAJkAAJkAAJGEyABstgoKyOBEiABEiABEiABGiw2AdIgARIgARIgARIwGACNFgGA2V1JEACJEACJEACJECDxT5AAiRAAiRAAiRAAgYToMEyGCirIwESIAESIAESIAEaLPYBEiABEiABEiABEjCYAA2WwUBZHQmQAAmQAAmQAAnQYLEPkAAJkAAJkAAJkIDBBGiwDAbK6kiABEiABEiABEiABot9gARIgARIgARIgAQMJkCDZTBQVkcCJEACJEACJEACNFjsAyRAAiRAAiRAAiRgMAEaLIOBsjoSIAESIAESIAESoMFiHyABEiABEiABEiABgwnQYBkMlNWRAAmQAAmQAAmQAA0W+wAJkAAJkAAJkAAJGEyABstgoKyOBEiABEiABEiABGiw2AdIgARIgARIgARIwGACNFgGA2V1JEACJEACJEACJECDxT5AAiRAAiRAAiRAAgYToMEyGCirIwESIAESIAESIAEaLPYBEiABEiABEiABEjCYAA2WwUCdVF1qaipOnDiBwoULIygoyEmhMRYSIAESyNME0tLScOnSJZQtWxbBwcF5moVXk6fB8qqyAI4fP44KFSp4OEOmRgIkQALuJnDs2DGUL1/e3Ukw+mwJ0GB5uGPEx8ejSJEiEAM4MjIy4EyvX7+OJUuWoFOnTggLCwv4fiffwNycrE7OsVE36uY0Anr75MWLF+UfwBcuXEBUVJTT0mI8BhCgwTIAolOrEANYDFxhtPQarMWLF6Nz586eNFjMzak9N3eDRd2om5MICIOlp0+qzs9OYsBYsidAg+XhnqE6gPVOHG5AytzcoFLWGKkbdXMaAb19UnV+dhoHxpOVAA2Wh3uF6gDWO3G4ASlzc4NKNFjuVIm6adFNdX7W0gbL2EuABste/qa2rjqAaUJMlce0yqmbaWhNrZi6mYrXtMr16qY6P5uWECs2jAANlmEonVeR6gDWO3E4jwT/onaDJlpiZJ/UQsl5ZahbVk1U52fnqcyIMhOgwfJwn1AdwJwU3dk5qBt1cxoB9kkaLKf1SSviocGygrJNbdBg5QyeE75NnVKxWeqmCNCm26kbDZZNXc/WZmmwbMVvbuM0WDRYXjy/TM9X4s0dacbUThNiDEera9Grm+r8bHWebC9wAjRYgTNzzR2qA1jvxOEGQMzNDSpljZG6UTenEdDbJ1XnZ6dxYDxZCdBgebhXqA5gvROHG5AyNzeoRIPlTpWomxbdVOdnLW2wjL0EaLDs5W9q66oDmCbEVHlMq5y6mYbW1Iqpm6l4NVV+7XoKDpy+jD9PX0Li9VTkCw1G/tAQ+b9RBcJQp0xhFA6/9bVhenVTnZ81JcRCthKgwbIVv7mNqw5gvROHuVkZUztzM4aj1bVQN6uJG9NeILqlpqbhSlIyLicm40qi+N+U//vfZMRfvY6jZxNw/HwCikTkQ8ViESiQLwRXk1JkoMIE5Q8LxuVrybgk/13HxWvJ8ndBQUBkeBhKRuZHcFAQTl28Jv+djBf/myj/++yVpFwTFnXINsNCZB2tqhXHW/fW4KtyjOkmnquFBstzkt5MiAYrZ3EDmfDd1kWYm9sUuxGv13SLT7iO177ahpDgINQvWxgxh/aheu26EF5IGKKr11OQkJSC6ymp0vgUCg/Frph4bDx8DpcSb5giO64iEWGoWaowIsNDkZicKv8lJafi9MVrOBF/7ZaQ7qtXGuMev40Gyw6hXNAmDZYLRNIbIg0WDRa/Rah39Fh/n9cM1oy1hzH4h926QYYGB6Fg/lAUyh+KgvlD5H+Lx3MVihZA+aIRuHA1CcfOJeB6SppcUUpNS5MrXOLRXuFwUfZGeWHcQoKCkPJ/vz99MVGWLRUZjlKR+VFa/G9UOEoVDkfpqHAUjQhDkFiqyuY6cylRPkJMSU2TdRQrmA+1SkbQYOlW2ds30mB5WF8aLBosGiz3DHCvGaznZm3Cz7tO4e7aJRERFoy/jp9ApfJlpVGKyBeK8LAQaYzCQoNw8ap4/JeEKtEF0bJqNGqUKoT8ocE5Gh0nqapXN9X52UkMGEv2BGiwPNwzVAew3onDDUiZmxtUyhojdXOHbmKFp/GHS+WK0rcvtEL9MoV0rfK4IVu9fVJ1fnYDm7weIw2Wh3uA6gDWO3G4ASlzc4NKNFjuVAnYGROPB8atkY/3tg3qiLTUFBqsTGKqzs9u7Rt5KW4aLA+rrTqAaULc2TmoG3Wzm8CUVYcwbPEe+XhwWu87PLeBPyNfveNNdX62W2O2758ADZZ/Rq4toTqA9U4cbgDG3NygElew3KkS8NT0jfh13xm816UOnmlTlQYrGyFV52e39o28FDcNlofVVh3ANCHu7BzUjbrZSUAcu3D7kCW4kpSChS/fifrlomiwaLDs7JK2tU2DZRt68xumwcqZMU2I+f3PjBaomxlUc65TnGwuzoESZ0LldHRBxrvFpvbfDsah3xdb5KGfWwd2RHBwEA0WDZa1HdchrdFgOUQIM8KgwaLB4jENZowsc+o02zympaVJs5TxEqtNh+OuyLOdTl9KxPkrSThyNgH7Tl1CzIWr8oBNcRXMFyLPnionz6Dy/YuQh4iKA0V3x17Eyv1nZF2+SxzCObFnE/l/zc7NHEW01ao3N9X5WVt0LGUnARosjfTHjx+PUaNGITY2FvXq1cPYsWPRpk2bHO8Wv58wYQKOHj2K6OhoPPLIIxgxYgTCw8PlPYMHD8aQIUNuub9UqVI4efJk+s/EhCjKTJ48GefPn0fz5s3x2Wefyfa1XKoDWO/EoSU2u8swN7sV0Nc+dQuM264T8Zi57i9sPx6Po+cS5MnpZl9itatskQL4sFt93FG5GA1WDsBV52ezdWT96gRosDQwnDdvHnr27Alhslq3bo1JkyZh6tSp2L17NypWrJilhtmzZ6NPnz6YNm0aWrVqhf3796N379547LHHMGbMmHSD9fXXX+OXX35Jvz8kJAQlSpRI//8ff/wxhg0bhhkzZqBmzZoYOnQoVq1ahX379qFw4cJ+I1cdwPww84vYkQWomyNl8RtUoLqJP8B2xMRjw6Fz2HkiXq5CXbx2HQmJKfIwT3FQ55+nL/ttN7pQPlQvWQhlowqgaMF8KBMVjtqlI1GpeATEa2PCQoJx4sJVHD/v+yfeBSj+OwGpaZAnn4uVrTY1SqBlteLytTeZr0Bz8xu0gwrozU11fnYQAoaSAwEaLA1dQ6wcNW7cWK5I+a46deqgW7duclUq8/XSSy9hz549WLZsWfqvXn/9dWzcuBGrV69ON1gLFizAtm3bso1ATJ5ly5bFK6+8grfeekuWSUxMhFjlEsbrueee8xu56gDWO3H4DcwBBZibA0TQEUJe1i0xOQWr9sdh38mLOBR3Bb8dPIvYTO/Gy4xUPMLr0qAMujUqi8rFC6JE4fy37KUKDoI8Vd3sKy/rlhNb1fnZbM1YvzoBGiw/DJOSkhAREYH58+eje/fu6aX79+8vzdHKlSuz1DB37lz069cPS5YsQbNmzXDo0CF06dIFTz75JN5+++10gyUeOUZFRSF//vzy8d/w4cNRtWpV+XtxT7Vq1bBlyxY0atQovY2uXbuiSJEimDlzpl/1VQcwJ0W/iB1ZgLo5UpYsQSWnpMo9TL7N49npJjaZi5cfL9tzCt/9cQIXEq7fUk9EvhC0rh6N2ysUQe3SheUKlPjZ5WvJcjVLrESJx3V2X+yTWRVQnZ/t1pTt+ydAg+WH0YkTJ1CuXDmsXbtWPu7zXcIMCZMjHtdld40bNw5i1UqsRCUnJ+P555+Xjxh9148//oiEhAT56O/UqVPy8d/evXuxa9cuFC9eHOvWrZOPI2NiYuRKlu969tlnceTIEfz8889ZmhUrXOKf7xIDuEKFCoiLi0NkZKT/3pCphJgUly5dio4dO8KLm6WZW8BdwvYbvNIntxy9gCembpQbxrs1LIuoiDAcPH0Jf/51DJXKl8PV5LQbm8/PXEnfaC7gi5cTt6paDJWKF0TdMoXlf+cPC7FdF38BeEW37PLUm5uYn8X+3Pj4eF3zsz/m/L39BGiwNBosYXhatmyZXlrsjZo1a5Y0RZmvFStW4PHHH5emSaxMHThwAGLFq2/fvhg4cGC2LV65ckWuWL355pt47bXX0g2WMHhlypRJv0fUcezYMfz0009Z6slu47woNGfOHLkKx4sESMAZBBYdDcaSmGBNwUTlS0PtqDQ0ik5Drag0iMd6vNxPQPyB3aNHDxos90uZYwY0WH7E1fOIUHy7sEWLFvJbh77riy++gFh9unz5MoKDs59YxUpR9erV5V4vPY8IuYKlfaTq/atTewv2lWRu9rHX2vJr87fjh+0n0aH2jS+1iMeElYqG4/Txw6hSrTrC84WhSvEIVCtZEJWKRWg6g0pr23aUY5/MSp0rWHb0RGvbpMHSwFusQjVp0uSWR3x169aF2A+V3SZ3UbZDhw5yM7rv+vLLL/H0009LgyW+LZj5EuZIrGAJEzZo0CD5aFE8Gnz11Vflqpa4hNkrWbIkN7lr0MxfEe4J8UfImb/3im5/G78W4jHhZz0ao8ttN1aovZJbdj2HuWVvsMQeXD4idOZcY0RUNFgaKPqOaZg4caJ8TCjOpZoyZYrcL1WpUiX06tVL7tPymS3xqG706NGynO8RodiDJYyXqEtcb7zxBh588EF5zMPp06fl40SxYX7Hjh2yTnEJgybqnD59OmrUqCE3wYvHjzymQYNofopwwldnaEcNXtHtjmG/4MylRHz/UmvcVr4IDZYdncmgNvX2SW5yN0gAB1dDg6VRHLFBfeTIkfKg0fr168vzrNq2bSvvbteuHSpXrizPqxKX2NTu26MlNqmLs62EmRI/E98AFJfYoyXOtBIb0MXvxSPFDz/8EGJlzHf5DhoV525lPGhUtK/lUh3AeicOLbHZXYa52a2Avva9oJv4ZmDtgTf2UIpXyYhv/onLC7nlpCpzy0pGdX7WN4J4l5UEaLCspG1xW6oDmJOixYIZ1Bx1MwikSdUcOH0JHUavQuH8odg+uFOuxzSYFILl1bJP0mBZ3ukc0CANlgNEMCsEGqycyXLCN6vXmVuvF3RbvvcUnp6xCXXKROLH/jdft+WF3LiClfUU+5yYqM7P5o401m4EARosIyg6tA7VAcwJ36HC+gmLujlbN/FuwPe/34VOdUthcq+m6cFSN2frZrR5VJ2f3Ukrb0VNg+VhvVUHMCd8d3YO6uZs3YYu3I2paw6jz51VMPCBm3suqZuzdaPBcqc+dkZNg2UnfZPbpsHiI0IvnsC/ePFidO7c2bVvF3hu1ib8vOsUBj9YF71bV+EKlsnzoNnV6zXGqvOz2XmxfnUCNFjqDB1bg+oA1jtxOBZIhsCYmxtUyhqjF3Tr/J/V2B17EdN6N8XdtUvRYLmzKyrrpjo/uxxbngifBsvDMqsOYC98mBm9rO+G7kLdnKuSOHrltsFLcCkxGUtfbYsapQorf1A7N9ubkbFPZlVJdX52g+55PUYaLA/3ANUBzEnRnZ2DujlXtwsJSbj9g6UywD0f3IcC+W6+1YG6OVe33CLTq5vq/OxOWnkrahosD+utOoD1ThxuQMrc3KCS9x4R7jgejwc/XYPoQvmx6b0OtyTIPpm3+qTq/OxOWnkrahosD+utOoA54buzc1A35+q2eEcsXpi9BY0qFsG3L7SmwXKuVJoj0zveVOdnzQGyoG0EaLBsQ29+w6oDWO/EYX5m6i0wN3WGdtTgZt0SkpIxceUhfLLsTzzUsCw+eaIRDZYdncjgNvX2SdX52eA0WJ0JBGiwTIDqlCpVB7DeicMp+ecWB3Nzg0rufkR49nIith+Px+o/4/DrvtM4HHclPaEX21fDgHtr02C5sxsaopvq/OwBdJ5PgQbLwxKrDmCaEHd2Dupmn25Jyan4dutxTFl9GAdOX84SSL6QYJQrWgCf9miEemWjDPmgti9b7S2zT2ZlpTo/a6fPknYRoMGyi7wF7aoOYE6KFohkQhPUzQSofqo8fyUJX206hs9/O4KYC1fTS1ctURBNKhbF3bVLolW1aEQWCE1/uXPmKqmb9boZ0aJe3VTnZyNiZx3mEqDBMpevrbWrDmC9E4etSWtsnLlpBOWwYk7Sbe/Ji1i8PRabjpyX/8TqlbjENwSfa1sVjzatgKgI7S//dVJuRsvO3LiCZXSfckN9NFhuUElnjDRYOYPjhK+zU9l8mxN02xkTj/8s+xNLd5+6hUa9spHo1bISut5eDuFhN8+30orMCblpjTXQcsyNBivQPuOF8jRYXlAxhxxosGiw+C5C4wb45cRk/HvJPsxc9xdS04CgIKBT3VJoW7ME7qhcDDVKFsrx8Z+WKGhCtFByXhm9uqnOz84jwYgyE6DB8nCfUB3AeicONyBlbm5QKWuMVut2PSUV3287gVV/nsGaP+Nw9kqSDKrLbWXwaoeaqF6ykGEgrc7NsMA1VMTcuIKloZt4rggNluckvZkQDRZXsLiCpX+Ai2MWXpyzBesPnUuvpHzRAhjWvQHuqllCf8U53EkTYjhSSyrUq5vq/GxJcmxEiQANlhI+Z9+sOoD1ThzOpnIjOubmBpXsWcESL2QW51YNXLBLfiOwYL4QPNW6ClpVL46mlYohX2iwKfDYJ03BanqlenVTnZ9NT4wNKBOgwVJG6NwKVAew3onDuURuRsbc3KCStQZLfAvwhz9OYPKqQ9h36pJsvHLxCEzp1RQ1ShU2HRj7pOmITWlAr26q87MpybBSQwnQYBmK01mVqQ5gvROHsyhkHw1zc4NK1hisExeuYsG2GMz67Qhi46/JRgvlD0WP5hXxYvvqiCqg/agFFarskyr07LtXr26q87N9GbNlrQRosLSScmE51QGsd+JwAyrm5gaVzDNY4jHgmgNxmLjyINYdPIu0tBttlSicH0+3riLNlVXGypcl+2Te6pOq87M7aeWtqGmwPKy36gDmhO/OzkHdsuomDNVPO0/im60xEC9dPnMpEftP3XyVTfMqxfBwk/LoentZ5A8N/AwrI3oKdTOCovV16NVNdX62PlO2GCgBGqxAibmovOoA1jtxuAERc3ODSsasYG0/fgHvf78LW49euKXC8LBgPNGsolyxqlAswnYg7JO2S6ArAL26qc7PuoLlTZYSoMGyFLe1jakOYL0Th7VZ6muNuenjZvddgegmVq2+2HAUH/ywC9dT0lAgTHwbsDJqlS4M8dLlO6oUk6+1ccoVSG5OiVlrHMwtKynV+Vkre5azjwANlkb248ePx6hRoxAbG4t69eph7NixaNOmTY53i99PmDABR48eRXR0NB555BGMGDEC4eHh8h7x39988w327t2LAgUKoFWrVvj4449Rq1at9DrbtWuHlStX3tLGY489hrlz52qKWnUAc1LUhNlxhagbsPvERXyy7E/8tOuk1OfeeqXwYdf6KBl5Y/w58aJuTlTFf0x6dVOdn/1HxhJ2E6DB0qDAvHnz0LNnTwiT1bp1a0yaNAlTp07F7t27UbFixSw1zJ49G3369MG0adOkcdq/fz969+4NYY7GjBkjy9933314/PHHcccddyA5ORnvvvsuduzYIessWLCgLCMMVs2aNfHBBx+ktyHMWFRUlIaoAdUBrHfi0BSczYWYm80C6Gzen27CWI36eS9+3XdGthASHIS37quFvm2qKr3GRme4Ad3mL7eAKnNYYeaWVRDV+dlhEjOcbAjQYGnoFs2bN0fjxo3lipTvqlOnDrp16yZXojJfL730Evbs2YNly5al/+r111/Hxo0bsXr16mxbPHPmDEqWLClXrNq2bZtusG6//Xa5WqbnUh3AnBT1ULf/nryo28Vr1zHk+934Zutx+Y3A4CCgc4MyeL5dNdQrq+0PEruVy4u62c3ciPb16qY6PxsRO+swlwANlh++SUlJiIiIwPz589G9e/f00v3798e2bduyPMITBcQjvH79+mHJkiVo1qwZDh06hC5duuDJJ5/E22+/nW2LBw4cQI0aNeQqVv369dMN1q5duyD2kpQqVQr3338/3n//fRQurO3QQ9UBrHfiMLfLGlM7czOGo9W1ZKfbuStJ6DVtA3bGXJThPHBbGbzeqRaqRN9YCXbLxT7pFqVujVOvbqrzsztp5a2oabD86H3ixAmUK1cOa9eulY/7fNfw4cMxc+ZM7Nu3L9saxo0bB7FqJcyReAT4/PPPy0eM2V2iTNeuXXH+/PlbVrimTJmCKlWqoHTp0ti5cyfeeecdVK9eHUuXLs22nsTERIh/vksM4AoVKiAuLg6RkZEB92wxcYi2OnbsCC++0465BdwlbL8hc588fSkRvaZvwsEzV1CsYBgm9GiExhWL2B6nngA43vRQs/8evbqJ+Vnsz42Pj9c1P9ufOSPwR4AGS6PBWrduHVq2bJleetiwYZg1a5bcpJ75WrFihdxfNXToUIjHi2J1Sqx49e3bFwMHDsxS/sUXX8SiRYuwZs0alC9fPseINm/ejKZNm0L8r3hkmfkaPHgwhgwZkuXnc+bMkatwvEjASwSupQCf7AxBTEIQiuRLwwt1U1CqgJcyZC5eJpCQkIAePXrQYHlYZBosP+LqeUQovl3YokUL+a1D3/XFF1/g2WefxeXLlxEcfPNlsS+//DIWLFiAVatWydWq3C6x0pU/f35p7MSG+cwXV7C0j1S9f3Vqb8G+knkht/b33IOX5+3Cyj/jEF0oH756thkqFHX3HxF5QTeuht+cF7iCZd8caVXLNFgaSItVqCZNmtzyiK9u3brysV52m9xF2Q4dOshjF3zXl19+iaeffloarJCQEPnoUJirb7/9FmLFS+y/8neJx4QNGjS4ZSN8bveoPuPXu7fAXx5O+D1zc4IKgcVwOO4KZqw9hBU7jiAB+XHmchLEYaFzn22J2yu487FgRgLsk4H1B6eU1qub6vzslPwZR84EaLA09A7fMQ0TJ06UjwknT54MsT9KbECvVKkSevXqJfdp+cyWeFQ3evRoWc73iFDswRLGS9QlrhdeeAHi0d133313y9lX4ggGcRTDwYMHIY576Ny5s3xOL45vEHu6xO9+//13adL8XaoDWO/E4S8uJ/yeuTlBBW0xxMZfxeDvd2HJ7lPp7wwUd+YGAUclAAAgAElEQVQPDcZ/Hm+E++qX1laRw0uxTzpcoBzC06ub6vzsTlp5K2oaLI16iw3qI0eOlAeNim/5ifOsMh6nULlyZcyYMUPWJja1+/ZoxcTEoESJEnjwwQflz4oUufGXdlBQULYtT58+XZ6ZdezYMfzjH/+Qm9vFqpfYrC6+iSi+RVisWDFNUasOYL0Th6bgbC7E3GwWQEPzYpX3+z9OYOCCnbh4LVne0a5mNCqnncKDd7dCtVKRKBKRT0NN7ijCPukOnTJHqVc31fnZnbTyVtQ0WB7WW3UA65043ICUuTlXpeSUVCzaEYuJKw9hT+yNoxduKx+FUY80RNXi4Vi8eLFc2fXiN1uZm3P7ZU6R6Z1LVOdn95HKexHTYHlYc9UBrHficANS5uY8la4mpeCrTccwZfUhHD9/VQYYkS8Ez7WthhfaV0NYSDCom/N00xIRdctKSXV+1sKdZewlQINlL39TW1cdwJwUTZXHtMrdptuFhCTMXHcEM3/7C+LQUHEVL5gPvVtVRs+WlW55DOi23AIRmbkFQss5ZfXqpjo/O4cAI8mJAA2Wh/uG6gDWO3G4ASlzc4ZKR88m4LHJvyE2/poMqGKxCPRtWxV/b1Ie4WFZv8hB3ZyhW6BRUDeuYAXaZ7xQngbLCyrmkAMNVs7icsK3v+MfP5+AxyatR8yFq6hUPAJvdKqF++uXRmjIzXPiMkdJ3ezXTU8E1I0GS0+/cfs9NFhuVzCX+GmwaLCcuhF827ELeHH2FmmuqkYXxNxnW6BkZLjf0cgPar+IHFmAutFgObJjmhwUDZbJgO2sngaLBsspBuvitetYvuc0rl1PwZFzCZiy6hCSU9NQuXiEPCi0dJR/cyXU5Ae1nTOK/rapGw2W/t7j3jtpsNyrnd/IabBosOwyWOsOxmFv7CVUKBaBmPMJ+GT5gfQN7D5VutxWBsO7NUBURJjfvuwrwA9qzagcVZC60WA5qkNaFAwNlkWg7WiGBosGyyqDJVamxDcAxb9Plv0pT13PfInVquolC0NssercoAwealg2xwN3c1KOH9R2zCTqbVI3Giz1XuS+Gmiw3KeZ5ohpsGiwrDBY322LwTvf7EBCUko68JDgILStEY3TlxKRmJyKJ1tVxuN3VJBnWalc/KBWoWffvdSNBsu+3mdfyzRY9rE3vWUaLBosMw2WeJXN5FWHMOLHvRJ0WEiQPFpBvHj5vS51Uat0YcP7OD+oDUdqSYXUjQbLko7msEZosBwmiJHh0GDRYJllsA6cvoShi/Zgxb4zEvLTravg3S51IFauzLz4QW0mXfPqpm40WOb1LufWTIPlXG2UI6PBosEy2mAdPHMZk1YexH+3xCAlNU2uWr19fx30ubOKcn/VUgE/qLVQcl4Z6kaD5bxeaX5ENFjmM7atBRosGiyjDNaZS4kYtmg3vvvjBNLSbnDtVLcU3ulcB1WiC1rWx/lBbRlqQxuibjRYhnYol1RGg+USofSESYNFg6VisK6npGLfyUtYf+gsxi0/gPir1yXQDnVK4fl21dCkUlE93VLpHn5QK+Gz7WbqRoNlW+ezsWEaLBvhm900DRYNVqAGKyk5Fd9uPY5f9pzG2gNxt3wzsF7ZSIz4WwPcVr6I2V03x/r5QW0beqWGqRsNllIHcunNNFguFU5L2DRYNFiBGCyxp0q8vuanXSfTwUWGh6JhhSJy1er/Na+Y63sCtfRJ1TL8oFYlaM/91I0Gy56eZ2+rNFj28je1dRosGiytBkscufDegp2YveEo8oUE48X21XFPnZKoWyYSwSZ/MzCQQcAP6kBoOacsdaPBck5vtC4SGizrWFveEg0WDZZWgyW+GSjOswoKAj7r0VietO7Eix/UTlTFf0zUjQbLfy/xXgkaLO9pmp4RDRYNlhaD9cexC3h4wjr58uXBD9ZF79bWHLmgZ+jxg1oPNfvvoW40WPb3QusjoMGynrllLdJg0WD5M1hXEpPxwLg1OBx3BeLly58+0Sjg9wNa1qEB8IPaStrGtUXdaLCM603uqYkGyz1aBRwpDRYNVm4GS+y7GvD1dny9+TjKRIXjp/5tERURFnA/s/IGflBbSdu4tqgbDZZxvck9NdFguUergCOlwaLBys1gjfxpL8avOCj3Xc1+pjlaVYsOuI9ZfQM/qK0mbkx71I0Gy5ie5K5aaLDcpVdA0dJg0WDlZLAmrjyIj/7vJc3DuzdAj+YVA+pbdhXmB7Vd5NXapW40WGo9yJ1302C5UzdNUdNg0WBlZ7D2xF5E509Wy1fevHN/bTx3VzVN/ckJhfhB7QQVAo+ButFgBd5r3H8HDZb7NcwxAxosGqzsDNYzM3+XJ7WLTe3iSAY3XfygdpNaN2OlbjRY7uy5alHTYGnkN378eIwaNQqxsbGoV68exo4dizZt2uR4t/j9hAkTcPToUURHR+ORRx7BiBEjEB4enn6PvzoTExPxxhtv4Msvv8TVq1dxzz33QNxTvnx5TVHTYNFgZTZYm4+cl0cyhAQHYemrbVG1RCFNfckphfhB7RQlAouDutFgBdZjvFGaBkuDjvPmzUPPnj2luWndujUmTZqEqVOnYvfu3ahYMeveldmzZ6NPnz6YNm0aWrVqhf3796N379547LHHMGbMGNmiljqff/55/PDDD5gxYwaKFy+O119/HefOncPmzZsREhLiN3IaLBqsjAZLfGvwiSnrsf7QOTzWtAI+fuQ2v33IaQX4Qe00RbTFQ91osLT1FG+VosHSoGfz5s3RuHFjuSLlu+rUqYNu3brJVanM10svvYQ9e/Zg2bJl6b8S5mjjxo1YvXq1/Jm/OuPj41GiRAnMmjVLGjNxnThxAhUqVMDixYtx7733+o2cBosGK6PB+m5bDPrP3SZfhfPrgHYoV6SA3z7ktAL8oHaaItrioW40WNp6irdK0WD50TMpKQkRERGYP38+unfvnl66f//+2LZtG1auXJmlhrlz56Jfv35YsmQJmjVrhkOHDqFLly548skn8fbbb0NLncuXL5ePBMWKVdGiRdPbaNiwoTR2Q4YM8dsTabBosHwGa/+pS+j66VpcvZ6Cl++ujtc71fLbf5xYgB/UTlTFf0zUjQbLfy/xXgkaLD+ailWjcuXKYe3atfJxn+8aPnw4Zs6ciX379mVbw7hx4+QjPfFYJjk5GeJxn3jE6FuJ8lfnnDlz8NRTT0Hsw8p4derUCVWqVJGPKTNfomzG8sJgiRWvuLg4REZGBtx7xaS4dOlSdOzYEf5OBA+4cptvyEu5XbqWjIcnrsfhswloVbUYpj3ZRO7BcuOVl3Rzoz45xUzdsjdYYn+ueFqhZ372Uv/wai40WBoN1rp169CyZcv00sOGDZOP7/bu3ZulhhUrVuDxxx/H0KFD5aPAAwcOQKx49e3bFwMHDpSP+oTByq3OnAyWMDvVqlXDxIkTs7Q7ePDgbFe2RF1iFY5X3iMgjmKYtj8Y288Fo0i+NAy4LQWFnH1Ye94TiRnnSQIJCQno0aMHDZaH1afB8iOulsd5masQ3y5s0aKF/Nah7/riiy/w7LPP4vLly3JFy99jRz2PCLmCpX2k5pW/qGdsOI6RP/+JsJAgzOlzB26vUEQ7JAeWzCu6ccXYgZ0vh5D09knxhIErWO7RWU+kNFgaqIlVqCZNmqQ/4hO31K1bF127ds12k7so26FDB3z88cfptYujFp5++mlpsMQ3AP3V6dvkLozZo48+KusRR0SIIxq4yV2DaH6KeHlPSOz5yxg7fzkKlq6KGb8dQWoaMLRbffyjRSV1cDbX4GXdmJvNnUtn83p1U90jqzNc3mYhARosDbB9RyqIx3LiMeHkyZMxZcoU7Nq1C5UqVUKvXr3kIz/fNwrFo7rRo0fLcr5HhGIPljBeoi5x+atTlBH3LFy4UB7TUKxYMXkm1tmzZ3lMgwbN/BXROyn6q9fu38dcuIpun67BmctJ6aH8rXE5/PvvDREkXjro8suruglZmJs7O6de3Wiw3Kl3IFHTYGmkJTaojxw5Uq4i1a9fX55n1bZtW3l3u3btULlyZWmExCUeAfr2aMXExMjjFh588EH5syJFbj6iya1OUc+1a9cwYMAAiD1UGQ8aFRvXtVyqA1jvxKElNrvLeDG3y4nJeGTCOuw9eQnF86eh8+0VUbdcETzcuDzyhQbbjdyQ9r2omw8MczOki1heiV7dVOdnyxNlgwEToMEKGJl7blAdwHonDjcQ8kpu166nYNH2WOyOvYj1h85i14mLiC6UDy/WTMA/unf25Lc/xSPyzp2ZmxvGGc1jziqpzs9u0j+vxkqD5WHlVQewV0xIdhK7PbfklFR8+fsxfLb8AE5evJaeYv7QYMzucwditq+lCXHZ2HZ7n8wNN3PLSkd1fnZZ986T4dJgeVh21QHMSdGZneNqUgpe/nKLfGGzuEpHhqNzgzKoUqIg2lSPRrmofPKLEFzlcaZ+OUXF8eYuvVRX51TnZ3fSyltR02B5WG/VAcwJ33md4/yVJDw983dsPXoBYrXqnftr44nmFZE/9Oa7Kamb83TTEhF100LJeWX06qY6PzuPBCPKTIAGy8N9QnUA65043IDUjbmJtwI8NeN3rNh3BlEFwvC/TzZF08rFsuB2Y25a+wxz00rKWeWoGx8ROqtHWhMNDZY1nG1phQYrZ+xunPAX74jFC7O3yJc1L3ixNeqWzf71R27MTesAYW5aSTmrHHWjwXJWj7QmGhosazjb0goNlncM1qVr19Fh9EqcupiIf95TA691rJljcvwws2W4KTdK3ZQR2lKBXt1U52dbkmWjARGgwQoIl7sKqw5gvROHGyi5KbdzV5IwcMFOLNoRi0rFI/DzK20RHnZzz1Vm3m7KLdC+wtwCJeaM8tSNK1jO6InWRkGDZS1vS1ujwXL3CtaVxGRMXX0YU1YfgjhEVFyfP90MbWuWyLUf8cPM0mFmWGPUzTCUllakVzfV+dnSJNmYLgI0WLqwueMm1QGsd+JwAx0n5yY2s8/ZeBRjlu5H3P+98qZe2Ui8c38d3Fkj2i9eJ+fmN3g/BZibKkF77qduXMGyp+fZ2yoNlr38TW2dBsudK1jfbj2OV+f9IYOvXDwCr3eqhS4NyiA4WNu7BPlhZuqwMq1y6mYaWlMr1qub6vxsalKs3BACNFiGYHRmJaoDWO/E4Uwat0bl1NzEOVf3jF4Jse+qb5sqePO+2ggLCew9gk7NzYh+wdyMoGh9HdSNK1jW9zr7W6TBsl8D0yKgwXLfCtZbX2/HvE3HULNUISx8uY2ulzTzw8y0IWVqxdTNVLymVa5XN9X52bSEWLFhBGiwDEPpvIpUB7DeicN5JLJG5LTc4q9ex/+uOYxPlv0pg/26X8tsDxHVwtZpuWmJWWsZ5qaVlLPKUTeuYDmrR1oTDQ2WNZxtaYUGyx0rWL/uO41/frkVl67d+KZgr5aV8EHX+rr7DD/MdKOz9UbqZit+3Y3r1U11ftYdMG+0jAANlmWorW9IdQDrnTiszzTwFp2S27XrKbj7f1bgRPw1+Viw/z01cX/90po3tGeXuVNyC1wV/3cwN/+MnFiCunEFy4n90uyYaLDMJmxj/TRYzl/BmrbmMD5YuBtlosLx6xvtcj1AVGtX4oeZVlLOKkfdnKWH1mj06qY6P2uNj+XsI0CDZR9701tWHcB6Jw7TEzOgASfkJg4SbTvyV5y9koQRf2uAJ5pVNCAzwAm5GZJINpUwN7PImlsvdeMKlrk9zJm102A5UxdDoqLBcvYK1qfL/8T/LNkvz7pa+tpdAR/HkFN2/DAzZPhYXgl1sxy5IQ3q1U11fjYkeFZiKoE8Y7BEZ16+fDlq1aqFOnXqmArVKZWrDmC9E4dT8s8tDrtzS0lNQ4sRy3DmUiLGPnY7ujUqZxg2u3MzLBGuYJmJ0tK62Se5gmVph3NIY541WI8++ijatm2Ll156CVevXkXDhg3x119/QbyGZO7cuXj44YcdIoF5YdBgOXcFa93BOPSYsgFRBcLw+7sddJ13xRWsMPMGjw0104TYAN2AJvXqpjo/GxA6qzCZgGcNVunSpfHzzz9LYzVnzhy8//77+OOPPzBz5kxMnjwZW7duNRmt/dWrDmC9E4f9mfuPwO7c3v12B2ZvOIpHm5bHyEca+g84gBJ25xZAqAEXZW4BI3PEDdSNK1iO6IgWB+FZg1WgQAHs378fFSpUQK9evVC2bFl89NFHOHr0KOrWrYvLly9bjNr65miwnLmClZySiubDl8nN7Z8/3Qxta5YwtHPww8xQnJZVRt0sQ21oQ3p1U52fDU2ClZlCwLMGq2bNmhg6dCi6dOmCKlWqyMeCd999t1zFuueeexAXF2cKUCdVqjqA9U4cTmKQUyx25rbmzzj84383oGhEGDa+28Gwze2+XO3MzWztmZvZhM2pn7pxBcucnuXsWj1rsMaPH4/+/fujUKFCqFSpErZs2YLg4GCMGzcO33zzDX799VdnK2NAdDRYzlzBevu/2zH392PyWAZxPIPRFz/MjCZqTX3UzRrORreiVzfV+dnoPFif8QQ8a7AEqk2bNuHYsWPo2LGjNFriWrRoEYoUKYLWrVsbT9NhNaoOYL0Th8MwZBuOXbmJk9vFtwcvJFzHnGeao1X1aMNx2ZWb4YlkUyFzs4Ky8W1QN65gGd+rnF+jpw2WkfjFitioUaMQGxuLevXqYezYsWjTpk22TbRr1w4rV67M8rvOnTtLgyeuoKCgbO8dOXIkBgwYIH9XuXJlHDly5JZyb731ltxLpuWiwcqZkl0T/pwNR/Gvb3egbFQ4Vr3ZHqEhwVqkDKiMXbkFFKTOwsxNJzibb6NuNFg2d0FbmveUwXrttdc0Qxw9erTmsvPmzUPPnj0hTJZY+Zo0aRKmTp2K3bt3o2LFrKdvnzt3DklJSen1nz17Vn6bUdzTu3dv+fOTJ0/e0v6PP/6IPn364MCBA6hatWq6wRI/69u3b3pZsRLnW43zlwANlrMMljj7qsPolTgcdwUDH6iLPndW8Sehrt/zw0wXNttvom62S6ArAL26qc7PuoLlTZYS8JTBat++/S3wNm/ejJSUFHm4qLjEtwpDQkLQpEkTeeio1qt58+Zo3LgxJkyYkH6LOKy0W7duGDFihN9qxGrXoEGD5OpXwYIFsy0v6rp06RKWLVuW/nuxgvXKK6/If3ou1QGsd+LQE6vV99iR2087T6LfF5sRGR6K3965BwXzh5qSth25mZJINpUyN6tIG9sOdcvKU3V+NlYh1mYGAU8ZrIyAxArVihUr5LlXRYsWlb86f/48nnrqKflo7/XXX9fEU6xERUREYP78+ejevXv6PWID/bZt27J9FJi54gYNGqBly5by/K3srlOnTqF8+fIy1h49etxisBITE+VqmDhu4u9//7t8fJgvXz5NsasOYE6KmjBrKiQOuP3bhHXYevQCXmxfDQPura3pPj2FqJseavbfQ93s10BPBHp1U52f9cTKe6wl4FmDVa5cOSxZskTul8p47dy5E506dcKJEyc0kRblRF1r165Fq1at0u8ZPny4NET79u3LtZ6NGzdCrIBt2LABzZo1y7as2Hcl9lWJtsLDw9PLjBkzRq6cCYMo6nnnnXfQtWtX+agxu0uYMfHPd4kBLIyZOJIiMjJSU74ZC4mJY+nSpfJLAmFh3js126rcfv/rPD7+eT/+OB6PsJAgrHy9LUoUzh+wHlpvoG5aSTmrHHVzlh5ao9Grm5ifo6OjER8fr2t+1hofy9lHwLMGq3Dhwvjuu+/k2VcZL/FoUJgU8ThOy+UzWOvWrZOrUL5r2LBhmDVrFvbu3ZtrNc899xzEvTt27MixXO3ataWJEUdI5Hb997//xSOPPCINU/HixbMUHTx4MIYMGZLl5+Ike7EKx8t6ArvPB2HS3hDZcL7gNHSvnIpWpdKsD4QtkgAJOIpAQkKCfGJBg+UoWQwNxrMGS5zeLr7J9+9//xstWrSQ0NavXy8fsYl3FIrVJy2XyiNCMYDKlCmDDz74QJ7Jld21evVqGY943Cg2wud2xcTEyEeJIg+xKpb54gqWFkVvlNH7V6f2Fm6UfHLGJqw7eA4dapfAkIfqoqSJK1e+2KzKLVAWRpRnbkZQtL4O6paVOVewrO+HVrfoWYMlzM0bb7yBadOmyQ9TcYWGhspv6onjFnLabJ6dAMLMiI3x4luEvku8bkeshOW2yX3GjBno168fhDHKbsVJ1CW+VSgeW4ozu/xdCxcuxIMPPiiPbsju24uZ71d9xq93b4G/PJzweytyO3YuAW1G/gpxIseqAe1RoZg1q4hW5GaXhszNLvJq7VK37A1WVFQUV7DUupaj7/aswfJRv3LlCg4ePAixybh69eoBGStfHb5jGiZOnJi+WX3KlCnYtWuXPCVerJaJfVqZzZbYTC9+Ll7Tk90lDJBY4RKrbMKIZbx+++03uVIlvhkpBuHvv/+OV199FU2bNpWPPrVcNFg5U7Jiwv/3kn0Yt/wA2tSIxqw+WVcctWiop4wVuemJy4h7mJsRFK2vg7rRYFnf6+xv0ZMGKzk5WW4WF4/d6tevbwhlsXolNqOLoxZEnWIDuni0Jy5xsKg4UkGsWPkucSSEOB5CbLQX+6uyu8S3CsURDKJOYaIyXuLVPi+88ILc4yUe/Qkj9/jjj+PNN9/UvJ+KBss+gyVe6Nz64+U4dTERn/VojC63lTGkH2qphB9mWig5rwx1c54mWiLSq5vq/KwlNpaxl4AnDZZAWq1aNfnOQX/7muzFb27rqgNY78RhblbG1G52bkt3n0LfzzehWMF8+O2du5E/9MZGdysus3OzIoec2mBudtLX3zZ1y8pOdX7WrwbvtIqAZw3W9OnT5dlVX3zxBYoVK2YVT0e1ozqAOSnqk3PNn3F4YfZmXLyWjGfurIL3HqirryKdd1E3neBsvo262SyAzub16qY6P+sMl7dZSMCzBqtRo0bytTOi84vHa5k3tYtHcF6/VAew3onDDVzNyu27bTF47as/IF6L06RSUUx78g5ERVh7hphZuTlBV+bmBBUCj4G6cQUr8F7j/js8a7CyOw8qo1zvv/+++9XzkwENVs6AzJjwxRcpWn20HLHx19C9UTmM+FsDhIdZ92jQl60ZuTllsDA3pygRWBzUjQYrsB7jjdKeNVjekEctCxosaw3WgdOX5cuc84UGY/v7nWwxVyJjfpipjRu77qZudpFXa1evbqrzs1rUvNsKAjRYVlC2qQ3VAax34rAp3YCaNSO3GWsPY/APu9G6enHMfubG4bZ2XGbkZkce2bXJ3JyiRGBxUDeuYAXWY7xR2rMGKyUlRR6l8NVXX+Ho0aPyhckZr3PnznlDwVyyoMGydgXrmZm/45c9p/HWfbXxfLtqtvUvfpjZhl6pYeqmhM+2m/Xqpjo/25YwG9ZMwLMGa9CgQfKlyK+99hoGDhyId999F3/99RcWLFgA8bt//vOfmiG5taDqANY7cbiBl9G5XU9Jxe1DluBKUgoWvnwn6pe79VwzK5kYnZuVsftri7n5I+TM31M3rmA5s2eaG5VnDZY4B+uTTz5Bly5dIF78LA4d9f1MnJAuXoDs9YsGy7oVrI2Hz+HRSb/Jc682vdsBwcFBtnUvfpjZhl6pYeqmhM+2m/Xqpjo/25YwG9ZMwLMGSxzLsGfPHvnOPvE6mkWLFqFx48Y4dOgQxBEO4g3mXr9UB7DeicMNXI3ObfSSffhk+QE8cFsZfNqjsa0IjM7N1mQyNc7cnKSG9lioG1ewtPcW75T0rMESr6n5/PPPIV7ULN4JKFay3n77bYj3Cr788ss4ffq0d1TMIRMaLOtWsLqPX4utRy/g44cb4LE7Ktrat/hhZit+3Y1TN93obL1Rr26q87OtSbNxTQQ8a7CEmYqMjMS//vUvfP3113jiiSfk+wLFhnfx0uSPPvpIEyA3F1IdwHonDjcwMzK3lfvP4MlpG2Xaa9++G+WKFLAVgZG52ZpINo0zN6cpoi0e6sYVLG09xVulPGuwMsu0YcMGrF27FtWrV8dDDz3kLRW5ghWwnkZN+KcvXUPn/6xG3OUk9GpZCR90Nebl4gEnlOEGo3JTicGse5mbWWTNrZe60WCZ28OcWXueMVjOxG9uVFzBypmvyoR/4sJVvPn1dgQFAWcuJWLvyUuoXbowFrzY2rbDRTNmqpKbuT1SvXbmps7QjhqoGw2WHf3O7jY9a7DKli2Ldu3ayX933XUXxJ6svHbRYJljsN7/bidm/nYkvfLwsGB5NEP1koUd0cX4YeYIGQIOgroFjMwRN+jVTXV+dkTyDCJXAp41WF9++SVWrlyJFStWYP/+/ShVqpQ0Wj7DVadOHc93DdUBrHficANYvbldu56CZsN+wcVryXipfXUIc9WsSnE0q1LMMWnrzc0xCeQSCHNzg0pZY6RuXMFyZ89Vi9qzBisjllOnTuHXX3/FwoUL5bcIU1NTIU569/pFg2X8Cta3W4/j1Xl/yI3sq99sb+t5Vzllxw8zd45s6pa3dFOdn91JK29F7WmDdfnyZaxZsyZ9JWvr1q2oW7euXMkSr9Hx+qU6gDnhZ+0hj036DRsOn8NrHWvin/fUcGQXom6OlMVvUNTNLyJHFtCrm+r87EgYDOoWAp41WOL8q+3bt6N+/frysWDbtm3leVhFihTJM11AdQDrnTjcAFhPbofOXMbd/14JcUj7mrfuRlmbj2PgClaYG7qa5hj19EnNldtckLnxEaHNXdCW5j1rsIoVK4agoCB06NAhfbN7Xth3lbEX0WAZ84hw85FzGPXzPmw5egFJyaloX6sEpj/VzJYBq6VRfphpoeS8MtTNeZpoiUivbqrzs5bYWMZeAp41WAKrWMESm9zFZvfVq1cjODhYPh5s3749+vXrZy95C1pXHcB6Jw4LUlNuQmtuwlC1+mg54i4nyjZLRebH+P/XGE0qOWdTe2YYWnNThmhDBczNBugGNEnduIJlQFGWAGEAACAASURBVDdyXRWeNlgZ1di8eTM+/fRTfPHFF9zkrrGbclIEFmyNwSvztkljNadvC1SNLihXRp18UTcnq2PMqqrbMmSfpMFyW581Il7PGiyxoV2sXol/YvXq0qVLaNiwoXxcKFawxLsJvX5xBUv9w6zbZ2ux7dgFvN6xJl526KZ2rmB5YyTThLhTR726qc7P7qSVt6L2rMEKDQ1Fo0aN0s++EpvcxbsJ89KlOoD1ThxuYKwlt61Hz6P7+HXIFxKMde/cjehC+d2QGrTk5opEsgmSublTOerGFSx39ly1qD1rsIS5yGuGKnNXoMFSW8F6Ze5WLNh2Ag83Lo9/P9pQbaRZeDc/zCyEbWBT1M1AmBZWpVc31fnZwhTZlE4CnjVYgseFCxfw9ddf4+DBgxgwYADENwu3bNkiT3UvV66cTmTuuU11AOudONxAyF9uVxKT0eiDpUhKScUPL92JBuWj3JCWjNFfbq5JhCtYbpbqltjZJ7mC5ZnOHEAinjVY4huE99xzjzz36q+//sK+fftQtWpVDBw4EEeOHMHnn38eACZg/PjxGDVqFGJjY1GvXj2MHTtWnquV3SX2eYlvLma+OnfujEWLFskf9+7dGzNnzryliDi7a/369ek/S0xMxBtvvAHx2p+rV6/KfEQc5cuX1xQ7DVbOmPxN+Mv3nsLTMzahQrECWDWgveM3tmfM1F9umjqPQwsxN4cK4ycs6kaD5c6eqxa1Zw2WOP+qcePGGDlyJAoXLow//vhDGqx169ahR48e0nRpvcTrdXr27CnNTevWrTFp0iRMnToVu3fvRsWKFbNUc+7cOSQlJaX//OzZs3KDvbhHGCufwRKv8Jk+fXp6uXz58slVNt/1/PPP44cffsCMGTNQvHhxvP766xB1i29EhoSE+A2fBku/wRr8/S7MWPcXejSviOHdG/hl7aQC/DBzkhraY6Fu2lk5qaRe3VTnZycxYCzZE/CswYqKipKPA6tVq3aLwRKrV7Vq1cK1a9c09wmxsiTM2oQJE9LvEYeWduvWDSNGjPBbj1jtGjRokFz9KliwYLrBEo8wFyxYkO398fHxKFGiBGbNmoXHHntMljlx4gQqVKiAxYsX49577/XbruoA1jtx+A3MAQX85XbPv1fg4JkrmPiPxrivfhkHRKw9BH+5aa/JeSWZm/M00RIRdctKSXV+1sKdZewl4FmDJfZZ/fTTT/KbhBlXsJYsWYI+ffrg2LFjmsiLlaiIiAjMnz8f3bt3T7+nf//+2LZtW7aPAjNX3KBBA7Rs2RKTJ09O/5VYyRLmSqxaiceY4gDUYcOGoWTJkrLM8uXL5SNBsWJVtGjR9PvESpgwdkOGDPEbv+oAzquTYsyFq2j90XL5SpytgzohqoC7XsmSV3XzOyAcXoC6OVygHMLTq5vq/OxOWnkras8arGeffRZnzpzBV199JR+7iT1Z4rGaMCfiyAaxqqTlEqtGYkP82rVr0apVq/Rbhg8fLvdQib1duV0bN26EWAHbsGEDmjW7+XoV8dixUKFCqFSpEg4fPiz3hiUnJ8vHf/nz58ecOXPw1FNPQezDynh16tQJVapUkY8pM1+ibMbyYgCLFa+4uDhd36gUE8fSpUvRsWNHhIW5y2T40za33OZvPo5/LdiNRhWi8NWzzf1V5bjf51XdHCdEgAFRtwCBOaS4Xt3E/BwdHQ3xtCKvf+PdIVIaHoZnDZbovOIw0Z07d8pDRsuWLYuTJ0/KlSTxiM33qM4fUZ/BEnu3xL2+S6w2icd3e/fuzbWK5557Tu772rFjR67lxONDYbbmzp2Lv/3tbzkaLGF2xGPPiRMnZqlv8ODB2a5sCbMmVuF4aSMwfX8wtp0Nxn3lU3F/hVRtN7EUCZAACQRAICEhQe4HpsEKAJrLinrSYIm/KMRKj9gzJQyS2IuVmpoq91GJze+BXCqPCMUAKlOmDD744AOIR4r+rho1auCZZ57BW2+9pesRIVew/BG++fuc/upMSU1D849+RfzVZMzr2wyNKxbRXqlDSur9i9oh4ecaBnNzg0pZY6RuWZlwBcudfTmQqD1psAQAsUFcrBwJ06J6iUd8TZo0kd8i9F1169ZF165dc93kLr79J14qHRMTI78FmNslvmkoHkWKfVq9evWSf9WIHMS7Ex999FF5q1jlEkc0cJO7qqI5nxX1866TeG7WZhQOD8XWgR0RGhKs3pjFNejdE2JxmLqaY266sNl+E3XL3mCJL2NxBcv27mlaAJ41WOJIA7Fv6KOPPlKG5zumQTyW821WnzJlCnbt2iUf6wlDJMxR5m8UinOyxM/FY7+M1+XLlyEe5z388MNyhUscGfGvf/0LR48exZ49e+SmfHGJYxoWLlwoj2kQ+8jEmVjCiPGYBmVJ8cuuE5i+ZDM+6XMPikfeeHx6PSUV945ZhUNxV/BCu2p4877a6g3ZUAM/zGyAbkCT1M0AiDZUoVc3bnK3QSyLm/SswXr55ZflYaLVq1dH06ZNs+y5Gj16dECoxeqVOFNLrCLVr18fY8aMkZvlxSUOFq1cubI0Qr5r//798jgI8a1FsW8q4yUODRWb7cULqcVRDcJkiRdQf/jhh3JTuu8SR0mIE+jFHqqMB41mLJNbEqoDWO/EERBYGwqnpaXhzo+XI+bCNTzSuBz+59HbZRSz1h/BwAU7UaxgPqwY0A6R4e7c2O9V3aQJvn5druCKQ3u9+MUL5mbDhKDYpN4+qTo/K4bN2y0g4FmDJQxLTldQUJDc4+T1S3UA6504nM51Z0w8Hhi3Jj3Muc+2QI2ShXDv2FWIu5yEIQ/Vw5OtKjs9jRzj86puNFiu7ZI0xtlIpzo/u7c35J3IPWuw8o6EOWeqOoC9+kH97yX7MG75AQQHpSE1LQhlo8KRcD0FFxKuo0p0QSx5tS3CXLj3ytcTvKobDZZ7ZzX2yazaqc7P7u0NeSdyGiwPa606gL0yKSanpOK1r/5A+aIFMODeWug0ZhX+PH0Zj1RJwYozBeSqlbhqlSqMMY/djrplI13dK7yiW3YiMDd3dk3qRoPlzp6rFjUNlho/R99Ng3VDnvWHzuLxyTdeot3vrmqYuPIgQoOD8GGT6yhTrznGLjuIrreXRc8WlVz5rcHMnZAfZo4elny068FDi/XsnVOdn93Zy/NW1DRYHtZbdQB75YN68qqDGL741gNh76xeHH8vcYqbpV3W/73SJ7k657KOl0u4evuk6vzsHYLezYQGy7vaQnUA6504nIb0xTlbsGh7LPKFBiMp+cbJ7B88VAdRZ3bQYDlNLD/xeKVP0mC5rOPRYHlHMAszocGyELbVTdFg3SDeZuRyHDt3FaMfbYjhi/fgalIKfnn1TmxctYwGy+pOqdgeDZYiQJtup25ZwavOzzZJyWYDIECDFQAstxVVHcBemBTPXUlC4w+XSun+eL+TPEz02vUUlCoUxvOU3NaheQ6WCxW7EbIX5pKc4OvNTXV+dm1nyEOB02B5WGzVAax34nAS0hX7TqP39N9RNboglr/RLj00L+Rm9ITvJN2YmzsPuaVu2nVTnZ/dMF7zeow0WB7uAaoD2Asm5JNlf2L00v3odntZjH28EQ2Wy/u7F/okTYh2E+KG7qq3T6rOz25gk9djpMHycA9QHcB6Jw4nIX1m5u/4Zc9pDHqgLp6+swoNlpPE0RGLF/okDRYNlugDqvOzjuHDWywmQINlMXArm1MdwG7/MBPvHGw2fBnOXErEf59viSaVitFgWdkBTWjL7X0yNyTMzYQOY0GVenVTnZ8tSI1NKBKgwVIE6OTbVQew3onDCUzE6e17T16S7xwMCQ7CzsH3okC+EBosJ4ijEIOb+6S/tJmbP0LO/L1e3VTnZ2fSYFQZCdBgebg/qA5gvROHXUhTU9Ow5kAc5mw4imV7T+F6SpoMpU6ZSPzYv80tYbktt0CYMrdAaDmnLHVzjhaBRKJXN9X5OZAYWdYeAjRY9nC3pFXVAax34rAkuWwaGfTdTnz+25H034SFBKFMVAG83qkmut5ejgbLLmEMbNdtfTKQ1JlbILScU1avbqrzs3MIMJKcCNBgebhvqA5gvROHHUgz7rd6tGl5uaG9ZsnCCA4OyjYcN+UWKE/mFigxZ5Snbs7QIdAo9OqmOj8HGifLW0+ABst65pa1qDqA9U4cliWYoaFj5xLQZuSv8iXOO4fci/Cwm/utsovHTbkFypO5BUrMGeWpmzN0CDQKvbqpzs+Bxsny1hOgwbKeuWUtqg5gvROHZQlmaOi7bTHoP3cbGpaPwncv3ek3BDfl5jeZTAWYW6DEnFGeujlDh0Cj0Kub6vwcaJwsbz0BGizrmVvWouoA1jtxWJZghoYGf78LM9b9hd6tKmPwQ/X8huCm3PwmQ4MVKCJHlmefdKQsfoPSq5vq/Ow3MBawnQANlu0SmBeA6gDWO3EYkdHpS9fkS5krFS+oqbqHPl2D7cfj8ckTjfBQw7J+77EzN7/BKRZgbooAbbqdutkEXrFZvbqpzs+KYfN2CwjQYFkA2a4mVAew3olDb77imIWYC1cxdfUhfLnxGFLS0jDy4dvwcJPy2BN7EQu3n8CjTStkMV3i5c313/8ZyalpWP1me1QoFuE3BKtz8xuQgQWYm4EwLayKulkI28Cm9OqmOj8bmAKrMokADZZJYJ1QreoA1jtxBJr7iQtX8dyszdh36hKSklOz3H5vvVLydTcpqWkoUTg/5jzTHDVKFU4vt/HwOTw66Tf5u43/ugdBQdl/czBjxVblFigLI8ozNyMoWl8HdbOeuREt6tVNdX42InbWYS4BGixz+dpau+oA1jtxBJr061/9gf9uOS5vE6cqNK5YFK90qInle09j2trD6dUViQjDhYTrKF4wH2b3bY7apSPl7yauPIiPftwLYcQm9WyqqXmrctMUjMGFmJvBQC2qjrpZBNrgZvTqpjo/G5wGqzOBAA2WCVCdUqXqANY7cQSS/4HTl9BpzCqkpgGz+jRDi6rFERYSLKsQZ1tNWX0IP+08iefbVUfTSkXRc9oG7Iy5iKrRBfHTK22RLzQYz36+CUt2n8I799fGc3dV09S8FblpCsSEQszNBKgWVEndLIBsQhN6dVOdn01IhVUaTIAGy2CgTqpOdQDrnTgCYfDi7C1YtCMWHeuWwpRe/lefLiQkocPoVYi7nIi376+NLg3KoPMnq3HpWjLm92uJOyrffKFzbnFYkVsgHIwsy9yMpGldXdTNOtZGtqRXN9X52cgcWJc5BGiwzOHqiFpVB7DeiUNr8pv+OodHJv4GsWVKvCvQ98jP3/1fbz6ON+b/gYh8ISgdGY5DcVdQu3Rh/PDynemrX/7qMDs3f+2b+XvmZiZd8+qmbuaxNbNmvbqpzs9m5sS6jSFAg6WR4/jx4zFq1CjExsaiXr16GDt2LNq0ufUFwr6q2rVrh5UrV2apuXPnzli0aBHEgHzvvfewePFiHDp0CFFRUejQoQM++ugjlC1784iBypUr48iRm+/WExW+9dZbspyWS3UA6504/MUmHv3NXPcXhv+4V25q73p7Wfzn8Ub+bkv/vfi24cMT12Hr0QvyZ2WiwvHNC60g3juo9TIrN63tm1mOuZlJ17y6qZt5bM2sWa9uqvOzmTmxbmMI0GBp4Dhv3jz07NkTwmS1bt0akyZNwtSpU7F7925UrFgxSw3nzp1DUlJS+s/Pnj2Lhg0bynt69+6N+Ph4PPLII+jbt6/8+fnz5/HKK68gOTkZmzZtSr9PGKw+ffrIcr6rUKFCEP+0XKoDWO/E4S+29xbswBfrj8pi99QuidGP3o6oiDB/t93y+x3H49Ft/Fq5ivV1v1aoVfrmtwq1VGRWblraNrsMczObsDn1UzdzuJpdq17dVOdns/Ni/eoEaLA0MGzevDkaN26MCRMmpJeuU6cOunXrhhEjRvitQax2DRo0SK5+FSyY/cGZv//+O5o1ayZXrHymTRgsYbzEPz2X6gDWO3HkFqvvlTbiseCgB+rKk9e1HKuQXZ37Tl5CofBQlCuifeXKV48ZuenRyIx7mJsZVM2vk7qZz9iMFvTqpjo/m5EL6zSWAA2WH55iJSoiIgLz589H9+7d00v3798f27Zty/ZRYOYqGzRogJYtW2Ly5Mk5tvbLL7+gU6dOuHDhAiIjbxw/IAxWYmKiXA2rUKEC/v73v2PAgAHIly+fpl6gOoD1Thw5BXfozGU8OG4NriSl4J/31MBrHWtqysOMQkbnZkaMeutkbnrJ2XsfdbOXv97W9eqmOj/rjZf3WUeABssP6xMnTqBcuXJYu3YtWrVqlV56+PDhmDlzJvbt25drDRs3boRYAduwYYNcocruunbtGu68807Url0bX3zxRXqRMWPGyJWzokWLQtTzzjvvoGvXrvJRY3aXMGPin+8SA1gYs7i4uHTTFkjXEhPH0qVL0bFjR4SFBfYIL3M74rU3j07ZiL0nL6FZ5aL4/KmmCBGHXtl0GZmbTSnk2Cxzc5oi2uKhbto4Oa2UXt3E/BwdHS23jPj+qHZaboxHjQANlkaDtW7dOrkK5buGDRuGWbNmYe/evbnW8Nxzz0Hcu2PHjmzLicEpVqaOHj2KFStW5DrQ/vvf/8q9W8IwFS9ePEt9gwcPxpAhQ7L8fM6cOXIVzq4rTZxxdSAYm+OCUSg0DQNuS0GR/HZFw3ZJgARIwH4CCQkJ6NGjBw2W/VKYFgENlh+0Ko8IxQAqU6YMPvjgA4hHipkvYa4effRR+U3C5cuXZ2uaMt4TExOD8uXLY/369XJVLPPl1BWs6euOYPiP+xAaHIQZvZugeRVtZ1WZ1usB+U1Oo1bnzIxTT93MTQ81+++hbvZroCcCvbpxBUsPbXfdQ4OlQS9hZpo0aSK/Rei76tatKx/X5bbJfcaMGejXrx+EMcq84uQzV3/++Sd+/fVXlChRwm8kCxcuxIMPPnjLRvjcblJ9xq93b0HGmM5fSUKz4b/gekoaBj9YF71bV/GbpxUFjMjNijj1tMHc9FCz/x7qZr8GeiLQq5vq/KwnVt5jLQEaLA28fcc0TJw4MX2z+pQpU7Br1y5UqlQJvXr1kvu0MpstcU6W+PncuXNvaUUcx/Dwww9jy5YtEKapVKlS6b8vVqyY3MT+22+/yZWq9u3by3OyxLcMX331VTRt2hTfffedhqgB1QGsd+LIGNz6Q2fx+OT1KF+0AFa/2V73NwY1JRxAISNyC6A5S4syN0txG9YYdTMMpaUV6dVNdX62NEk2posADZZGbGL1auTIkfKohfr160NsQG/btq28WxwsKr7xJ1asfNf+/ftRq1YtLFmyRG4Sz3j99ddfqFIl+5UcsZol6hPm64UXXpB7vMSjP2HkHn/8cbz55pua91OpDmC9E0fGXGdvOIJ3v92J9rVKYPpT2W/y1yiBocWMyM3QgAysjLkZCNPCqqibhbANbEqvbqrzs4EpsCqTCNBgmQTWCdWqDmC9E0fG3D9cuBv/u+Yw+txZBQMfqOsELDIGI3JzTDKZAmFuTlUm97ioW97STXV+dietvBU1DZaH9VYdwEZM+L2nb8SKfWcwvHsD9Gie9dR7u/AbkZtdsftrl7n5I+TM31M3Z+riLyq9uqnOz/7i4u/tJ0CDZb8GpkWgOoD1ThwZE2ozcjmOnbuKec+2QPOqWY+WMC15PxUbkZtdsftrl7n5I+TM31M3Z+riLyq9uqnOz/7i4u/tJ0CDZb8GpkWgOoD1Thy+hK5dT0GdQT9BnIO16b0OiC7knMOvVHMzTTQDKmZuBkC0oQrqZgN0A5rUq5vq/GxA6KzCZAI0WCYDtrN61QGsd+Lw5bz35EXcN3Y1IsND8cf7nRzzDUIRn2pudurqr23m5o+QM39P3Zypi7+o9OqmOj/7i4u/t58ADZb9GpgWgeoA1jtx+BJatD0WL87ZgkYVi+DbF1qblqeeilVz09OmVfcwN6tIG9sOdTOWp1W16dVNdX62Kj+2o58ADZZ+do6/U3UA6504fGA+WfYnRi/dj0ealMf//L2ho3ip5uaoZDIFw9ycrE7OsVG3vKWb6vzsTlp5K2oaLA/rrTqAVSf8/nO34rttJ/DWfbXxfLtqjiKtmpujkqHBcrIcmmNjn9SMylEF9eqmOj87CgKDyZYADZaHO4bqANY7cfiQPjhuDXbExGNSzya4t15pR5FWzc1RydBgOVkOzbGxT2pG5aiCenVTnZ8dBYHB0GDltT6gOoD1ThyCc1paGuq9/zMSklLwy2t3oXrJQo7Cr5KboxLJJhjm5nSFso+PuuUt3VTnZ3fSyltRcwXLw3qrDmCVCT82/ipajliO0OAg7PnwPoSFBDuKtEpujkqEBsvpcmiOj31SMypHFdSrm+r87CgIDIYrWHmtD6gOYL0Th+A8dfUhDF20B1WjC2L5G+0ch14lN8clkykg5uZ0hbiC5U6FjNVNdX72EkOv5sIVLK8qC0B1AOv9oJ678Sje/maHJDvg3lp4sX11x1HWm5vjEuEKlhsk0RQj+6QmTI4rpFc31fnZcSAYUBYCNFge7hSqA1jPxLHmzzj0nLZBnt4uXvD8Xpc6jjpg1Ce3ntzc0lWYm1uUujVO6pa3dFOdn91JK29FTYPlYb1VB7CeCX/w97swY91feOC2Mhj3RCNHmishuZ7c3NJVmJtblKLBcqdSxuimOj97gZ3Xc6DB8rDCqgNYzwf10zN+x/K9pzG8ewP0aF7RsXT15ObYZDIFxtzcopQxH9RuyJZ9MqtKqvOzG3TP6zHSYHm4B6gOYD2TYofRK3Hg9GXMfqY5WlePdixdPbk5NhkaLLdIk2uc7JPulFGvbqrzsztp5a2oabA8rLfqAA504khNTUPtQT8hKTkVq99sjwrFIhxLN9DcHJtINoExNzepdTNW6pa3dFOdn91JK29FTYPlYb1VB3CgE/7J+GtoMWIZQoKDsO/D+xDqsLOvMkodaG5u6ibMzU1q0WC5Uy113VTnZ7dzywvx02B5WGXVARzoB/XGw+fw6KTfULFYBFa92d7RZAPNzdHJ8BGhm+TJMVb2SXfKqFc31fnZnbTyVtQ0WB7WW3UABzpxfL35ON6Y/wfurB6NL55p7miygebm6GRosNwkDw1WWJgn9PIloXcuUZ2fPQXRo8nQYHlUWJGW6gAOdOIYvWQfPll+QH57UHyL0MlXoLk5OZfMsTE3N6ml/qjJDdmyT2ZVSXV+doPueT1GGiwP9wDVARzopPjK3K1YsO0E3r6/NvrdVc3RZAPNzdHJcAXLTfJwBYsrWLIPqM7Pnuj0Hk+CBsvDAqsO4EBNyN/Gr8WWoxcw/v81RucGZRxNNtDcHJ0MDZab5KHBosGiwfLEiPWfBA2Wf0auLWG1wWo6dCniLidh4ct3on65KEdzo8FytDw0ITQhrumgeucS1fnZNYDycKA0WB4WX3UABzJxXElMRr33f5Y0tw/uhMhwZ29kDSQ3t3UR5uY2xW7ES93ylm6q87M7aeWtqGmwNOo9fvx4jBo1CrGxsahXrx7Gjh2LNm3aZHt3u3btsHLlyiy/69y5MxYtWiR/npaWhiFDhmDy5Mk4f/48mjdvjs8++0zW7bvEz//5z3/i+++/lz966KGHMG7cOBQpUkRT1KoDOJAJf+/Ji7hv7GoUiQjDtkGdNMVnZ6FAcrMzTj1tMzc91Oy/h7rZr4GeCPTqpjo/64mV91hLgAZLA+958+ahZ8+eECardevWmDRpEqZOnYrdu3ejYsWs79s7d+4ckpKS0ms+e/YsGjZsKO/p3bu3/PnHH3+MYcOGYcaMGahZsyaGDh2KVatWYd++fShcuLAsc//99+P48ePShInr2WefReXKlfHDDz9oiFp9E2UgE8fPu07iuVmbcVv5KHz/0p2a4rOzUCC52RmnnraZmx5q9t9D3ezXQE8EenWjwdJD21330GBp0EusLjVu3BgTJkxIL12nTh1069YNI0aM8FuDWO0aNGiQXP0qWLCgXL0qW7YsXnnlFbz11lvy/sTERJQqVUoar+eeew579uxB3bp1sX79erm6JS7x3y1btsTevXtRq1Ytv+2qDuBAJo6pqw9h6KI96HJbGXzWo7Hf2OwuEEhudscaaPvMLVBizihP3ZyhQ6BR6NVNdX4ONE6Wt54ADZYf5mIlKiIiAvPnz0f37t3TS/fv3x/btm3L9lFg5iobNGggjZFvJerQoUOoVq0atmzZgkaNGqUX79q1q3z8N3PmTEybNg2vvfYaLly4cEt14vdjxozBU0895be3qA5gLRPH7A1HsPCPWMTGX8VfZxPwQrtqePO+2n5js7uAltzsjlFv+8xNLzl776Nu9vLX27pe3VTnZ73x8j7rCNBg+WF94sQJlCtXDmvXrkWrVq3SSw8fPlwaIfFIL7dr48aNcgVqw4YNaNasmSy6bt06+agxJiZGrmT5LvEI8MiRI/j5558h6hePD/fv339L9eJxojBX77zzTpZmxSqY+Oe7xACuUKEC4uLiEBkZGXCvEhPH0qVL0bFjR4Rl860msRLXZPivuHQtOb3u/3m4PrrefjOngBu16AZ/uVkUhinNMDdTsJpeKXUzHbEpDejVTczP0dHRiI+P1zU/m5IMKzWUAA2WRoMlTJFYhfJdYv/UrFmz5OO63C7xuE/cu2PHjvRiPoMlzFuZMjfPi+rbty+OHTuGn376SRqs7AxcjRo10KdPH7z99ttZmh08eLDcOJ/5mjNnjlyFM/q6dB14b1MogpCGrpVSERoMtCyZJv+XFwmQAAmQQM4EEhIS0KNHDxosD3cSGiw/4qo8IhQDSBioDz74AOKRou8y6xGh1StYm46cxxNTf0fZqHCsfKOtq4aJ3r863ZAkc3ODSlljpG55SzeuYLlT70CipsHSQEs84mvSpIn8FqHvEhvQxZ6p3Da5i0d8/fr1k48Cixcvnn6vb5P7q6++ijfffFP+XBi5kiVLZtnknvHRovjvFi1aOGaT+1ebqHQb5QAAIABJREFUjuHNr7e74uXOmWXWu29CQ3exvQhzs10CXQFQN13YbL9Jr27cg2W7dKYHQIOlAbHvmIaJEyemb1afMmUKdu3ahUqVKqFXr15yn1ZmsyXOyRI/nzt3bpZWxLcFRfnp06dDPPYTjwRXrFiR5ZgG8RhRHAshLrFHS7TnlGMaRv60F+NXHMQ/WlTE0G7OfrkzDZaGju6CIno/zFyQGg8adYNI2cSot0/SYLlU8ADCpsHSCEusXo0cOVIetVC/fn35Tb62bW88FhMHi4rzqcSKle8Sm9PFUQpLliyRm8QzX76DRoV5ynjQqKjbd4nztDIfNPrpp5865qDRF2ZvxuIdJ/Felzp4pk1VjSSdUUzvpOiM6HOPgrm5QaWsMVK3vKUbDZY79Q4kahqsQGi5rKzqAPY34d//n9XYE3sR//tkU9xTp5Sr6PjLzVXJZAqWublTPeqWt3RTnZ/dSStvRU2D5WG9VQdwbhO+WIGrO+hnXL2egmWv34VqJQq5iiQ/zFwlV3qw1I26OY2A3j6pOj87jQPjyUqABsvDvUJ1AOc2cZyMv4YWI5YhOAjY++H9yOeysxn0Topu6C7MzQ0q8RGhO1UyTjfV+dkr/LycBw2Wh9VVHcC5fVD/dvAsnpiyHpWKR2DlgPauo0gT4jrJZMDUjbo5jYDePqk6PzuNA+PhClae6gOqAzjzxHEhIQn/b+oG3F+/NIoXyo93vtmBu2qWwMynb5xQ76ZL76TohhyZmxtUMm4lxA3Zsk9mVUl1fnaD7nk9Rq5gebgHqA7gzJPiwu0n8NKcrfKxoNjUvnT3KfRuVRmDH6rnOoqc8F0nGVew3CkZdctBN9X52cXdIc+EToPlYalVB3BmEzJ19SEMXbTnFmKDH6yL3q2ruI4iDZbrJOMHtTslo240WC7uuWqh02Cp8XP03UYbrKELd2PqmsO35DzjqTvQrlZJR3PILjgaLNdJxg9qd0pG3WiwXNxz1UKnwVLj5+i7jTZYL87ZgkXbY+U3BpOSU2Xuqwa0R8Xixr9I2mywNFhmEzanfupmDleza6VuWQmrzs9ma8b61QnQYKkzdGwNqgM486T48IR12HzkPF7pUAPjlh9A4fBQbHq3A0JDgh3LIKfAOOG7TjKuhLhTMurGFSwX91y10Gmw1Pg5+m6jDVbrj5Yj5sJVfPNCKwQHBSF/aDDqlIl0NAMarDBX6kPdqJtbOq7eP9ZU52e38MnLcdJgeVh91QGcceIICQlFzfd+RHJqGn57526UiSrganJ6J0U3JM3c3KBS1hipW97STXV+dietvBU1DZaH9VYdwBkn/PPXUtBs2I2T2/cPvd+VjwUzSs0PM3d2fOpG3ZxGQG+fVJ2fncaB8WQlQIPl4V6hOoAzThx7Tl3BQ5+uRanI/Njwrw6up6Z3UnRD4szNDSpxBcudKhmnm+r87BV+Xs6DBsvD6qoO4Iwf1L/uP4tnZ21GwwpF8N2LrV1PjSbEnRJSN+rmNAJ6++T/b+88oKUosjd+SSIgj0UUWHJUogRBFCQYiAbARZG4KIiuSF6X5CKgPAkrwrqAhEVZFAkKkhVQRJIBBBVYgg8QBJWgsJJdHnu++//3nHlvXpg3PT3T1f3VOZxdobvq3t+trv7mVlWX3fHZbRxoDzNYvuoDdh/g4IHj7a1HZfiSXdK8ahGZ1qWO8RwjHRRNcJy+mRCl6GVCTPCWfTI0SnbHZxPi7ncbmcHycA+w+wAHD4oTPkySqR8nGXs0Tuowc8A3s+Mzboyb2whE2iftjs9u40B7mMHyVR+w+wAHDxyDFu2SRduPyuCWleSpxuWN5xjpoGiC4/TNhCgxg2VmlKIXN7vjs1f4edkPZrA8HF27D3Dwi7rr69tky4FTMrF9TWlTq7jx1ChCzAwh48a4uY1ApH3S7vjsNg60hxksX/UBuw9w8MDRbNImOXjynMzrebvcXq6Q8RwjHRRNcJy+mRCl6GVCTPCWfTI0SnbHZxPi7ncbmcHycA+w+wBbg2LLli3llhc+lIu/Jcv6Z5tI6UL5jKfGAd/MEDJujJvbCETaJ+2Oz27jQHuYwfJVH7D7AFsDR4O7mkqdxHXKbs8LLeTaXDmM5xjpoGiC4/TNhCgxg2VmlKIXN7vjs1f4edkPZrA8HF27D7D1oi5fu6HcP3mLFMybS7YPb+YJYhQhZoaRcWPc3EYg0j5pd3x2GwfawwyWr/qA3QfYGjjyVagrPeZs14OdV/Vt6AmGkQ6KJjhP30yIUvQyISZ4yz4ZGiW747MJcfe7jcxgebgH2H2ArUHxbOFbZNiS3XJ3pcIyq1tdTxDjgG9mGBk3xs1tBCLtk3bHZ7dxoD3MYPmqD9h9gK2BY3fOijJtw0HpVK+UjG5b3RMMIx0UTXCevpkQJWawzIxS9OJmd3z2Cj8v+8EMVpjRnTJliowfP15++OEHqVq1qkycOFEaNkx/uuz06dMybNgwWbRokfzyyy9StmxZefnll6VVq1baYpkyZeS7774Laf3pp5+WyZMn6983adJE1q9fn+Ka9u3by7x588Ky2u4DbL2oFxwvIpuSTkli2+rSsV6psNp2+0UUIW6PUNr2MW6Mm9sIRNon7Y7PbuNAe5jBiqgPzJ8/X7p06SIQWQ0aNJBp06bJzJkzZffu3VKqVKjguHz5sl5XuHBhGTp0qJQoUUKOHDki+fPnlxo1aqgNJ06ckCtXrgTs2blzpzRt2lTWrVunwsoSWDfddJOMGjUqcF2ePHmkQIECYflh9wHGwLFixUoZ8VUeOX3hN1n2zJ1SvUR4bYdlYBwvinRQjKPJYTdN38JG5aoLGTdXhSNsYyKNm93xOWwDeWHcCDCDFQb6evXqSe3atWXq1KmBqytXrixt2rSRl156KaSG1157TbNde/bskVy5coXRgki/fv1k+fLlsn//fsmWLVtAYNWsWVOzZZEUuw8wBo45i1bKqO05JVeObLJzZHPJndP8TzSAZaSDYiRxiPU99C3WxKPTHuMWHY6xriXSuNkdn2PtJ9vLOgEKrEyYIRuVN29eWbhwobRt2zZwdd++fWXHjh0hU3i4ANOA119/vd63ZMkSufHGG6Vjx44yaNAgyZEjVKCgjWLFismAAQM042UVZLJ27dolV69elSJFigg++Pn8889rJiycYvcBxsCROGeVvL4vh1QrniDLe3tjByEFVji9x53XRPoyc6c3Ka2ibyZEKdTGSONmd3w2k5a/rKbAyiTex44dk+LFi8umTZukfv36gasTExNl9uzZsnfv3pAaKlWqJIcOHZJOnToJ1lQhK9WrVy+BKBs+fHjI9QsWLFABdvjwYRVaVpkxY4au3SpatKhgCnHIkCFSoUIFWbNmTZpWX7p0SfDHKniAS5YsKSdPnpSEhIQs92wMHH1mfihrj2WX9nVKyIutq2S5DrfeAN/AEdOy4WYZ3epLarvomymRChVY7JPmxS7S5w3j8w033CBnzpyJaHw2j5T/LKbAClNgbd68We64447A1aNHj5Y5c+boNGDqgnVTFy9elIMHDwYyVhMmTAgskk99ffPmzeWaa66RZcuWZWjNtm3bpE6dOoL/xZRl6jJixAgZOXJkyN/PnTtXs2mRlCm7s8veM9mlfbkrUr/I1Uiq4D0kQAIkQAKpCJw/f15/WFNgebdrUGBlEttIpggbN26sWZG1a9cGal+1apVOHSLDBDFlFewkLFeunO42bN26dYbWYKowd+7cKuywmzB1iXYGC77XTVwn569kk8VP3a7ThF4pkf7qNMF/+mZClEJtZNz8FTdmsMyMd1aspsAKgxYWud966626i9AqVapUUUGU1iJ3rKNC1ujAgQOSPXt2vWXSpEkyduxYwZRjcEHWCbsSscswZ86cGVqDacLq1avruq9GjRplarndOf4Dx8/I3RM2em6BO8BFum4iU+guuIC+uSAIEZjAuEUAzQW3RBo3u+OzC1ynCZkQoMAKo4tYn2nA7kBME06fPl2wPgoL0EuXLi1du3bVdVqW2IJYggDr1q2b9O7dW9dgPf7449KnTx/9NpZVkpOTdY1Vhw4dZMyYMSksSUpKkrfeekuzXpinxychBg4cKPhMwxdffJHmYvnUrth9gJduPyJ95n8t1YolyPI+3lngToEVRqd36SWRvsxc6k4Ks+ibCVEKtTHSuNkdn82k5S+rKbDCjDeyV+PGjdMPjVarVk1eeeWVQBYJu/3w4dA33ngjUNuWLVukf//+utMQ4qt79+4huwhXr14tWH+FhfJYtxVcINI6d+6si9vPnj2ri9Xvu+8+3UWIHYrhFLsPcOKKXTJ9wyFd4D623f99v8srJdJB0QT/6ZsJUYrei9oEb9knQ6Nkd3w2Ie5+t5ECy8M9wO4D3HHGFtmc9LO88GAV6VK/rKdIccA3M5yMG+PmNgKR9km747PbONCeUAIUWB7uFXYf4M+STsjc1Vukd9vGUqGoN77gboU70kHRhO5C30yIEjNYZkYpenGzOz57hZ+X/aDA8nB07T7AfFGb2TkYN8bNbQTYJzlF6LY+GQt7KLBiQTlObVBgpQ+eA36cOqXNZhk3mwDjdDvjRoEVp64X12YpsOKK39nGKbAosLz4lfqVK1fq7lr65uz4Ec3aKbAosKLZn0ypiwLLlEhFYCcFFgUWRUgED06cbqEIiRN4m81GGje747NNs3l7DAhQYMUAcryasPsARzpwxMvfrLRL37JCyz3XMm7uiUVWLGHcmMHKSn/xyrUUWF6JZBp+UGAxg8UMljkPOEWIObEKtjTSuNkdn82k5S+rKbA8HG+7D3CkA4cJSOmbCVEKtZFxY9zcRiDSPml3fHYbB9oTSoACy8O9wu4DHOnAYQJS+mZClCiwzIwS4xZO3OyOz+G0wWviS4ACK778HW3d7gNMEeJoeByrnHFzDK2jFTNujuJ1rPJI42Z3fHbMIVYcNQIUWFFD6b6K7D7AkQ4c7iPBX9QmxCQcG9knw6HkvmsYt9CY2B2f3RdlWpSaAAWWh/uE3QeYg6KZnYNxY9zcRoB9kgLLbX0yFvZQYMWCcpzaOHPmjPzud7+TI0eOSEJCQpatwKC4evVqadasmSc/6kjfstwl4n4D+2TcQxCRAYxb2gKrZMmScvr0aSlQwFtnvUbUSTx4EwWWB4NqufT9998LHmAWEiABEiABdxLAD+ASJUq40zhaZYsABZYtfO6+OTk5WY4dOyb58+eXbNmyZdlYTDFCoEWaActygzG8gb7FEHYUm2LcoggzhlUxbqGwr169Kr/++qsUK1ZMsmfPHsNosKlYEaDAihVpA9uxu4bLzS7TNzdHJ33bGDfGzW0EvNwn3cbaNHsosEyLWAzt9fLAQd9i2JGi2BTjFkWYMayKcYshbDblGgIUWK4JhfsM4aDovpiEYxHjFg4l913DuLkvJuFY5OW4heM/r0mfAAUWe0e6BC5duiQvvfSSDBkyRHLnzu0pUvTNzHAyboyb2wh4uU+6jbVp9lBgmRYx2ksCJEACJEACJOB6AhRYrg8RDSQBEiABEiABEjCNAAWWaRGjvSRAAiRAAiRAAq4nQIHl+hDRQBIgARIgARIgAdMIUGCZFjHaSwIkQAIkQAIk4HoCFFiuD1H8DJwyZYqMHz9efvjhB6latapMnDhRGjZsGD+DstgydkAuWrRI9uzZI3ny5JH69evL2LFj5eabbw7U1KRJE1m/fn2Kmtu3by/z5s3LYmuxvXzEiBEycuTIFI0WKVJEfvzxR/07fCUa/z59+nT55ZdfpF69ejJ58mSNo9tLmTJl5Lvvvgsx8+mnn1YfTIrZJ598os/Qtm3b9DlavHixtGnTJuBbOHFC/Pr06SNLly7V+x588EF59dVX9ZzReJaMfMPZg88995ysXLlSDhw4oGft3XvvvTJmzBj9crlV0or1oEGD9Lp4lszi1q1bN5k9e3YKE/GMffrpp4G/w+7CP//5z/L222/LhQsX5J577hGMqTwWJ56RjW3bFFix5W1Ma/Pnz5cuXbrogNCgQQOZNm2azJw5U3bv3i2lSpUywo8WLVrIo48+KnXr1pX//ve/MmzYMPnmm2/Uh3z58qkPeFnfdNNNMmrUqIBPEGNuP3wVAuudd96RtWvXBuzOkSOH3HjjjfrfEJKjR4+WN954Q/178cUXBS+NvXv36tFJbi4nTpyQK1euBEzcuXOnNG3aVNatW6fxMilmq1atkk2bNknt2rXlD3/4Q4jACidOLVu2FJwrCrGM0rNnT4EwWbZsWVzDmJFvOGi+Xbt28sQTT0iNGjVU5Pfr10+fw61bt6YQWN27d9frrHLdddcJ/sSzZBY3CKyffvpJXn/99YCZ11xzjVx//fWB//7Tn/6kMcIzWKhQIRk4cKD8/PPPKrbxrLJ4nwAFlvdjHJGH+DWGl8LUqVMD91euXFl/fSMzZGLBi7tw4cKasWrUqFFAYNWsWVOzcyYVCKz33ntPduzYEWI2siLIEuCFhmwACn5NI8OFF/qTTz5pkqvqx/Lly2X//v16piYElokxg+3BGaxw4vTvf/9bqlSpopkRPJMo+P933HGHZmaDs7HxDGpq39Ky5YsvvpDbbrtNs5PWjzQIRcQXf9xa0vINAuv06dP6DKZVIDDxY2fOnDmCjDgKzoXF2a7I6jVv3tyt7tKuKBKgwIoiTK9UdfnyZcmbN68sXLhQ2rZtG3Crb9+++kJPPaVmit/ffvutVKxYUbNY1apVCwisXbt26ZQaBAiyBc8//7zrszwQWJh6QqYNH4HFyzcxMVHKlSunUzLly5eXL7/8UmrVqhUIT+vWrXVaKfXUhpvjh74IsThgwAAZOnSo0TFL/aIOJ06zZs1S3/EyDy6I4yuvvCKPPfaYK8IXjsBCtrVZs2bqS0JCgtoNgQXxjzhDfDz88MPy7LPPCrJBbinpCSyIK9iJWDRu3FgzxvgBh/LRRx/plCAyVgULFgy4gmwefqSmnt53i6+0I7oEKLCiy9MTteGXVvHixXVqA+uWrIIXOF7OmGYyrUBAQWBgqmLDhg0B82fMmCFly5aVokWLCqai8NX6ChUqyJo1a1ztIqYwzp8/r9N/mKrAFCAyGhCLiA+mdY8ePZpivQumlpA9+OCDD1ztW7BxCxYskI4dO8rhw4cDvpgas9Qv6s2bN2caJzxzmGLat29fipgh7hBX6K9uKJkJrIsXL8qdd94plSpVkjfffDNgMkQiMuUQIZ9//rn6g+cUyxHcUtLyDUsoMI1ZunRpOXjwoPz1r3/V6U9M/+EHz9y5czU+EI/BBQIT4w2WXLB4nwAFlvdjnGUPLYGFFwCmIqyCX2hIeeNFblrp1auXrFixQjZu3JjhIlMMkHXq1NGBEgO/KeXcuXOatfrLX/4it99+u764Ecff//73ARewzuXIkSPy/vvvm+KWTqUgS5DReiNTYpaewMooTun9qEEmFmuXBg8e7IpYZiSwsOAdmSmI5I8//jiQvUrL8HfffVfXbp08eVLXLbmhZCYeYSM2MEBsYXPMQw89lK7AwlpCPKevvfaaG1yjDQ4ToMByGLCJ1XttirB37966VgKLvPHrMaOCTBd+gQavnTAlhhi8kX3DFIsXpgiRbcOUJ3aCIquRXjElZn6cIoS4euSRR3TaGtNmmYkmZF2xyy54zVm8n79wBBZshOjt0aOHrnvkFGG8o+aO9imw3BEH11mBNT233nqr7iK0Chbb4kVnyiJ3vHghrrCwGL+cMQBmVjBNWL169RQL4TO7xw3/jqkIiCpMA2K6AuuW+vfvrxktFIhmrA8xaZE71plhKgVZt5w5c6aL2ZSYpbfIPaM4WYvcP/vsM10gjoL/jyyl2xe5W+IKmxOwA9Ta4ZrR84LNDA888ECKhfDxfr7CEVinTp3SZRXY6dm1a1exFrljOhQC08pyQTxykXu8Ixq79imwYsfaqJaszzQglY1pQgwcWPuCNT5IhZtQ8N0krIVYsmRJit1WWBiOTzEkJSXJW2+9Ja1atZIbbrhBP9+ArdT4N+x4cvNWanxfBy8i7MY6fvy4rsHC5gMs4Ed8IKQghLGNHMISU00QmSZ8pgF9Kzk5WbONHTp0SPFNJNNidvbsWcHmChRsOJgwYYLcddddup0fsQsnTth4gWlEa90ORDRiHO/PNGTkGwQ+PkuBjRYQTdhAYhX4jmnfLVu2aKYKPPBM4pmD2MQUPZ7ZeJaMfIP9EP/wD1Pwhw4d0g0YmAKFILY+g4LPNMB3rKHDPXhmIcT4mYZ4Rja2bVNgxZa3Ua0hezVu3DhdX4Bdd1iQan3ewARH8MszrQLRgW3WyIx07txZF7djQMUupvvuu093EQZ/z8aNvuL7XpjyxFoVZAaQ0XjhhRd0Sz+K9QFLvJSDPzRq7Z50o0/BNq1evVrXX0EQYkG3VUyLGUQtBETq8sc//lFfvOHECTvRUn9o9B//+EfcPzSakW8QIOlNx1vfM4P4wo8gZOKQgYVoRL9G1hW7mONZMvINn67BTsDt27frjkiILMQYzx/GEKtgYT+m6/EjL/hDo8HXxNNHtu08AQos5xmzBRIgARIgARIgAZ8RoMDyWcDpLgmQAAmQAAmQgPMEKLCcZ8wWSIAESIAESIAEfEaAAstnAae7JEACJEACJEACzhOgwHKeMVsgARIgARIgARLwGQEKLJ8FnO6SAAmQAAmQAAk4T4ACy3nGbIEESIAESIAESMBnBCiwfBZwuksCJEACJEACJOA8AQos5xmzBRIwhkCTJk2kZs2aMnHiRFfYjA9xPvnkk/LOO+/oB1PxcUfYF04pU6aM9OvXT/+wkAAJkECsCVBgxZo42yMBFxNwm8BatWqVnn+JL2vj4GccaZT6XEJ8ER0iCl/VDi4nTpyQfPnyxfWr4BR5Lu7sNI0EHCZAgeUwYFZPAiYRcEJgXblyRXBsUfbs2bOMAkfCjB8/Xg//Ta+kJ7Cy3JgDN1BgOQCVVZKAIQQosAwJFM30DwGInFtuuUWuvfZamTlzph6M+9RTT+kBsyg4XBbnvAVPlyF7U7BgQbHOebPOUnv//fdl8ODBet4bDu2eN2+eHjY7YMAAOXr0qJ69+M9//jOQ5UHb1nmFb775ph54jUNrcc6adbbj5cuX5bnnntODstEursehxbgXxRI8uB/nyu3bt0/279+f5tl0OKAa57V99dVXev4jzujDwdXIUuG8yNmzZwcCj7Pq4HtwSevMOJwlCVapxQ3sx+HlOCT5o48+0rPvZs2apWc59ujRQw8bBnfYXb58+UAzuB714aBzHGIMG4cNGxbIpOHfUM9PP/0khQoVknbt2snf//535QH/ggumPFE2b96scUGbyMq1bdtWD+dGxg0Ftnfv3l0PD166dKkkJCTIkCFDpHfv3oHq0mvXP08KPSUBdxOgwHJ3fGidDwngxQzxBBHUsWNH2bJli4qNDz74QJo2bZolgYVDoP/2t7+pgHrkkUekePHikjt3bhkzZowecI0XOwTOoEGDlDTahgDDyx3CauvWrdKzZ09dk/XEE0/oNZ06dVIbUAcEx+LFi1VwffPNN1KxYkUVWLinbt26mn2C6ChRokRAPFghhcDDQc7wDcIBIhBt9OrVSwXNmTNnVKhMnz5dhQjEHsRQcIHYw+G7w4cP14OhUa677jr9k5bAgv8TJkzQdVzweceOHTr1CCFYqlQpefzxx/UQZUxNooA5uMGOhg0bSlJSkvoGmyHksDYMrCBcq1atKj/++KOKRfiBQ5pr1Kih11vsihYtqpzq16+vohUCF1OZzzzzjF6Lg8gtgYX7hw4dKg899JDa0b9/f7ULfSCjdn34yNBlEnAlAQosV4aFRvmZAEQOptU2bNgQwHDbbbfJ3XffraImKxmstWvXyj333KP14F5kQSASICpQkBlDfch0WQLr+PHjmq2xMlbItCCLsnv3br0XIur7779XcWWVe++9V2BjYmKiCqzHHntMxQtEQ3oFWaB3331XszRWW1OmTFHhA3GFKUUIO/xJnbkKrjO9KcK0BBaEIIQNyqeffqpZPWTwIKxQIJRg+4ULF/S/GzVqJC1btlRuVrEyc8eOHVOxNm3aNNm5c6fkypUrxNW0pgi7du0qefLk0fussnHjRmncuLGcO3dOM5e4r3LlygGhh+seffRR+c9//iMrV67MtF0/Pz/0nQTcQoACyy2RoB0k8P8EILCQDZk8eXKACRZ6IxOEqaisCCyIJSvrg+wIMiV4iVsFWRhMgX355ZcBgQXxhXassmTJEp32unjxoixatEgzOtZUlnXNpUuXNNMyf/58FVjY+YfrLeGUVnBxfYECBQJZG1yD7A+yS1hzhYxStAXWggUL5OGHH1ZzDh48qELz888/12wbCqZYIWQh8DAtBz+Tk5M1e2YViF/4Bo6nTp2SBg0aCKb+WrRoIa1atZIHHnggMH2YlsBCbL/99tsUggz3nz9/XkUshBXug+hDZs4qkyZNUh6w+8iRIxm2y4eJBEgg/gQosOIfA1pAAikIpLXQvE2bNjp1BfFy+PBhXT8EUVSrVi29F9NMhQsXDlmDhU8b4D6UtDI9mIp77733NNuEgrYzEliYmsIUITJcwaID92JaDlNg4S46x/Qk1o0FiznYAZ/gY8mSJaMusDCdCZYoaQlVa02XxQ2ZppEjR6p4TF3ACVk2ZLvWrFkjyBYuXLhQ15ph7RUyWmkJLAgoTPP16dMnpE6ISqy5S09gQWQdOHBA78uoXT5SJEAC8SdAgRX/GNACEsiSwMKLFWuqVqxYoRkTFLzgmzVrFhWBhawXMilWwfQYslj4OyxYv/nmm+WTTz7RNUlplXAFVnpThJiSxOL5cKcI586dqxmzX3/9NYU5aU0RZlVgITtVqVIlnUYMp2AdGK7HOrbatWvrGjPYNnDgwMDtEKhYq/Xhhx+mWyVsr1Klik4HWqVDhw6aWQv+O+vfUrcbjq28hgRIwFkCFFjO8mXtJJBlApn/88pxAAAFvUlEQVRlsFAh1g4hQ4JdcSdPntSF6pjqSr2LMJIMFsQBFmVDGCBLhv//8ssv63+jdO7cWTZt2qR/h2wT2seuvOrVq6vgC1dgWYvcseYJU5cQCdjNZy1yR1vhTBFiRx6EEDJIWPMF8Yk/0RBYWFx+//33665BTC1C9H399de6UB27HeErpgzr1aunbSIbh3VZmMLDlC5EL7JgWFuGzQXYMYj7sfkAfoMtpiGxDg0i+dVXX1XGsB2xQ7vIuOHf+vbtq6K6efPmmbab5U7HG0iABKJOgAIr6khZIQnYIxCOwMILGWt0sGYJGaVx48ZFLYOFNUJYd4TMEKYBIayweN1aT/Xbb7+puPjXv/6ln3qAkIDgw1QaRFa4AguUMvpMQ7gCC9dhxyOm57AmKqPPNGQ1g4W6IbJGjRqlOzshapGhghCEOML0KjYPIB4QWvAfbKyNBVhID34Qj1inZn2mAbsiIZ6wQxR/h89CtG/fXncNWgIL8cVU7PLlyyV//vy60B4iCyWzdu31QN5NAiQQDQIUWNGgyDpIgARIIIoE+IHSKMJkVSQQJwIUWHECz2ZJgARIID0CFFjsGyRgPgEKLPNjSA9IgAQ8RoACy2MBpTu+JECB5cuw02kSIAESIAESIAEnCVBgOUmXdZMACZAACZAACfiSAAWWL8NOp0mABEiABEiABJwkQIHlJF3WTQIkQAIkQAIk4EsCFFi+DDudJgESIAESIAEScJIABZaTdFk3CZAACZAACZCALwlQYPky7HSaBEiABEiABEjASQIUWE7SZd0kQAIkQAIkQAK+JECB5cuw02kSIAESIAESIAEnCVBgOUmXdZMACZAACZAACfiSAAWWL8NOp0mABEiABEiABJwkQIHlJF3WTQIkQAIkQAIk4EsCFFi+DDudJgESIAESIAEScJIABZaTdFk3CZAACZAACZCALwlQYPky7HSaBEiABEiABEjASQIUWE7SZd0kQAIkQAIkQAK+JECB5cuw02kSIAESIAESIAEnCVBgOUmXdZMACZAACZAACfiSAAWWL8NOp0mABEiABEiABJwkQIHlJF3WTQIkQAIkQAIk4EsCFFi+DDudJgESIAESIAEScJIABZaTdFk3CZAACZAACZCALwlQYPky7HSaBEiABEiABEjASQIUWE7SZd0kQAIkQAIkQAK+JECB5cuw02kSIAESIAESIAEnCVBgOUmXdZMACZAACZAACfiSAAWWL8NOp0mABEiABEiABJwkQIHlJF3WTQIkQAIkQAIk4EsCFFi+DDudJgESIAESIAEScJIABZaTdFk3CZAACZAACZCALwlQYPky7HSaBEiABEiABEjASQIUWE7SZd0kQAIkQAIkQAK+JECB5cuw02kSIAESIAESIAEnCVBgOUmXdZMACZAACZAACfiSAAWWL8NOp0mABEiABEiABJwkQIHlJF3WTQIkQAIkQAIk4EsCFFi+DDudJgESIAESIAEScJIABZaTdFk3CZAACZAACZCALwlQYPky7HSaBEiABEiABEjASQIUWE7SZd0kQAIkQAIkQAK+JECB5cuw02kSIAESIAESIAEnCVBgOUmXdZMACZAACZAACfiSAAWWL8NOp0mABEiABEiABJwkQIHlJF3WTQIkQAIkQAIk4EsCFFi+DDudJgESIAESIAEScJIABZaTdFk3CZAACZAACZCALwlQYPky7HSaBEiABEiABEjASQIUWE7SZd0kQAIkQAIkQAK+JECB5cuw02kSIAESIAESIAEnCVBgOUmXdZMACZAACZAACfiSAAWWL8NOp0mABEiABEiABJwkQIHlJF3WTQIkQAIkQAIk4EsCFFi+DDudJgESIAESIAEScJIABZaTdFk3CZAACZAACZCALwlQYPky7HSaBEiABEiABEjASQIUWE7SZd0kQAIkQAIkQAK+JECB5cuw02kSIAESIAESIAEnCfwPdau+LvtKHKEAAAAASUVORK5CYII=\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f23d0c2a080> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f236c0e0cf8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 109        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 23         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11598698 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.078     |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.00809   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000569   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 745         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070500866 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.89        |\n",
      "|    explained_variance   | -0.376      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0617     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0125      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 739         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048894115 |\n",
      "|    clip_fraction        | 0.462       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.91        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0965     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0456     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00368     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.707       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 767         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036515933 |\n",
      "|    clip_fraction        | 0.465       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0523     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.702       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 763         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043721087 |\n",
      "|    clip_fraction        | 0.477       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.96        |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0371     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00248     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.722      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03861792 |\n",
      "|    clip_fraction        | 0.474      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.95       |\n",
      "|    explained_variance   | 0.948      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0527    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0471    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00213    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.715     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 791       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0424608 |\n",
      "|    clip_fraction        | 0.475     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 5.9       |\n",
      "|    explained_variance   | 0.948     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0292   |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | -0.0446   |\n",
      "|    std                  | 0.183     |\n",
      "|    value_loss           | 0.00209   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.725      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 785        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04126692 |\n",
      "|    clip_fraction        | 0.5        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.88       |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0401    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0469    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00184    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 799         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033490755 |\n",
      "|    clip_fraction        | 0.483       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.88        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0773     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00167     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.729       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 775         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053956717 |\n",
      "|    clip_fraction        | 0.505       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0449     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00165     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.726      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 782        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04874031 |\n",
      "|    clip_fraction        | 0.501      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.97       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00453    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0475    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.0015     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.729       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 763         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055190515 |\n",
      "|    clip_fraction        | 0.5         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.95        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0392     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0448     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00146     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.732       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 777         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042960916 |\n",
      "|    clip_fraction        | 0.51        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.98        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0223     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00154     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.733       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 763         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040994413 |\n",
      "|    clip_fraction        | 0.519       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.05        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0426     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00141     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.731       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 772         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052787006 |\n",
      "|    clip_fraction        | 0.52        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.1         |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0361     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00149     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.735      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05562014 |\n",
      "|    clip_fraction        | 0.522      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.16       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0338    |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00148    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.745      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 806        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04330797 |\n",
      "|    clip_fraction        | 0.526      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.18       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0416    |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.742      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 796        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05426056 |\n",
      "|    clip_fraction        | 0.524      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.22       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.066     |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00143    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 788         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058766235 |\n",
      "|    clip_fraction        | 0.529       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.25        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0623     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0453     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00137     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.756       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 778         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059461903 |\n",
      "|    clip_fraction        | 0.542       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.3         |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.058      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.0015      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.755      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 764        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05777074 |\n",
      "|    clip_fraction        | 0.54       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.32       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0785    |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0428    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00135    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.756      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 782        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04978593 |\n",
      "|    clip_fraction        | 0.536      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.33       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0563    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.757       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 759         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062838696 |\n",
      "|    clip_fraction        | 0.537       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.37        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0692     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.759      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 764        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06996098 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.44       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0146     |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0464    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.758       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 791         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062502824 |\n",
      "|    clip_fraction        | 0.56        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.49        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0129      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00133     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 780         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061955065 |\n",
      "|    clip_fraction        | 0.543       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.56        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.05       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.762      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05695828 |\n",
      "|    clip_fraction        | 0.546      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.6        |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.049     |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0408    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.762      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06161911 |\n",
      "|    clip_fraction        | 0.572      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.62       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0968    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0426    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00132    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.766      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06353586 |\n",
      "|    clip_fraction        | 0.567      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.67       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.099     |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0433    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.767       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 782         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061310746 |\n",
      "|    clip_fraction        | 0.567       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.76        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0591     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00118     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.768      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06703136 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.82       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0296    |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0435    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 765         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058580585 |\n",
      "|    clip_fraction        | 0.562       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.92        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0372     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00118     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.772       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 740         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061955355 |\n",
      "|    clip_fraction        | 0.577       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.97        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0639     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.774      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07403137 |\n",
      "|    clip_fraction        | 0.564      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.01       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0174    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.039     |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06143996 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.09       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0392    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.775       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 795         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058659125 |\n",
      "|    clip_fraction        | 0.568       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.12        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0905     |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    std                  | 0.173       |\n",
      "|    value_loss           | 0.00104     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 831        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05473997 |\n",
      "|    clip_fraction        | 0.572      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.16       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0595    |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0377    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.775       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 797         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061780415 |\n",
      "|    clip_fraction        | 0.579       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.21        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0407     |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0373     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 0.0011      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.778      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 763        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05757151 |\n",
      "|    clip_fraction        | 0.578      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.3        |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0709    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0382    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.779       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060521137 |\n",
      "|    clip_fraction        | 0.581       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.36        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0271     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00112     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.781       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078381255 |\n",
      "|    clip_fraction        | 0.584       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.41        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0373     |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0344     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00112     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.783      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 794        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06408231 |\n",
      "|    clip_fraction        | 0.578      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.49       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.065     |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0337    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.785      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06617008 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.58       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0353    |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0359    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.000982   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.788       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 785         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077911295 |\n",
      "|    clip_fraction        | 0.595       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.64        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0744     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00106     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.791       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 765         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076595135 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.71        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0614     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0358     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 0.00105     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 798        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07921113 |\n",
      "|    clip_fraction        | 0.592      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.8        |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0353    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0327    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 766        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07660602 |\n",
      "|    clip_fraction        | 0.581      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.87       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0598    |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0311    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.000992   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.793       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072735295 |\n",
      "|    clip_fraction        | 0.589       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.97        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0348     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 0.00102     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.793       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 792         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085904405 |\n",
      "|    clip_fraction        | 0.579       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.06        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.046      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 0.00104     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.794      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 770        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07603276 |\n",
      "|    clip_fraction        | 0.587      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.11       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0139    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.796       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055758476 |\n",
      "|    clip_fraction        | 0.578       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.16        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0563     |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 0.00103     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07241278 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.24       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0691    |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 755         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075302675 |\n",
      "|    clip_fraction        | 0.594       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.28        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0454     |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 0.000976    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.801       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 796         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063952446 |\n",
      "|    clip_fraction        | 0.58        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.33        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00757    |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 0.00102     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.803     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 753       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0598032 |\n",
      "|    clip_fraction        | 0.597     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.44      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00354   |\n",
      "|    n_updates            | 1080      |\n",
      "|    policy_gradient_loss | -0.0293   |\n",
      "|    std                  | 0.163     |\n",
      "|    value_loss           | 0.000958  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.805      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08018909 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.54       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0058     |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.000952   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.805      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 789        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07088951 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.6        |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0147    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.806       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 787         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066244096 |\n",
      "|    clip_fraction        | 0.596       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.63        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0116     |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.000968    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.808       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 796         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082179494 |\n",
      "|    clip_fraction        | 0.607       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.66        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0446     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.000942    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.808      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 765        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07475838 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.77       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0332     |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.000894   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.809      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 764        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08122401 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.87       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0466    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.000816   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.81        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 778         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079303786 |\n",
      "|    clip_fraction        | 0.607       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.94        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0253     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.000872    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.812       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 791         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061041016 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.01        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0402     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.159       |\n",
      "|    value_loss           | 0.000842    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 769        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09417999 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.11       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0398    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000855   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 785        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08384001 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.23       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0705    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0247    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.000758   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 800        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07549423 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.35       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0685    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0238    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000759   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 768        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07585373 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.5        |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00769    |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0277    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00074    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.815       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074367106 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.58        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0514     |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.155       |\n",
      "|    value_loss           | 0.000746    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 800        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07709063 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.66       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.027     |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000749   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 793        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09265313 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.75       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.012     |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000772   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.817     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 765       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0982478 |\n",
      "|    clip_fraction        | 0.618     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.82      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00386  |\n",
      "|    n_updates            | 1400      |\n",
      "|    policy_gradient_loss | -0.0267   |\n",
      "|    std                  | 0.153     |\n",
      "|    value_loss           | 0.000782  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.817       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 777         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083795145 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.89        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0225     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.000773    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.819     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 783       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0770528 |\n",
      "|    clip_fraction        | 0.614     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.98      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0441   |\n",
      "|    n_updates            | 1440      |\n",
      "|    policy_gradient_loss | -0.0225   |\n",
      "|    std                  | 0.152     |\n",
      "|    value_loss           | 0.000738  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 796        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08041523 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0452    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000688   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 778         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081069686 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0759     |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000731    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 749        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08976076 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0128    |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0274    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.00074    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 805        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10623851 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0614    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000652   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 789        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07858156 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000511  |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000626   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.823    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 808      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.078501 |\n",
      "|    clip_fraction        | 0.6      |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 10.4     |\n",
      "|    explained_variance   | 0.988    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0167  |\n",
      "|    n_updates            | 1560     |\n",
      "|    policy_gradient_loss | -0.0186  |\n",
      "|    std                  | 0.15     |\n",
      "|    value_loss           | 0.000689 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 764        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10261834 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0629    |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000674   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07960223 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0364    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000625   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.824       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 768         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060925603 |\n",
      "|    clip_fraction        | 0.606       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.5        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00685    |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 0.000627    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 797        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07956751 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0832    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000545   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 785        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08102087 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0383    |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000654   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "seed 3: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f236c0e5278> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f236c0b6da0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.837       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088500746 |\n",
      "|    clip_fraction        | 0.606       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.6        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0139     |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000652    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 573        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15740149 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0651    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0191    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 608        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08288424 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0323    |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 617        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10620625 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0271    |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0235    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07584275 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0377    |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0237    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.839       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 607         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104139075 |\n",
      "|    clip_fraction        | 0.623       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0654     |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.147       |\n",
      "|    value_loss           | 0.0011      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09516229 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0185    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 615         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101700805 |\n",
      "|    clip_fraction        | 0.613       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.1        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.086      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07540858 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0624    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08626471 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0587    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 643        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08964271 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00954    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 623        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08385065 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0211    |\n",
      "|    n_updates            | 1900       |\n",
      "|    policy_gradient_loss | -0.0242    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08263471 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0535    |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 639        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11201743 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0354    |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09150356 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0174    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10429158 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0444    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0251    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.84        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087693654 |\n",
      "|    clip_fraction        | 0.607       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0531     |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.00106     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 655        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10861623 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0306    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.028     |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09789217 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.033     |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000997   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09428374 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0411    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00095    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 612        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10934906 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0342    |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000979   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10016129 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00241   |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0231    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00093    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.841       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097322404 |\n",
      "|    clip_fraction        | 0.631       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.8        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0353     |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.000818    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10538765 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0565    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000929   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.841    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 630      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.084465 |\n",
      "|    clip_fraction        | 0.622    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 11.9     |\n",
      "|    explained_variance   | 0.986    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0689  |\n",
      "|    n_updates            | 2160     |\n",
      "|    policy_gradient_loss | -0.0205  |\n",
      "|    std                  | 0.141    |\n",
      "|    value_loss           | 0.000865 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10619807 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0281    |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000858   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "seed 3: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 31 x 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f236c065d30> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f23643b10b8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 112        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 22         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11911945 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000862   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 329        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12618704 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0107    |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00144    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 336        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10644741 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0445    |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10366185 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0541    |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09654635 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00263    |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12706012 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0333     |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083706714 |\n",
      "|    clip_fraction        | 0.623       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0557     |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.00139     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 341         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110688366 |\n",
      "|    clip_fraction        | 0.62        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0251     |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 0.00139     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11664897 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0241    |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.0191    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10960542 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0489    |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0242    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00135    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 340       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1077256 |\n",
      "|    clip_fraction        | 0.631     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.5      |\n",
      "|    explained_variance   | 0.979     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00946   |\n",
      "|    n_updates            | 2400      |\n",
      "|    policy_gradient_loss | -0.0186   |\n",
      "|    std                  | 0.137     |\n",
      "|    value_loss           | 0.00127   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08706713 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0174    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10627465 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0357    |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00129    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10281541 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0699    |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10349989 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0379    |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 344         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113111675 |\n",
      "|    clip_fraction        | 0.635       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0292     |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09581495 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0429    |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 332        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10995851 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0192    |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09565526 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0762    |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.851     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 342       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1317504 |\n",
      "|    clip_fraction        | 0.635     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.8      |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0602   |\n",
      "|    n_updates            | 2580      |\n",
      "|    policy_gradient_loss | -0.0181   |\n",
      "|    std                  | 0.135     |\n",
      "|    value_loss           | 0.00116   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103714034 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0455     |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08979169 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0681    |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10626767 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0664    |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10311641 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0347    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0202    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114243366 |\n",
      "|    clip_fraction        | 0.64        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0161     |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00112     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 341         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.115729704 |\n",
      "|    clip_fraction        | 0.636       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00203    |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00114     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=25,\n",
    "                                grid_fidelity_factor_array =[0.25, 0.5, 1.0],\n",
    "                                episode_limit_array=[50000, 50000, 50000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
