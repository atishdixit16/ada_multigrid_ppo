{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_1_singlegrid_quarter'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 3e-6,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.1,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[150,100,80], log_std_init=-2.9),\n",
    "                verbose = 1,\n",
    "                target_kl = 0.05,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/coarse_grid_functions.py:51: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1mFirst-class function type feature is experimental\u001b[0m\u001b[0m\n",
      "  for j in range(len(p_1)-1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f4572852a90> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f45728495c0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.59 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.594    |\n",
      "| time/              |          |\n",
      "|    fps             | 106      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.597       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015503553 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.236      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0926      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.598       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027701471 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.25       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.042       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.601      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 249        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03151405 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.308     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0617     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0244     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.604       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022714242 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0727      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0155      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019427568 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0616      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0115      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014360016 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0764      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00904     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013760949 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.038       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00822     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.611       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009902736 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0519      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0072      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.613       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008958871 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0713      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00705     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007303399 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0934      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0066      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009353575 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0451      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00661     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.621       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006472853 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0923      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00611     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.626        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046646623 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0553       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00591      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.63         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056614517 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0525       |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00583      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.633        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075015277 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0559       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00553      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009601568 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0705      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00538     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.639       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009925622 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0345      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007462728 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0642      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00497     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.645        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046749176 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0399       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00501      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.646        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052710087 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0601       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0049       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.649       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005696991 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0406      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00464     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007913312 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0556      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00486     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.653        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074240863 |\n",
      "|    clip_fraction        | 0.311        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0241      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0045       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005299613 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.654        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034764528 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0396       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00456      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.655        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046053766 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0441       |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0266      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00458      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.656       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005309331 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00422     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.657       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005764833 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0611      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006988463 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.062       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0041      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008306416 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0377      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.661     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 248       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0072301 |\n",
      "|    clip_fraction        | 0.351     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.878     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0676    |\n",
      "|    n_updates            | 620       |\n",
      "|    policy_gradient_loss | -0.0269   |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.00394   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006913173 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0791      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00377     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006545782 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0379      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008365149 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0378      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00402     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009714067 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00374     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.664        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053011654 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0696       |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006423101 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0973      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00365     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.666        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052858116 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0912       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0262      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00378      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007949164 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0531      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00374     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002187717 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0781      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006781551 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0703      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004051268 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0496      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00367     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062743486 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0691       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00359      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004979509 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0572      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058360607 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0565       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00355      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027345777 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.047        |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0258      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00338      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.673       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004990545 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0782      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00346     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046329917 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0614       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00341      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.673       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004152441 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.065       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004351902 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0915      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00338     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007312745 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0508       |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00335      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052306233 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0626       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00335      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064719645 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.04         |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0259      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00331      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065525533 |\n",
      "|    clip_fraction        | 0.332        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0454       |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0256      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00327      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.676       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004936126 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0398      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0033      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.677      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 250        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00893417 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.897      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0549     |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00346    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068483264 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0354       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00339      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008488083 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00299     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010516125 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0437      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0032      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004216942 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0398      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00329     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065822066 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0401       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00337      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.678         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 250           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0015205741 |\n",
      "|    clip_fraction        | 0.352         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.903         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0394        |\n",
      "|    n_updates            | 1240          |\n",
      "|    policy_gradient_loss | -0.028        |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.0032        |\n",
      "-------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004209116 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0686      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00339     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038013817 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0996       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00319      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049222945 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0741       |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00302      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 255         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008699736 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.046       |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076989145 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0494       |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00315      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038225024 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0487       |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00309      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 240          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051201284 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0517       |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00308      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006335014 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0537      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058701574 |\n",
      "|    clip_fraction        | 0.38         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.047        |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00303      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008667147 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0424      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00314     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029282123 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0572       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00328      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011079729 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0495      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005433315 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0665      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00341     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008448789 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0496      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00314     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038493066 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0403       |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.0262      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00321      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009438509 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0729      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00308     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054321648 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0565       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00302      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008461821 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0707      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008116079 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0452      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00328     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056304187 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0602       |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.0263      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00321      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006671718 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0554      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00303     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 254          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069763004 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0352       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00315      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007056549 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0536      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00291     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065945536 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.029        |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0029       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038998842 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0772       |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00299      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008294767 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0606      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00291     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007416576 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.043       |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0029      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008757383 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009507966 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0912      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009003803 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0989      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00303     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066122115 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0516       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00315      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004652715 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009355774 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0572      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00281     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075594606 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0523       |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00283      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044002025 |\n",
      "|    clip_fraction        | 0.385        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0404       |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00291      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004515782 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0812      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00295     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004181826 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0806      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051917555 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0399       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.026       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00293      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081857145 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0453       |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0028       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006694746 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0451      |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00285     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076762973 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0408       |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00307      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066503943 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0411       |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.003        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009569365 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0388      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00279     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010091531 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009478992 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0685      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00282     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004431936 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0679      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002911204 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0301      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00285     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009801889 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0762      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00285     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 252        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00803495 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.918      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0509     |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00277    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006077385 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0462      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0026      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079167755 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0605       |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00305      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055221645 |\n",
      "|    clip_fraction        | 0.374        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0507       |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.003        |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008319348 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0576      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00253     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007280135 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0774      |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00287     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039933473 |\n",
      "|    clip_fraction        | 0.384        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.066        |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00271      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072856396 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0428       |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00263      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009339884 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0677      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00271     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009866047 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0684      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00277     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012622258 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0448      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007302657 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0472      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 250        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01271157 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.919      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0439     |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.0549     |\n",
      "|    value_loss           | 0.00268    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009284911 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0289      |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00287     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010969537 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.058       |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00271     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003094533 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0675      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00273     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009777593 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0648      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00272     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071327775 |\n",
      "|    clip_fraction        | 0.374        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0829       |\n",
      "|    n_updates            | 2560         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00285      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027329712 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0535       |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00288      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023319393 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0562       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.0263      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00276      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061586946 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0828       |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00273      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047443868 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0659       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00265      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053496836 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0469       |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.0029       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004653764 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.103       |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00287     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056616575 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0659       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00251      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008349055 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00273     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011001887 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0517      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00264     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006057483 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0641      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00262     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013009104 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0714      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00288     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060969233 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 92           |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0526       |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00275      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008395013 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0691      |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00276     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073969127 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0558       |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00265      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.681      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 248        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00466629 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.927      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0765     |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    std                  | 0.0549     |\n",
      "|    value_loss           | 0.00257    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063234093 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0964       |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00265      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007660216 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00264     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082832035 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 92           |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0549       |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.0027       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f4572446908> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f45725dc588>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006592798 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 92          |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.075       |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00289     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.601      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 236        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03261771 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.476     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0475     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0641     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.605       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034250923 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.18       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0708      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0378      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.606      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 239        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03983616 |\n",
      "|    clip_fraction        | 0.381      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.466     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0539     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0229     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.607       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029545229 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0502      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0144      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.614       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027530098 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0253      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0101      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.615      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 233        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01951104 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.683      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0783     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00806    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.619       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014926551 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0429      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00722     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.62       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 231        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01039381 |\n",
      "|    clip_fraction        | 0.336      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.799      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.049      |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0251    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00626    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.624     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 233       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0096928 |\n",
      "|    clip_fraction        | 0.339     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.81      |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0403    |\n",
      "|    n_updates            | 180       |\n",
      "|    policy_gradient_loss | -0.0264   |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00608   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.629       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010694569 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0714      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00589     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008935439 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00578     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.633       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010327518 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0663      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00572     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006420663 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00511     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.637       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009483946 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0614      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00522     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010879433 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00512     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.641        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 236          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070581674 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0589       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00514      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010783603 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00473     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.645        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 234          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082113175 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0502       |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00519      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.647       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008283702 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0047      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.65         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 235          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073689194 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0674       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00484      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.652      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 237        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00844332 |\n",
      "|    clip_fraction        | 0.34       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.857      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.066      |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00475    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.654     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 233       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0096127 |\n",
      "|    clip_fraction        | 0.346     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.862     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0901    |\n",
      "|    n_updates            | 440       |\n",
      "|    policy_gradient_loss | -0.027    |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00468   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.656        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029226064 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0687       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0245      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00458      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007250157 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.065       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0044      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 233          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056521357 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0729       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00453      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009899834 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0928      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00457     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006917706 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.041       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00445     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.662       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005110082 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0042      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.662        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048077675 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0552       |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00413      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.663        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 240          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069179265 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0559       |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00414      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.663         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 237           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059212744 |\n",
      "|    clip_fraction        | 0.337         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.871         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0571        |\n",
      "|    n_updates            | 620           |\n",
      "|    policy_gradient_loss | -0.0272       |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.00419       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007168728 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0417      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00414     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007922521 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.063       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008942589 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0364      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004092327 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0511      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005412662 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0543      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00367     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006316277 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0589      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0038      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 240          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059427978 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0903       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00377      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005376813 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0709      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006884399 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.078       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0037      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005267979 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0539      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049629183 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0511       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00367      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050691008 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0521       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00351      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002076575 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0928      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 237          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044919522 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0481       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00381      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007311416 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0612      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0037      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070066154 |\n",
      "|    clip_fraction        | 0.329        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0457       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0256      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0035       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 240          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053995075 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0741       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00342      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005432823 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.055       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077956705 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0505       |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00372      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 237          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064368905 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0475       |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00362      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.674        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068247644 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0492       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00322      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 240          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044704615 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0614       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00325      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042011053 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0444       |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00334      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008785838 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0584      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00345     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 238          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070462585 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0482       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00327      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066428543 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0369       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00337      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.676       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006872046 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0768      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052446933 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0612       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00337      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005143562 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0489      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00337     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064593432 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0779       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00326      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050736247 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.058        |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008871913 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0604      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00338     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005732241 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0493      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00308     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056735813 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0673       |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00304      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061299414 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0507       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0254      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00322      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008273654 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0395      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00312     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054620774 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0494       |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.0266      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006159544 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00307     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080130305 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0883       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00328      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003030309 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0528      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049013137 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0472       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069287717 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0519       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00308      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.678     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 245       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0077068 |\n",
      "|    clip_fraction        | 0.363     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.905     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0807    |\n",
      "|    n_updates            | 1480      |\n",
      "|    policy_gradient_loss | -0.0291   |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00326   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011015216 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0583      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00322     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006633258 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0505      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007168719 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0696      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006165576 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0526      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00288     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035113753 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0507       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00275      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049883993 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0579       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00274      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038845004 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0651       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00302      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063794404 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0365       |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046669366 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.073        |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00291      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008348564 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0468      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00277     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005132842 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0554      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00265     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050162286 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0753       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00285      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053512393 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0539       |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0027       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004576084 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0594      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0028      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064362646 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.034        |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00282      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 248        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00639154 |\n",
      "|    clip_fraction        | 0.369      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.921      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0552     |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0296    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00272    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061843246 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0483       |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00283      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005265364 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0478      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00275     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 240          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056934743 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0494       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00272      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006719342 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0998      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0028      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063637435 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.049        |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00274      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073007615 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0609       |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0026       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071420283 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0545       |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00271      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008676159 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0526      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00272     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063130734 |\n",
      "|    clip_fraction        | 0.387        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0455       |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0027       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 243        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00570724 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.918      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0394     |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00281    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010444647 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0593      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00254     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056991293 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0424       |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0026       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064439746 |\n",
      "|    clip_fraction        | 0.384        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0552       |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00276      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009033928 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0724      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00272     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077750357 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0592       |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00268      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005702329 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0627      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00253     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006208244 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0819      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0026      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070924847 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.059        |\n",
      "|    n_updates            | 2160         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00279      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007881865 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0497      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00278     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009766045 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0372      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00267     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007229051 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.053       |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00251     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054631024 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0591       |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00252      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008066103 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0615      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00273     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.681      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 249        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00823533 |\n",
      "|    clip_fraction        | 0.367      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.922      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0721     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0284    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00266    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097615095 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.928        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0642       |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00251      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108100325 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0506       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00245      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009380737 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0753      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00249     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054932656 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.931        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.065        |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00242      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057725757 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.117        |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00262      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007369903 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0631      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0026      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069103837 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0339       |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00258      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 240          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025723607 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0622       |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00262      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0099954875 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0717       |\n",
      "|    n_updates            | 2460         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00275      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063100965 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0662       |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00255      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008116332 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0461      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0025      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007907325 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0595      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00255     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010432807 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0854      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00259     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007288697 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0728      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00253     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008486646 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0693      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00261     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053477944 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.057        |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00251      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053088395 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.93         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0763       |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00243      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056134434 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0702       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00259      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062907785 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0866       |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00257      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077727735 |\n",
      "|    clip_fraction        | 0.374        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0705       |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00267      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050911247 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.933        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0731       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00238      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006635584 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0421      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00236     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.681      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 248        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00570108 |\n",
      "|    clip_fraction        | 0.393      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.932      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0328     |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0307    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00233    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062442273 |\n",
      "|    clip_fraction        | 0.384        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0393       |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00249      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045532854 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.928        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0914       |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0025       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047994615 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.93         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0738       |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00246      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061730803 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.93         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0629       |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00244      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050493507 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.931        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0622       |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00238      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010973344 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0482      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00235     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048305006 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.936        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0714       |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.0258      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00226      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.682      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00688124 |\n",
      "|    clip_fraction        | 0.381      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.926      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0632     |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.0292    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00247    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.682      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 243        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01088543 |\n",
      "|    clip_fraction        | 0.39       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.93       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0558     |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0284    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00238    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f457244fef0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f4524280358>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.599       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006841746 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0593      |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00241     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008441448 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0678      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0827      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.603       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036776222 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.4        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0878      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0336      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.605       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037886694 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.371      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0884      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0203      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.609       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031634964 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.084       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0126      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.609       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023138948 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.575       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0328      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00905     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.612      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 235        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01902612 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.723      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.052      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00683    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.615       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014740577 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.072       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00615     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011999324 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0322      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00588     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009492916 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0525      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00543     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.623       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010967347 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0583      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00508     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.623       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007900062 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.089       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00504     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.623        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061360775 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0397       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.026       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00504      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.625       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011871862 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0628      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00499     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.626       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014109999 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0648      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.627      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 246        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00510236 |\n",
      "|    clip_fraction        | 0.334      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.866      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.042      |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00438    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.63         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060845404 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0552       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0245      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00456      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.633       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012400851 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0664      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00404     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.637       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012367246 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0336      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00426     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.639        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 237          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058953343 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0582       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0262      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00415      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.641       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003532982 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0329      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00401     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.641       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008809748 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0711      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.642        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064934194 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0512       |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00381      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.645       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008403584 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0407      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.646       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007718435 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0752      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00406     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.647        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0111936275 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.081        |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0245      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00384      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.649       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006145814 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0589      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005348435 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.059       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004923704 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0414      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00333     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.654        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023266436 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0618       |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00346      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.656       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011344736 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0548      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00362     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 238          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042720973 |\n",
      "|    clip_fraction        | 0.332        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0439       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0242      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00353      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 5              |\n",
      "|    mean_reward          | 0.659          |\n",
      "| time/                   |                |\n",
      "|    fps                  | 246            |\n",
      "|    iterations           | 1              |\n",
      "|    time_elapsed         | 10             |\n",
      "|    total_timesteps      | 2560           |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00017822385 |\n",
      "|    clip_fraction        | 0.35           |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | 91.8           |\n",
      "|    explained_variance   | 0.9            |\n",
      "|    learning_rate        | 3e-06          |\n",
      "|    loss                 | 0.0868         |\n",
      "|    n_updates            | 640            |\n",
      "|    policy_gradient_loss | -0.0261        |\n",
      "|    std                  | 0.0551         |\n",
      "|    value_loss           | 0.00332        |\n",
      "--------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083399415 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0557       |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.661        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047372817 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0375       |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.0259      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00346      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.664        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036762594 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0498       |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00309      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.665        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028889328 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0542       |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0255      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00334      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.666        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064543905 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0357       |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.0256      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00329      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011148497 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00318     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008196855 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0698      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011829317 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069169314 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.056        |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00337      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005862546 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0452      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0032      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050961403 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0745       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00314      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058733253 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0381       |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00323      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017754287 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0585       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00326      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.67          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 244           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0002821833 |\n",
      "|    clip_fraction        | 0.363         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.912         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0664        |\n",
      "|    n_updates            | 920           |\n",
      "|    policy_gradient_loss | -0.028        |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.00301       |\n",
      "-------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072675883 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0342       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061362833 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0765       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00316      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065400093 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0587       |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0031       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 236          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0124492645 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0519       |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00313      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008941963 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0619      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006332445 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.077       |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036290647 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0723       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0254      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00292      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015626848 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0751       |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00307      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003862071 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.674        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038417191 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0619       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0029       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.674        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020778417 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0566       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00301      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.674        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045217066 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0882       |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00286      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.674      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 248        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00310359 |\n",
      "|    clip_fraction        | 0.346      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.915      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0553     |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0264    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00286    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.673       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007734853 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0406      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00305     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070697577 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0468       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00313      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.673       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006059545 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0469      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00275     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008246323 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00288     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002224502 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0532      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00288     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004053429 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0514      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00269     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009686893 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0831      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00305     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056568547 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0477       |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00281      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007617122 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.092       |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00277     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035754174 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0592       |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0028       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054401965 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0913       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0253      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00282      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.676       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007355654 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0354      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00276     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004625976 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0475       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00269      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053565144 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0561       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00288      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055996925 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0905       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00295      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071042986 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.076        |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00283      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008633685 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0719      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00268     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064302175 |\n",
      "|    clip_fraction        | 0.382        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0567       |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00277      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048030736 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0597       |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.0249      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00267      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006064835 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0645      |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00266     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 240          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075011374 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0564       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00252      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053663016 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0676       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0254      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00287      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006365505 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0593      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00271     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009886676 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0362      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0027      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033530951 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0478       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00274      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032480597 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0549       |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00254      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063271313 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0353       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00273      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006718844 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0417      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00253     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008888384 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0808      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00261     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033498749 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0539       |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00259      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004750678 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00267     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054523377 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0567       |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00257      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005199328 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0484      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0025      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048244325 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0644       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0246      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00266      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005301133 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0538      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00245     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008078694 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0384      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00249     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005399853 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0494      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00245     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008888448 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0416      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00247     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008499673 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0629      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00264     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006791705 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0841      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00253     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060374946 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0707       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00256      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008993533 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0706      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0024      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008317322 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.063       |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00245     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008149177 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00248     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052992105 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.928        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0581       |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00248      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 238          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075627565 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.931        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0418       |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00241      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.679      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 245        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00786986 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.932      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0883     |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00237    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005238107 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0455      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00255     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009858686 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0777      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00255     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008603525 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0783      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00263     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008110935 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0697      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00254     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004332957 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0577      |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00226     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006238499 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0739      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00228     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006220305 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00252     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064668087 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.928        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0556       |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.0253      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00249      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056224554 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.932        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.047        |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00238      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007894832 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0449      |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00236     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073824646 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.938        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0529       |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00216      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006710455 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0439      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00219     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010893801 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0604      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00242     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005147314 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0403      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00252     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048866095 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.932        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0444       |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00237      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010630188 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0406      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00238     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008646807 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0614      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00231     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008965668 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0713      |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00225     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009241223 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0391      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00226     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 238          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032077848 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.94         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0477       |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.0263      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00215      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009876209 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0538      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007890457 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0564      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00235     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008438257 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0806      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00243     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007909107 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0763      |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00226     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012306139 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0416      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00226     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010235315 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0679      |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00229     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010850149 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0507      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0024      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.681     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 240       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0093608 |\n",
      "|    clip_fraction        | 0.367     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.9      |\n",
      "|    explained_variance   | 0.93      |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0874    |\n",
      "|    n_updates            | 2680      |\n",
      "|    policy_gradient_loss | -0.0269   |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00245   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010780787 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0417      |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00223     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006664425 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0432      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00211     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008033464 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0459      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00219     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009917105 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00227     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031258315 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.933        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0576       |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.0256      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00232      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040003536 |\n",
      "|    clip_fraction        | 0.392        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.939        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0384       |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00217      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006669858 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0543      |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00226     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008960095 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0704      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0021      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034872354 |\n",
      "|    clip_fraction        | 0.386        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0494       |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00221      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006715241 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0366      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00218     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 227          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075958846 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.933        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.066        |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00228      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006379354 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0432      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00211     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[0.25],\n",
    "                                episode_limit_array=[75000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
