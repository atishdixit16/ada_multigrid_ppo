{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_1_singlegrid_quarter'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 3e-6,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.1,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[150,100,80], log_std_init=-2.9),\n",
    "                verbose = 1,\n",
    "                target_kl = 0.05,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fbdbe8ca2e8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fbdbcc850f0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.59 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.594    |\n",
      "| time/              |          |\n",
      "|    fps             | 200      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.597       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015503636 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.236      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0926      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.598       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027700674 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.25       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.042       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.601      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 210        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03151442 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.308     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0617     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0244     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.603       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022731388 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0769      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0155      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.609      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02047947 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.504      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0564     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0271    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0115     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014205614 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0738      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00904     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014776167 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0347      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00822     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.611       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009310255 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0572      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0072      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.614       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008558616 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0711      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00705     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007448086 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0936      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00659     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009115359 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0507      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0066      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.621        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067782523 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0935       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0253      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00611      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.625        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037248284 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0557       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00591      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005490628 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0515      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00583     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.632        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065607997 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0635       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0258      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00553      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.637       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009056427 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0688      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00539     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009073767 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0319      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.644       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007067797 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0598      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00497     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.647        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 203          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052206665 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0394       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00499      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.647        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052545993 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0596       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00489      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004952848 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0499      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00463     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.652        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075270445 |\n",
      "|    clip_fraction        | 0.335        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0535       |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0253      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00485      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.654        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 202          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073877336 |\n",
      "|    clip_fraction        | 0.311        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0432       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0242      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00448      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.654        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060952753 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0674       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00436      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.655        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 203          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019960166 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0468       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00455      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.656        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052233576 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.037        |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00457      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.657       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006204811 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0678      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0042      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.658        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062398524 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0516       |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00407      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.66       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00753358 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.877      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0548     |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00408    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008065457 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.037       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00393     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008026439 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0636      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00392     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007908275 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0739      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00376     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.661      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00624949 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0381     |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00386    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008521202 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0341      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.004       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009663308 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00372     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005313185 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0708      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00371     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005771786 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0991      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00363     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005723181 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0959      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071375193 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0464       |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022626133 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0776       |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063076317 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0548       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039473595 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0501       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00365      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006813541 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0724      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005300158 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0597      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00355     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056967945 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0591       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00353      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030656694 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0258      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00336      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058521954 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.079        |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00344      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055377274 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.059        |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00339      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042487443 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0587       |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00314      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004626161 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0998      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014134794 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0523       |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00334      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006602481 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00334     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006882551 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0393      |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00329     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068489313 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0468       |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00325      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040783556 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0392       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0263      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00328      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008221057 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0669      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00345     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065386416 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0364       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00337      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008700815 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0467      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00298     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.678      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01025177 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.905      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0415     |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0303    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00319    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049066483 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0399       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00329      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007835656 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.679         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 210           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8967856e-06 |\n",
      "|    clip_fraction        | 0.354         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.904         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0422        |\n",
      "|    n_updates            | 1240          |\n",
      "|    policy_gradient_loss | -0.0281       |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.00318       |\n",
      "-------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039886297 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0593       |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.026       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00336      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.678      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 214        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00416435 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.908      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.104      |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00318    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066307215 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0712       |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00301      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008651048 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00334     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075731934 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0442       |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00313      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044481517 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0482       |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00308      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050039203 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0463       |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00306      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006487532 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0609      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00307     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064181476 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0479       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00302      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008187601 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0407      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00312     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003269437 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0537      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00325     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011181128 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00318     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055038095 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0672       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00339      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008005291 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00312     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003939572 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0416      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009151036 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0748      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00306     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057017743 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0538       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00301      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009265465 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0624      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007336679 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0328      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00326     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065843104 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0582       |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.0263      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006698507 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0543      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00303     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069742976 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0353       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00313      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076150326 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0515       |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00288      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006791231 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0323      |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00287     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004165971 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0774      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008317274 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0546      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00289     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007523045 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0469      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00289     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007872629 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0471      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 211        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01032806 |\n",
      "|    clip_fraction        | 0.369      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.908      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0924     |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00308    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 217         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009782597 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0878      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 216          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071295025 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0514       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00313      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004271099 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0375      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0028      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009022313 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0523      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0028      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082023535 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0464       |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00282      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042429743 |\n",
      "|    clip_fraction        | 0.386        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0468       |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00289      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 215          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049730064 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0677       |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00293      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005203697 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.078       |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 213        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00463098 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.915      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0405     |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00292    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008350149 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0511      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00278     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 217         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005247274 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0414      |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00282     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 215          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070633083 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0421       |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00305      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006781471 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0364      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00299     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009703982 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0379      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00277     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 214          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090463925 |\n",
      "|    clip_fraction        | 0.38         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.052        |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00295      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009511334 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0603      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00282     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 214          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044324994 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0695       |\n",
      "|    n_updates            | 2160         |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.003        |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040238113 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0276       |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00284      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 210        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00966779 |\n",
      "|    clip_fraction        | 0.367      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.918      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0776     |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.028     |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00282    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008694449 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.045       |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00275     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005908355 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0459      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00258     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008864725 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0514      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006637329 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0497      |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008279184 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0597      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00252     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 214          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065402417 |\n",
      "|    clip_fraction        | 0.387        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0772       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00285      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 217        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00447557 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.921      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0699     |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0287    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0027     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007264629 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0429      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00262     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009339777 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0683      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00269     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009919494 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0719      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00276     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012013653 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.043       |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00281     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006995541 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0496      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00281     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012815893 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0411      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00265     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 217         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008619726 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.032       |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00285     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010743883 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0566      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00268     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031369925 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0647       |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00272      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009658185 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0658      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.0027      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007477236 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0818      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030129552 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0511       |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00286      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022007017 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0464       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.0263      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00275      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048770695 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0939       |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00272      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004035026 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0584      |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00264     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.681      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00639472 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.915      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0438     |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.0549     |\n",
      "|    value_loss           | 0.00288    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 215          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051079094 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.101        |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.0262      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00285      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 216          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043556453 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.928        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0676       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.0025       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 217         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009221718 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0538      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00271     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010649572 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 92          |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0548      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00263     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056295125 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0638       |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.0026       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012414575 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 92          |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0765      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00286     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006038827 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 92          |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0493      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00273     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007898438 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0666      |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00274     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007498124 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0548      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00263     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 217          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047894595 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.928        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0725       |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00256      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 214          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072921813 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0923       |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00263      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 219          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060625225 |\n",
      "|    clip_fraction        | 0.387        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 92           |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0562       |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00262      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007759595 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 92          |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0534      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00269     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fbdbc039da0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fbd64186320>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006082666 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 92          |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0793      |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00287     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032633513 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.476      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0641      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.604       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034693487 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.17       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0698      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0378      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.606      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 203        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04002426 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.465     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0603     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0229     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.607       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029365933 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0538      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0144      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.613       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027726144 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.505       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0259      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0101      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019373255 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0772      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00806     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015355974 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00722     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009740907 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00626     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.624       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009541893 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0371      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00609     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.629       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009406346 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0778      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00591     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009039444 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0502      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0058      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.633       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009475094 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0641      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00575     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.636        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063322126 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0413       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0246      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00512      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008937001 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0591      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00524     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009291222 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0626      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.64         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072204424 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0607       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00515      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011285245 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.065       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00474     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.646      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 204        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00805054 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.838      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0491     |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0052     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.648        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 203          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074118166 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0553       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00471      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.651        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068108765 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0654       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00486      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007865369 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0672      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00477     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.655       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009314701 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0887      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00469     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027735978 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.071        |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0244      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00459      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.659        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075268834 |\n",
      "|    clip_fraction        | 0.332        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0676       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0254      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00442      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055933953 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0719       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00455      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009925294 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.091       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00459     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077251345 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0371       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0256      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00446      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.662       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004445684 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.056       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00421     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.663        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050566616 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.059        |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00414      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007718733 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.051       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00416     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.663         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 209           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013876855 |\n",
      "|    clip_fraction        | 0.339         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.871         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0604        |\n",
      "|    n_updates            | 620           |\n",
      "|    policy_gradient_loss | -0.0272       |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.0042        |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.664        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064074188 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0431       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00417      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007404676 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0592      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00395     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008397022 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0344      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00409     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004732102 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0519      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006041235 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00368     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007111603 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0592      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054844827 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0877       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0266      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00377      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050089536 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0787       |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00383      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063805105 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0779       |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00371      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054797963 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.049        |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00352      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005155808 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0586      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00368     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005201557 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025882989 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.085        |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00358      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005924487 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008378643 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00372     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007880524 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0552      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004336402 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0759      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00344     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064620106 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0518       |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00376      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.673       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007309258 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00373     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073943734 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0443       |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00363      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.674        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071524857 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0452       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00323      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044331043 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0672       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00327      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.675      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00372473 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.903      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0448     |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00336    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008348691 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0651      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00348     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077958107 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.053        |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0033       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 215          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072381767 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0357       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0034       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 204          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055255233 |\n",
      "|    clip_fraction        | 0.374        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0716       |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00318      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043316693 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0689       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00339      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004455635 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00338     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068101613 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0811       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00327      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005769321 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0553      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00298     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.678      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 211        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00929797 |\n",
      "|    clip_fraction        | 0.363      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.901      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0633     |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0271    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00339    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005861056 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0471      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0031      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005691892 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.067       |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00305     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004599273 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0557      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00324     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008690244 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00314     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005457625 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00298     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005836828 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008745414 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0893      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0033      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022833496 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.06         |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00309      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005525249 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077300817 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0524       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00311      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008045179 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0753      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00327     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011007348 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0602      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00322     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077029346 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0498       |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00304      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058841407 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0714       |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00303      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006342554 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0522      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00289     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034545541 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0456       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00277      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050207796 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0582       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00276      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003251195 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0579      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00304     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006578383 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0408      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046030907 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0658       |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00293      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007646018 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00279     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005679062 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00267     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005502614 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.065       |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00287     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066644205 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0513       |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00273      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039938716 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0473       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00283      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071226684 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0305       |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00285      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063762935 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0488       |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00274      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 216          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064574094 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.044        |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00287      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004456258 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0499      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00278     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005464098 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0507      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00275     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 216          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060224473 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.1          |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00283      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071006627 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0522       |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00279      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 215          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068958523 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0566       |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00262      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 217         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007382205 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.054       |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00274     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008928841 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00276     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056792526 |\n",
      "|    clip_fraction        | 0.385        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0495       |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.0308      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00273      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060195653 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0413       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00283      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010835797 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0563      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 214          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060962453 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.047        |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00261      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006692243 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00278     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008780968 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0727      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00276     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008707589 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0691      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00271     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 214          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058903424 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0594       |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00256      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006756082 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0823      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00263     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006040162 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0624      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008539641 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0474      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00282     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095676035 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0366       |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00272      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064846873 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0565       |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00256      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005032888 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0574      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00257     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077402233 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0684       |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00279      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008544415 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0657      |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00271     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009451712 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0783      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010596353 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0025      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009469405 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0814      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00254     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053860694 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.93         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0662       |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00246      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006467378 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00268     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007848379 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0665      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00265     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007069546 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0378      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00262     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 218          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029216378 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0549       |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00266      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009990561 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0684      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0028      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 215          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060983985 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0701       |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0026       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008821604 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008056775 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0612      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00261     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010013536 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0833      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00265     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 218          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066731037 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0833       |\n",
      "|    n_updates            | 2560         |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00258      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008119887 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0726      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00266     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005524659 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0537      |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006355858 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0766      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00247     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006177512 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0703      |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00264     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006549713 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0914      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0026      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070470246 |\n",
      "|    clip_fraction        | 0.374        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0657       |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00272      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051653953 |\n",
      "|    clip_fraction        | 0.374        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.932        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0732       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00241      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 218         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006485319 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0462      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00238     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005225739 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0295      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00235     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 203          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060337097 |\n",
      "|    clip_fraction        | 0.384        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.041        |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00253      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 217          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040153502 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0928       |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00253      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004041222 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0753      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00249     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062089623 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.93         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0641       |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00246      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 214          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041067004 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0665       |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00243      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010903153 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.05        |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0024      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005140123 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0661      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0023      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 214          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062965066 |\n",
      "|    clip_fraction        | 0.38         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0724       |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00251      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011541098 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0615      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00243     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fbd641ca710> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fbd503dba20>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.599       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006369002 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00245     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008611363 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0645      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0827      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.603       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036471147 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.39       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0906      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0336      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.605     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 205       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 12        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0380391 |\n",
      "|    clip_fraction        | 0.38      |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | -0.375    |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0909    |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | -0.0214   |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.0203    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.609      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 202        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03127714 |\n",
      "|    clip_fraction        | 0.378      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.335      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0802     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0126     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.609       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023681339 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.574       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0301      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00905     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.611     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 206       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 12        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0186771 |\n",
      "|    clip_fraction        | 0.343     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.725     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0562    |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | -0.023    |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00684   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.615       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014332309 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00616     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011970257 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0357      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00588     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.618      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 206        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00867331 |\n",
      "|    clip_fraction        | 0.345      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.83       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0539     |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00542    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.622        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112716975 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0608       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0231      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00508      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.622       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008531893 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0873      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00503     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.623        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048460127 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0378       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0259      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00503      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.624       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011932664 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00498     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.625        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0128508685 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0674       |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0044       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.626       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005663827 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.629        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 203          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067400276 |\n",
      "|    clip_fraction        | 0.334        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0621       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0246      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00455      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.632       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012890858 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0638      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00404     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011975783 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00426     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005647713 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.05        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00416     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.64         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044877767 |\n",
      "|    clip_fraction        | 0.327        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0331       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0244      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00401      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009833497 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0757      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.641      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 200        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00793908 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0493     |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0264    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00382    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008342582 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0405      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.644        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075665624 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0725       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0253      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00406      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.646      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01218692 |\n",
      "|    clip_fraction        | 0.338      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0854     |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00384    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.647       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005613193 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0626      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.649        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 203          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057478277 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0617       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00356      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005415781 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0453      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00333     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.652       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003361559 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0661      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00346     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.655       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010581171 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0535      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00362     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.656       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005057818 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0447      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00353     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007322341 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0822       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00333      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007673341 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0606      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051002414 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0404       |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.0259      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00347      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.662        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038924187 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0474       |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0031       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002762404 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00334     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007065469 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.045       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00329     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.665      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 210        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01055531 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.903      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0535     |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00318    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009018359 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0694      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011550871 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0637      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007347724 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0561      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00337     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063210875 |\n",
      "|    clip_fraction        | 0.329        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0419       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.024       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056840717 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0728       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00315      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063381107 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.031        |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00323      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023716062 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0624       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00326      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.669         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 211           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039167405 |\n",
      "|    clip_fraction        | 0.36          |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.912         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0605        |\n",
      "|    n_updates            | 920           |\n",
      "|    policy_gradient_loss | -0.0278       |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.003         |\n",
      "-------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008696845 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0382      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005884564 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.074       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006482959 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012495428 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0522      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00312     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008238286 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0662      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006187844 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0784      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042170943 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.069        |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0254      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00291      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 215          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024647564 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0747       |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00306      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.673       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003958121 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0453      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038508356 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0626       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0029       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018829138 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.06         |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00301      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047436683 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0992       |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00286      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037748008 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0531       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00286      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0089328885 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0427       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00305      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073793205 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0428       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00314      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073053776 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0489       |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00274      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007777524 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0514      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00288     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.674        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019762814 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0442       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00288      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.674        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045235814 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.055        |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00268      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.674        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097363265 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0834       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.025       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00305      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056630014 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0481       |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0028       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069876937 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0883       |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00277      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038655936 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0524       |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0028       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004837534 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0771      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.676       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008145491 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0336      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00276     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007976681 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0475       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0027       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005549374 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00289     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053118705 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0947       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00296      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061981557 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0717       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00284      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009129429 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0674      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00269     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066836895 |\n",
      "|    clip_fraction        | 0.383        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0612       |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00279      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 214          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049622864 |\n",
      "|    clip_fraction        | 0.327        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0617       |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.0249      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00269      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051318915 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0652       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00267      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062909545 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.053        |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00253      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049633505 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0638       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0256      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00289      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005899015 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0654      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00272     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011088848 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0394      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00271     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031046956 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0443       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00275      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 215          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037950545 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0544       |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00256      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 214          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064312876 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0372       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00276      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007535875 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0446      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00254     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008909893 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0785      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00263     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003691405 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0582      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00262     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.679      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 211        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00525521 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.922      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0964     |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.027     |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00269    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005358276 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0026      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 214          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040357234 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.928        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0429       |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00251      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058240383 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0735       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0246      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00267      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005194989 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0547      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00246     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 215          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077613266 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0417       |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0025       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006266743 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.052       |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00245     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008213553 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0492      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00249     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 218         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008664643 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0627      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00267     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 215          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075514675 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0888       |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00254      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006068605 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0667      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00257     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 216          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076377303 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.93         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0579       |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00242      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.679      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00720323 |\n",
      "|    clip_fraction        | 0.381      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.929      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0672     |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0292    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00246    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094794575 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.928        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0532       |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00249      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 217          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056858184 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0583       |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.0263      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0025       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 214          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070080487 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.93         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0458       |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00242      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008105988 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0772      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00238     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006868577 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0373      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010268691 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0746      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00257     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009620828 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0786      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00265     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007570109 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0723      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005418375 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0591      |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00227     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062673003 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.932        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0741       |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0023       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058599235 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.042        |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00253      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006210494 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0569      |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00251     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056250454 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.931        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0514       |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00239      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008539528 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0462      |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00238     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008232936 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0492      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00218     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006236595 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0529      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00221     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010607168 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0618      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00245     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 203          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054062577 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0411       |\n",
      "|    n_updates            | 2400         |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00255      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00418621 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.931      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.045      |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00239    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009711894 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00241     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009366441 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0651      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00234     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009104448 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0718      |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00227     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008530629 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0408      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0023      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003429678 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0468      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00217     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 194        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00999332 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.925      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0489     |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00259    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007933128 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.055       |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00239     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008553353 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0754      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00246     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 172         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007124263 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0602      |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0023      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012707094 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0406      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0023      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009931028 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0664      |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00231     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011359173 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0505      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00244     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009087118 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0864      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00247     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010131499 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0402      |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00226     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005486223 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0395      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00213     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007285935 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0477      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00221     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095743295 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.934        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0452       |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0023       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.681      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 204        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00399068 |\n",
      "|    clip_fraction        | 0.353      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.932      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0509     |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.0259    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00234    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004579422 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0391      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00218     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006171757 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.06        |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00228     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009354273 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0772      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00212     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 202          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041532456 |\n",
      "|    clip_fraction        | 0.386        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0499       |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00222      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 199          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075579584 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.936        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0345       |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0022       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007868442 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0729      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0023      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 198          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062878667 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.94         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0437       |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00212      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[0.25],\n",
    "                                episode_limit_array=[75000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
