{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_1_singlegrid_half'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 3e-6,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.1,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[150,100,80], log_std_init=-2.9),\n",
    "                verbose = 1,\n",
    "                target_kl = 0.05,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7ff8991ad240> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7ff899189630>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.59 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.594    |\n",
      "| time/              |          |\n",
      "|    fps             | 159      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.598      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01634406 |\n",
      "|    clip_fraction        | 0.332      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.207     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0807     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0894     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03104717 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -1.33      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0932     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0244    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.039      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.602      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03388219 |\n",
      "|    clip_fraction        | 0.373      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.168     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0836     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0241     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.603       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022994306 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.285       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0956      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0162      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021103466 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.506       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0121      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.61       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01553568 |\n",
      "|    clip_fraction        | 0.349      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.652      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0713     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00967    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.609      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01569449 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.729      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0484     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00837    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.615       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010497185 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0377      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00772     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010225991 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0643      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00721     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.622       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008885348 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.117       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00707     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.625        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072679045 |\n",
      "|    clip_fraction        | 0.331        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.797        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0518       |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00716      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.631       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008779451 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0832      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00663     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.635       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009355175 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00626     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.638        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057855784 |\n",
      "|    clip_fraction        | 0.336        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0678       |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00621      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.64       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00693537 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.833      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0488     |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00566    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.644      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00875642 |\n",
      "|    clip_fraction        | 0.323      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.832      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0628     |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00564    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.645       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009920475 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.066       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00582     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.645        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090452405 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.101        |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00526      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.647        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053530065 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0304       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00523      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.649       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008904869 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0442      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00515     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006339657 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0639      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00492     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.652        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051323352 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.075        |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00523      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.654       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009221753 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0049      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055781305 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0419       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0047       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.658        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058724373 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0645       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00469      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.659        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067490577 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0453       |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00464      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.658       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006036216 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0893      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00475     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.661        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061206548 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.072        |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00453      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.662        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0089378385 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.057        |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00439      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007374385 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0494      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00444     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005817041 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.072       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0043      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006368914 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0432      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00418     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007103947 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0478      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00429     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008855546 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0298      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00421     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009287509 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0654      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00457     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006180185 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0681      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.669      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00744901 |\n",
      "|    clip_fraction        | 0.335      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.89       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0482     |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0274    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00373    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077824267 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0577       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00416      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010569647 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0586      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00414     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009805709 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0737       |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00409      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005775088 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0537      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00412     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.673       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006613183 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0438      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007948985 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0706      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00409     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054092766 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0715       |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00386      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056816516 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0469       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00388      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.676       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006936416 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0425      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00371     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054676207 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0782       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00378      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006577122 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0038      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007988164 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0545      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00365     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065893503 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0565       |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0038       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054379376 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0941       |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00379      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005700463 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0456      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00399     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075842915 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0538       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00364      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.682       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007628816 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0331      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00389     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052737566 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0547       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00362      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066615073 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0526       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00389      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005187395 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.035       |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008801505 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0317      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00364     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012113338 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0308      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00354     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070824325 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0467       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00349      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071767867 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0658       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0037       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002074176 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0314      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008731371 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0726      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0035      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007406512 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0627      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00355     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006723103 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0737      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00353     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004678154 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00339     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008750483 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0807      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00348     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004622057 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0337      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00366     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011140767 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0688      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00355     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007954454 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0526      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00337     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006911391 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0539      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00356     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007857516 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0343      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00362     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044157924 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0509       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00326      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009998336 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0681      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0035      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066106347 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0633       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00361      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0111424625 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0315       |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00348      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060398667 |\n",
      "|    clip_fraction        | 0.32         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0384       |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00359      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086274175 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0531       |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00345      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077141253 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0492       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00349      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010361892 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0475      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00363     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009502575 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00329     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073102294 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0507       |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0034       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008736911 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00337     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048912615 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0389       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00341      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008340132 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0643      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0033      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062472285 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0537       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00336      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009967664 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.091       |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00335     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008567226 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0446      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0033      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009974873 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0369      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008254874 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00326     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009023627 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0516      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00343     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009503538 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0723      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00353     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008689779 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0457      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00341     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009574244 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0347      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00325     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011612961 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0553      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0033      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008757388 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00324     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004511523 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0407      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00345     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049815564 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.053        |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00329      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008064446 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0849      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007549891 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0441      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00318     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007249558 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0646      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00322     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007426894 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0379      |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00314     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006366378 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0362      |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00323     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063534915 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0521       |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00319      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011894599 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0706      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007693185 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0716      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010480374 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0833      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00313     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008736849 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0631      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070881783 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.061        |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00303      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009740183 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0528      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00304     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009185729 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0436      |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.688      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00642139 |\n",
      "|    clip_fraction        | 0.367      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.916      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0931     |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00305    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.688      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00994038 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.902      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0641     |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0032     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068757297 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0553       |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00318      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010065463 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0534      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00295     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066979704 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0749       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0031       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075320005 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.062        |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00304      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009486216 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0548      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008792594 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0567      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00298     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009565773 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0803      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008563587 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0504      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076427013 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0504       |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00309      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011781287 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0549      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00325     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074589225 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0431       |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00322      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011446434 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0563      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007224259 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0919      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006349519 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0441      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007652593 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0032      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052061705 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0853       |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00316      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005937022 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0573      |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00326     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057859034 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.107        |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00306      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071198316 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0468       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00313      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007376626 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.047       |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004238474 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0722      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00323     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004353693 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0609      |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00288     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009468297 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00312     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007476422 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0378      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00303     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042291046 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0433       |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.0028       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008703905 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0708      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00304     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004904422 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0635      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00407497 |\n",
      "|    clip_fraction        | 0.377      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.912      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0576     |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.0549     |\n",
      "|    value_loss           | 0.00307    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 169          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056615607 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0568       |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00304      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048980145 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0791       |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008491874 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0876      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00308     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007877964 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.076       |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069461702 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0446       |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00299      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7ff92c355dd8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7ff8900ef630>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.599        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054267617 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.06         |\n",
      "|    n_updates            | 2940         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00313      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.598       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033292074 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.513      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0515      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0646      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038881756 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.987      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.087       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0373      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.604      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04119288 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.351     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0726     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0231    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0236     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030559992 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0747      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0146      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.613       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027010206 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.532       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0472      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0101      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018642226 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0576      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00818     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016428951 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0742      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00749     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.621       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013587463 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0479      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0065      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.623       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012742639 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0451      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0061      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.628       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008041352 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0607      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00599     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.629       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008239922 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0528      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00545     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.634       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007041025 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0433      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00566     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.636        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060052723 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.051        |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00562      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009457901 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0728      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00544     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.641       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011226216 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00515     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.644       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006986295 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0785      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00513     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.648       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007689467 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.048       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00499     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007308078 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0487      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00503     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.653        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073067397 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0364       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00475      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.657       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006605035 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0659      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00489     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010443976 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0581      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00481     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011254159 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0701      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00473     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.662        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041763424 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0901       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00464      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008767435 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0665      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00466     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.666      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00568634 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.868      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0426     |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0289    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00453    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011193231 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0768      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00456     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005714959 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0556      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00471     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016266822 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0613       |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00441      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042212456 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0506       |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00441      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009666702 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0459      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001135841 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0696      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044154553 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0473       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00461      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071219294 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0544       |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00419      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004634419 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00435     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.676      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00685862 |\n",
      "|    clip_fraction        | 0.345      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.878      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.081      |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0292    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00415    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069558294 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.044        |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00404      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064400313 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.059        |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00404      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007221195 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0969      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00396     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005397418 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0853      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00405     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007384339 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0504      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00397     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062600984 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0565       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007395521 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0448      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00398     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047016353 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0491       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00396      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041876496 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0824       |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0037       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006930658 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0373      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00398     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069208564 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0619       |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00406      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068201185 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.066        |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00388      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064550578 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.104        |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00386      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076612593 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0605       |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00409      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.682       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009999728 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0808      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00397     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.682       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008060935 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0368      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00378     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072397827 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0394       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0035       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006337528 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00364     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063328804 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.047        |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00354      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008557352 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0646      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063456357 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0317       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067988634 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0445       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004993039 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.053       |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00368     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008292618 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0488      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00356     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005021298 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0354      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0035      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007884353 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0716      |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010100618 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0372      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00338     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.685      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00885863 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.897      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0482     |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00359    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003459996 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007964378 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0492      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00358     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072408677 |\n",
      "|    clip_fraction        | 0.327        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0452       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0263      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00345      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.685      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00962331 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.904      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.044      |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.03      |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00343    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064251153 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0552       |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00342      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067554414 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.038        |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00339      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008456024 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0822      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00349     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056262733 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0862       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00344      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039193155 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0518       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00351      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068708896 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.047        |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00311      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009747815 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0754      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00356     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008389247 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0671      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0034      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055953143 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0397       |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00326      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005418745 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0735      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00338     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062494935 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0456       |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00314      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004795155 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00318     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.689      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00608511 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.911      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0629     |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0297    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00317    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046323566 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0535       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00316      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053784936 |\n",
      "|    clip_fraction        | 0.382        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0345       |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00318      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010206103 |\n",
      "|    clip_fraction        | 0.382        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0698       |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00325      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008182416 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008555189 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0637      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004615587 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0553      |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00315     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006782517 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0843      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058958353 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00304      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061042844 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0219       |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.0308      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00312      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.691      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00768773 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.911      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0344     |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0301    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00306    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007193568 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0945      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005964047 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0548      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0032      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066123633 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0687       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00306      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007208279 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00298     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007675162 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0326      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00315     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008638692 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0554      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009764564 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0611      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007436904 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0517      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00293     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.691      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00318484 |\n",
      "|    clip_fraction        | 0.373      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.912      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0679     |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0298    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.0031     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065324428 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0418       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.003        |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008830726 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.051       |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00299     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045406534 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0557       |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00288      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005849111 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0565      |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00296     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009108117 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0513      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0031      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007095954 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060783876 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0416       |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00287      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007352665 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0403      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009760359 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0559      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00314     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071632685 |\n",
      "|    clip_fraction        | 0.399        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0497       |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.0322      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00299      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.692      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00596413 |\n",
      "|    clip_fraction        | 0.364      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.914      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0451     |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00296    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063188104 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.045        |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00301      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008227741 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0623      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00296     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004819581 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0834      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00296     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.692     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 166       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 15        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0083063 |\n",
      "|    clip_fraction        | 0.347     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.915     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0783    |\n",
      "|    n_updates            | 2280      |\n",
      "|    policy_gradient_loss | -0.0275   |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00295   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010877302 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0527      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00291     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.692      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01147528 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.917      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0426     |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0028     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010004953 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0626      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0029      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023863495 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0564       |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00294      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055090757 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0818       |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00287      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008303824 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0456      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00295     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008010614 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0469      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034116074 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0795       |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00287      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.692      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00995321 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.911      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0632     |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00312    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095705185 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0617       |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007910517 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.053       |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007267016 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0698      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010971859 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.063       |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00288     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004348746 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0996      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00284     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059607504 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0697       |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00295      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008284584 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0437      |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00294     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071344106 |\n",
      "|    clip_fraction        | 0.386        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0528       |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00287      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069106906 |\n",
      "|    clip_fraction        | 0.382        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0743       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044085206 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0903       |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00283      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005800386 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0478      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00304     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064751417 |\n",
      "|    clip_fraction        | 0.382        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0751       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00262      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075607807 |\n",
      "|    clip_fraction        | 0.387        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.077        |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0029       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007300563 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0381      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007284683 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0442      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015832692 |\n",
      "|    clip_fraction        | 0.39         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0722       |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00285      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046705455 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0641       |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00298      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054926933 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0449       |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00277      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007547042 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0648      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00269     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009719026 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0529      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00286     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007540062 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0607      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00278     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064673424 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0511       |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00295      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007860953 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0415      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00293     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7ff8a0bd7320> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7ff85c0ebda0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.598       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009021781 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0681      |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00281     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.599       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009385461 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0855      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0881      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039988313 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.34       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0802      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.034       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.603      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03806994 |\n",
      "|    clip_fraction        | 0.369      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.312     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.087      |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.021      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.606      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03131314 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.319      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0578     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0131     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022439647 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.566       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.036       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0097      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.611      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01726788 |\n",
      "|    clip_fraction        | 0.342      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.713      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0506     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00725    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.614       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014945164 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0572      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00654     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008985171 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0566      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00626     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010988107 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0637      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00587     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.622       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011362004 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0543      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00538     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.623       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008221346 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0615      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00516     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.623        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077926842 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0628       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0051       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.628       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011076766 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.631       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013160175 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0627      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.005       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.633       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005835307 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0484      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00472     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.636        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052236975 |\n",
      "|    clip_fraction        | 0.328        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0409       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0255      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00475      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.639      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00896323 |\n",
      "|    clip_fraction        | 0.333      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.86       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0852     |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00461    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.642       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006395611 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0405      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0046      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006351298 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00433     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.646        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019422949 |\n",
      "|    clip_fraction        | 0.314        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0364       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0256      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00427      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.648        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052535236 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0635       |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00448      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007730496 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0369      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00405     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.652       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008419216 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0408      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00414     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.654       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011405647 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0592      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00435     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.655       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011265141 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0584      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00405     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062246947 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0388       |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00393      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.659        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071572186 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0668       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00395      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.661        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067984327 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0428       |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.662       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007443717 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.06        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00376     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011211082 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00404     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.666        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059723468 |\n",
      "|    clip_fraction        | 0.328        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0301       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0259      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00372      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005251378 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0717      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00386     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061333505 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.039        |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00369      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051238895 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0528       |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.673      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00744358 |\n",
      "|    clip_fraction        | 0.364      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.894      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0696     |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00371    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004693994 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0041      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008020115 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0536      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 156          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074458704 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0577       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00364      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008738098 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0727      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00386     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010977993 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00343     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074383346 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0432       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00377      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008825993 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0403      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00356     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007980948 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0417      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008762333 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0484      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00636971 |\n",
      "|    clip_fraction        | 0.353      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.895      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0496     |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0283    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00361    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030648888 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0553       |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00359      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006101561 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0567      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00364     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007723108 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0519      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063270805 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0566       |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00363      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008622226 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0228      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00344     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008048323 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.041       |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00361     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054159192 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0746       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00365      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005237013 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0387      |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00343     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070740045 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0836       |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00345      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030024648 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0938       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00322      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071089715 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0647       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00337      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007917041 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0307      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00339     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005447775 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0642      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00345     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051497817 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0773       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00337      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013060531 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0379      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00324     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.685      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00791161 |\n",
      "|    clip_fraction        | 0.353      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.901      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0616     |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00338    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070383577 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0663       |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00316      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009684473 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0508      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00329     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002525139 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0643      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00328     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005019963 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0506      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00334     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009953475 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0359      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00339     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066415966 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0612       |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00324      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010013861 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0918      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00327     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004544559 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0621      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00333     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007043916 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0711      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00322     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073182643 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0263       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00333      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.688     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 165       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 15        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0046857 |\n",
      "|    clip_fraction        | 0.365     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.7      |\n",
      "|    explained_variance   | 0.907     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0653    |\n",
      "|    n_updates            | 1440      |\n",
      "|    policy_gradient_loss | -0.0291   |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.00319   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004665372 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0594      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00304     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007608342 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.115       |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0033      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008560836 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00322     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007837531 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0649      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00308     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010263083 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0436      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061424435 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.045        |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00312      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077248216 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.075        |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00311      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073455633 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.07         |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00325      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064456584 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0526       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0263      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00317      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005316791 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0636      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00307     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009282509 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0499      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00325     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052198945 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0888       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00307      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007117945 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0634      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.689      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00696463 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.912      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0564     |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0284    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00297    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008228719 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0564      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073976396 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0645       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00305      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006412144 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0559      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00285     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077689914 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.103        |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00308      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008181125 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0424      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005671567 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.043       |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070380447 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0604       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.003        |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008322108 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0404      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00304     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008602816 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0346      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007410881 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0491      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010839468 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0483      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00304     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075879754 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0618       |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00308      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009034502 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0581      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062121362 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0389       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0028       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007321176 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00282     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007859203 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0481      |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00277     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006571105 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.052       |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00285     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005358505 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0549      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00275     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009192541 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0697      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008647519 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0493      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00274     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010224196 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0461      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00291     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009793252 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0363      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011555794 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0996      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00289     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061830045 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.053        |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00287      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040428312 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0502       |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.0263      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00286      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005475533 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0759      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00277     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007476923 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0927      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00285     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057670176 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0486       |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00278      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005129519 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0506      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041820556 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0435       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00282      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010448513 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0432      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00258     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047021597 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.042        |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00274      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007513015 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0517      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00267     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073210597 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0408       |\n",
      "|    n_updates            | 2400         |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00278      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008359182 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00263     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009112102 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00272     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012249703 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0461      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00278     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007809925 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0421      |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00261     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073758513 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0334       |\n",
      "|    n_updates            | 2500         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0027       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001000993 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0549      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00261     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.691      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01029931 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.924      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0481     |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0296    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00265    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062779663 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0685       |\n",
      "|    n_updates            | 2560         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00265      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010344908 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0406      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0027      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010920721 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0904      |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00262     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.692      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01016672 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.923      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0737     |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00269    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072126626 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0451       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00264      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041509746 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.93         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0852       |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00247      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.692      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01232813 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.924      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0826     |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00261    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007001263 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0619      |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00264     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061312676 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0422       |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00244      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006019354 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0583      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00239     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007814661 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0667      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00266     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001939854 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0611      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00244     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004985613 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0467      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0025      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069939555 |\n",
      "|    clip_fraction        | 0.38         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0509       |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00262      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.692      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00903585 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.929      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0693     |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00244    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060692606 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0614       |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00255      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069083543 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0614       |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00264      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006649074 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0506      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063118665 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.102        |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00256      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[0.5],\n",
    "                                episode_limit_array=[75000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
