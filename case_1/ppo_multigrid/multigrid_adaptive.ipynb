{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_1_multigrid_adaptive'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 3e-6,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.1,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[150,100,80], log_std_init=-2.9),\n",
    "                verbose = 1,\n",
    "                target_kl = 0.05,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/coarse_grid_functions.py:51: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1mFirst-class function type feature is experimental\u001b[0m\u001b[0m\n",
      "  for j in range(len(p_1)-1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f88039a43c8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8802c17ac8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.59 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.594    |\n",
      "| time/              |          |\n",
      "|    fps             | 102      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.597       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015503553 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.236      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0926      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.598       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027701471 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.25       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.042       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.601      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 246        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03151405 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.308     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0617     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0244     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.604       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022714242 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0727      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0155      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019427568 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0616      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0115      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014360016 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0764      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00904     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013760949 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.038       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00822     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.611       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009902736 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0519      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0072      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.613       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008958871 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0713      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00705     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007303399 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0934      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0066      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009353575 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0451      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00661     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.621       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006472853 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0923      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00611     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.626        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 256          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046646623 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0553       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00591      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.63         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056614517 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0525       |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00583      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.633        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075015277 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0559       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00553      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009601568 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0705      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00538     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.639       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009925622 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0345      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007462728 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0642      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00497     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.645        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046749176 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0399       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00501      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.646        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052710087 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0601       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0049       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.649       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005696991 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0406      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00464     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007913312 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0556      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00486     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.653        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074240863 |\n",
      "|    clip_fraction        | 0.311        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0241      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0045       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005299613 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.654        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034764528 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0396       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00456      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.655        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046053766 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0441       |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0266      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00458      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.656       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005309331 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00422     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.657       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005764833 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0611      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006988463 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.062       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0041      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008306416 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0377      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.661     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 252       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0072301 |\n",
      "|    clip_fraction        | 0.351     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.878     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0676    |\n",
      "|    n_updates            | 620       |\n",
      "|    policy_gradient_loss | -0.0269   |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.00394   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006913173 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0791      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00377     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006545782 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0379      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008365149 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0378      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00402     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009714067 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00374     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.664        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053011654 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0696       |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006423101 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0973      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00365     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.666        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052858116 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0912       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0262      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00378      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007949164 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0531      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00374     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002187717 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0781      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006781551 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0703      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004051268 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0496      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00367     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062743486 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0691       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00359      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 255         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004979509 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0572      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058360607 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0565       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00355      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 255          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027345777 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.047        |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0258      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00338      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.673       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004990545 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0782      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00346     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046329917 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0614       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00341      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "seed 1: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f88029789e8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8802c08c18>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004152441 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.065       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.684      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 187        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01260156 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.834      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0672     |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0301    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00495    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002791047 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0897      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00463     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0128581105 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0485       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00468      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008222347 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00465     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010010257 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0045      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058306186 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0693       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00439      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004788852 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0482      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006617397 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0411      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009856972 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0697      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00419     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068899454 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0587       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00411      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0100083705 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0429       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00414      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074029355 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0421       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00405      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006541741 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0782      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00419     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 192          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055289892 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0756       |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00385      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041852714 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0696       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00386      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 193          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052358597 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0282       |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070577264 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0954       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00403      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004865548 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.063       |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005265215 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0534      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.004       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007978404 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0304      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00392     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014226123 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0484       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00372      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007330939 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0881      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00388     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010227472 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0669      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00378     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007430819 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0605      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068831146 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0775       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00364      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "seed 1: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8803014588> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8802df1ef0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 62           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061261654 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0497       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00372      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 74 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073526828 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.804        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0371       |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00625      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004374626 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0474      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00622     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004602647 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00593     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007976157 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.071       |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00584     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008179029 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0548      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00574     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007187015 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0525      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00585     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060785054 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0492       |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.0316      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00567      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010309095 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0712      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0058      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052671074 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0481       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00561      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007533434 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00564     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007247594 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0581      |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0057      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005760005 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0641      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00553     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006816998 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0549      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00567     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006667304 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.048       |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00561     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006505734 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0513      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00538     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007871213 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0517      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00572     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067063896 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.062        |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00572      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005529004 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0573      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00554     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006084514 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0657      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00542     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008203861 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0545      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00543     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008772677 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0603      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00529     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081617655 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0446       |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00547      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005963874 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0627      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00534     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008741448 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0583      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00532     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065641464 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0367       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00522      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8802e113c8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f880280d0b8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006048426 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0324      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00513     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.601      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 250        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03261771 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.476     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0475     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0641     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.605       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034250923 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.18       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0708      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0378      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.606      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 255        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03983616 |\n",
      "|    clip_fraction        | 0.381      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.466     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0539     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0229     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.607       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029545229 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0502      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0144      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.614       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027530098 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0253      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0101      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.615      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 251        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01951104 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.683      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0783     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00806    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.619       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014926551 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0429      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00722     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.62       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 251        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01039381 |\n",
      "|    clip_fraction        | 0.336      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.799      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.049      |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0251    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00626    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.624     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 253       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0096928 |\n",
      "|    clip_fraction        | 0.339     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.81      |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0403    |\n",
      "|    n_updates            | 180       |\n",
      "|    policy_gradient_loss | -0.0264   |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00608   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.629       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010694569 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0714      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00589     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008935439 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00578     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.633       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010327518 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0663      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00572     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006420663 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00511     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.637       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009483946 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0614      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00522     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010879433 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00512     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.641        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070581674 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0589       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00514      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010783603 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00473     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.645        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082113175 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0502       |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00519      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.647       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008283702 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0047      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.65         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 256          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073689194 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0674       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00484      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.652      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 255        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00844332 |\n",
      "|    clip_fraction        | 0.34       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.857      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.066      |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00475    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.654     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 250       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0096127 |\n",
      "|    clip_fraction        | 0.346     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.862     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0901    |\n",
      "|    n_updates            | 440       |\n",
      "|    policy_gradient_loss | -0.027    |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00468   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.656        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029226064 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0687       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0245      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00458      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007250157 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.065       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0044      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056521357 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0729       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00453      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009899834 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0928      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00457     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006917706 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.041       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00445     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.662       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005110082 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0042      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.662        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048077675 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0552       |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00413      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.663        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 255          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069179265 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0559       |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00414      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.663         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 251           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059212744 |\n",
      "|    clip_fraction        | 0.337         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.871         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0571        |\n",
      "|    n_updates            | 620           |\n",
      "|    policy_gradient_loss | -0.0272       |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.00419       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007168728 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0417      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00414     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007922521 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.063       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008942589 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0364      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004092327 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0511      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005412662 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0543      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00367     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006316277 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0589      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0038      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059427978 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0903       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00377      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005376813 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0709      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 255         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006884399 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.078       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0037      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005267979 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0539      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 255          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049629183 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0511       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00367      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050691008 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0521       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00351      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002076575 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0928      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 254          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044919522 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0481       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00381      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007311416 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0612      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0037      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070066154 |\n",
      "|    clip_fraction        | 0.329        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0457       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0256      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0035       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053995075 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0741       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00342      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "seed 2: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f880280d0b8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f88028ba860>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005432823 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.055       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014175693 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0643      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00527     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005531612 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0267      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00462     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003973526 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0481      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00452     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006879178 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0497      |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00442     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003920513 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0609      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00457     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003918928 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0544      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044926764 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.072        |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0046       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004346752 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0951      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00452     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005609378 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00427     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008024106 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0535      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00421     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062348903 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0466       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00433      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039530573 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0375       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00417      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007655156 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0583      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00427     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008820432 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0281      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00443     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005879608 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0368      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011078197 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.059       |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00395     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061403424 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0535       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00421      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074849636 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0552       |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00405      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009063688 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0387      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00383     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032162995 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0556       |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00388      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053257733 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0943       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00382      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028727292 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0611       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00376      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065648435 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0824       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00385      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007947365 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0655      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00366     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 184          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069647045 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0528       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00385      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "seed 2: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f880309e278> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f880309e908>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 63           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090523865 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0619       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00389      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 62 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013576438 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0679      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00628     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008054378 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0559      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00605     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042369487 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0387       |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00612      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.696      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00793356 |\n",
      "|    clip_fraction        | 0.336      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.835      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0277     |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00578    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041998774 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0772       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00616      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052301227 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0479       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00567      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008429575 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0505      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00601     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033007474 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0527       |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0058       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006011787 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0425      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00568     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008207622 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0416      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00575     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032708019 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0424       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0318      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0055       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010726532 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0531      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00558     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008109769 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0544      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00545     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060852556 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0443       |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.032       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00557      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008015165 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0593      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00575     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064849285 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0456       |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00579      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007008195 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0528      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00573     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011020762 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0428      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00565     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070208283 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0239       |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.0324      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00548      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067979665 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0562       |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00554      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011905056 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0634      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00545     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002637261 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0385      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0055      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006756118 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0356      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00563     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045646755 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0448       |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.0316      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00545      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075600296 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.823        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0599       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0315      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0058       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8802c547f0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f87f0059d30>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.599       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008919639 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0704      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00547     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008441448 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0678      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0827      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.603       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036776222 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.4        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0878      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0336      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.605       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037886694 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.371      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0884      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0203      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.609       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031634964 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.084       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0126      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.609       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023138948 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.575       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0328      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00905     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.612      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 245        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01902612 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.723      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.052      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00683    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.615       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014740577 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.072       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00615     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011999324 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0322      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00588     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009492916 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0525      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00543     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.623       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010967347 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0583      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00508     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.623       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007900062 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.089       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00504     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.623        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061360775 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0397       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.026       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00504      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.625       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011871862 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0628      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00499     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.626       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014109999 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0648      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.627      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 240        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00510236 |\n",
      "|    clip_fraction        | 0.334      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.866      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.042      |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00438    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.63         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060845404 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0552       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0245      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00456      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.633       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012400851 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0664      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00404     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.637       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012367246 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0336      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00426     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.639        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058953343 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0582       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0262      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00415      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.641       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003532982 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0329      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00401     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.641       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008809748 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0711      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.642        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064934194 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0512       |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00381      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.645       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008403584 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0407      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.646       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007718435 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0752      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00406     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.647        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0111936275 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.081        |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0245      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00384      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.649       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006145814 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0589      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005348435 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.059       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004923704 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0414      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00333     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.654        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023266436 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0618       |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00346      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.656       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011344736 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0548      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00362     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042720973 |\n",
      "|    clip_fraction        | 0.332        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0439       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0242      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00353      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 5              |\n",
      "|    mean_reward          | 0.659          |\n",
      "| time/                   |                |\n",
      "|    fps                  | 247            |\n",
      "|    iterations           | 1              |\n",
      "|    time_elapsed         | 10             |\n",
      "|    total_timesteps      | 2560           |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00017822385 |\n",
      "|    clip_fraction        | 0.35           |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | 91.8           |\n",
      "|    explained_variance   | 0.9            |\n",
      "|    learning_rate        | 3e-06          |\n",
      "|    loss                 | 0.0868         |\n",
      "|    n_updates            | 640            |\n",
      "|    policy_gradient_loss | -0.0261        |\n",
      "|    std                  | 0.0551         |\n",
      "|    value_loss           | 0.00332        |\n",
      "--------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083399415 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0557       |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.661        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047372817 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0375       |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.0259      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00346      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.664        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036762594 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0498       |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00309      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.665        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028889328 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0542       |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0255      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00334      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.666        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064543905 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0357       |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.0256      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00329      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011148497 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00318     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008196855 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0698      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011829317 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069169314 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.056        |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00337      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005862546 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0452      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0032      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050961403 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0745       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00314      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058733253 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0381       |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00323      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017754287 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0585       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00326      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.67          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 243           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0002821833 |\n",
      "|    clip_fraction        | 0.363         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.912         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0664        |\n",
      "|    n_updates            | 920           |\n",
      "|    policy_gradient_loss | -0.028        |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.00301       |\n",
      "-------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072675883 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0342       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061362833 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0765       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00316      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "seed 3: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f87f0061c88> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8802c3afd0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065400093 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0587       |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0031       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011188051 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0403      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00465     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076557905 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.049        |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00456      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064540775 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0418       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00438      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 184          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032833158 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0629       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00424      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041320445 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0491       |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00436      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055781305 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0335       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00399      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036110312 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0444       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0041       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.685      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00806571 |\n",
      "|    clip_fraction        | 0.351      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.881      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0456     |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00404    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059013306 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0433       |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00386      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014952391 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0563       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00379      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.686      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00583016 |\n",
      "|    clip_fraction        | 0.321      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.894      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0452     |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00363    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 185          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068862615 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0265       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00386      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010893014 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0552      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009802771 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0825      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00383     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076512485 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0592       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00356      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005053279 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00348     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077535273 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0702       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00391      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009983768 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00373     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.687     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 186       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 13        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0092116 |\n",
      "|    clip_fraction        | 0.365     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.896     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0409    |\n",
      "|    n_updates            | 1360      |\n",
      "|    policy_gradient_loss | -0.0297   |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.00363   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008580878 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0643      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00348     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007439351 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0569      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00345     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004522818 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0595      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00366     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005281174 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.687      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 189        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00402942 |\n",
      "|    clip_fraction        | 0.353      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.899      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0561     |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00361    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 184          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067441673 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0619       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00351      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "seed 3: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f87f0059128> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f87f0051908>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 62           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033643246 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0767       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00358      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 73 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007113427 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.081       |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00548     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055618705 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0816       |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00565      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008035442 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0531      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00516     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048375637 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0382       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00535      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041244896 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0733       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00557      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003666842 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0674      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00495     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008758766 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0762      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00541     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007998881 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0886      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00534     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006273985 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00534     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005719924 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0519      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00536     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072653233 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.058        |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00509      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077202767 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0381       |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00512      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008083877 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0513      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00522     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009214714 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0887      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00502     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005441296 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0517      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00508     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006475857 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0809      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00495     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028640092 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.053        |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.0308      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00502      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006408131 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0468      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00468     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 63 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022227527 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0602       |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00507      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006866923 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0687      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00471     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078065842 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.073        |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00479      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009713948 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0471      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00507     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051871566 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.048        |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00506      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004751933 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0631      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0046      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.698      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 96         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00723373 |\n",
      "|    clip_fraction        | 0.355      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.855      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0546     |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0302    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00496    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 62 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=25,\n",
    "                                grid_fidelity_factor_array =[0.25, 0.5, 1.0],\n",
    "                                episode_limit_array=[25000, 25000, 25000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
