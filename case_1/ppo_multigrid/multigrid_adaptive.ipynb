{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_1_multigrid_adaptive'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 3e-6,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.1,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[150,100,80], log_std_init=-2.9),\n",
    "                verbose = 1,\n",
    "                target_kl = 0.05,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8dbf6f2a58> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8dbf6e3470>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.59 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.594    |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.597       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015503636 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.236      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0926      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.598       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027700674 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.25       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.042       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.601      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03151442 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.308     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0617     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0244     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.603       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022731388 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0769      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0155      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.609      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 206        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02047947 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.504      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0564     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0271    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0115     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014205614 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0738      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00904     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014776167 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0347      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00822     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.611       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009310255 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0572      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0072      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.614       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008558616 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0711      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00705     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007448086 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0936      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00659     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009115359 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0507      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0066      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.621        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067782523 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0935       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0253      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00611      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.625        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037248284 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0557       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00591      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005490628 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0515      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00583     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.632        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065607997 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0635       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0258      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00553      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.637       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009056427 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0688      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00539     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009073767 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0319      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.644       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007067797 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0598      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00497     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.647        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052206665 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0394       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00499      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.647        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 203          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052545993 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0596       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00489      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004952848 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0499      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00463     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.652        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 204          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075270445 |\n",
      "|    clip_fraction        | 0.335        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0535       |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0253      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00485      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.654        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073877336 |\n",
      "|    clip_fraction        | 0.311        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0432       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0242      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00448      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.654        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060952753 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0674       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00436      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.655        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 204          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019960166 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0468       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00455      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.656        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052233576 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.037        |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00457      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.657       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006204811 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0678      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0042      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.658        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062398524 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0516       |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00407      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.66       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00753358 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.877      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0548     |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00408    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008065457 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.037       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00393     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008026439 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0636      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00392     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007908275 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0739      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00376     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.661      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00624949 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0381     |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00386    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008521202 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0341      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.004       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009663308 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00372     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005313185 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0708      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00371     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005771786 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0991      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00363     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005723181 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0959      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071375193 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0464       |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022626133 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0776       |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063076317 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0548       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039473595 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0501       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00365      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006813541 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0724      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005300158 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0597      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00355     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056967945 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0591       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00353      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030656694 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0258      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00336      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058521954 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.079        |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00344      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055377274 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.059        |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00339      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "seed 1: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8dbf6e3470> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8dbf6f7cf8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042487443 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0587       |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00314      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013235169 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0625      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00494     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029845804 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0911       |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00463      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012733981 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0467      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00466     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066221626 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0442       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00462      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010792184 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0474      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00448     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054006665 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0709       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00438      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005490288 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0468      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005322394 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0361      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00426     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010062948 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0706      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00418     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.684      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00727576 |\n",
      "|    clip_fraction        | 0.351      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.061      |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00409    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010082498 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0385      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00413     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007220918 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0414      |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00403     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 169          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066599725 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0755       |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00416      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058379113 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0774       |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00383      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038633526 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0687       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00384      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.686      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00399805 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.894      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0317     |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0293    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00372    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006710267 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0924      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00401     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.687     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 169       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 15        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0048051 |\n",
      "|    clip_fraction        | 0.338     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.89      |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.059     |\n",
      "|    n_updates            | 1340      |\n",
      "|    policy_gradient_loss | -0.0282   |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.00379   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004210937 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0552      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00398     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008582848 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0342      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0039      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022564917 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0501       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00371      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006430435 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0794      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00386     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008837161 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0693      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00376     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075751273 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0499       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00379      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074083866 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0786       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00362      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "seed 1: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8dbc07a048> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8dbc07a908>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057114484 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0432       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00369      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007154134 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0385      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00621     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003087911 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0568      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00618     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004172054 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0583      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00589     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008014301 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0693      |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00578     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008355593 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0057      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006669146 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0494      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00581     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051605403 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0631       |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00565      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010409528 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0689      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00577     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050622285 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0577       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00558      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070640533 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0601       |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00561      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061775954 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0546       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00568      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054906188 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0582       |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0055       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071935505 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0518       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00565      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072294264 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0443       |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.0308      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00558      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006212163 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0449      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00535     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069876225 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0558       |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00567      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065422803 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0643       |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00569      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048417537 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0619       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00551      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070617558 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0696       |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0054       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008438975 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0491      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00541     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.695      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00933288 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.844      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0552     |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.0328    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00527    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008198878 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0466      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00545     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053628744 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0591       |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0319      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0053       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007618907 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0605      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00527     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007024163 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0345      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00521     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjIAAAHUCAYAAAAgOcJbAAAAAXNSR0IArs4c6QAAIABJREFUeF7snQmcj9X+xz+zD2MYDGGsI1tkqZQSIrJXSkUpQigV6orcshSX/lLCvSgVuepairJElmRJtuxkzb4vMxizz/xf5+nOXMsw5/md3/n9fs/vfJ7Xq9e99TvPOef7/i7PZ85znucJyMzMzAQPEiABEiABEiABEnAggQAKGQd6jVMmARIgARIgARKwCFDIMBBIgARIgARIgAQcS4BCxrGu48RJgARIgARIgAQoZBgDJEACJEACJEACjiVAIeNY13HiJEACJEACJEACFDKMARIgARIgARIgAccSoJBxrOs4cRIgARIgARIgAQoZxgAJkAAJkAAJkIBjCVDIONZ1nDgJkAAJkAAJkACFDGOABEiABEiABEjAsQQoZBzrOk6cBEiABEiABEiAQoYxQAIkQAIkQAIk4FgCFDKOdR0nTgIkQAIkQAIkQCHDGCABEiABEiABEnAsAQoZx7qOEycBEiABEiABEqCQYQyQAAmQAAmQAAk4lgCFjGNdx4mTAAmQAAmQAAlQyDAGSIAESIAESIAEHEuAQsaxruPESYAESIAESIAEKGQYAyRAAiRAAiRAAo4lQCHjWNdx4iRAAiRAAiRAAhQyjAESIAESIAESIAHHEqCQcazrOHESIAESIAESIAEKGcYACZAACZAACZCAYwlQyDjWdZw4CZAACZAACZAAhQxjgARIgARIgARIwLEEKGQc6zpOnARIgARIgARIgEKGMUACJEACJEACJOBYAhQyjnUdJ04CJEACJEACJEAhwxggARIgARIgARJwLAEKGce6jhMnARIgARIgARKgkGEMkAAJkAAJkAAJOJYAhYxjXceJkwAJkAAJkAAJUMgwBkiABEiABEiABBxLgELGsa7jxEmABEiABEiABChkGAMkQAIkQAIkQAKOJUAh41jXceIkQAIkQAIkQAIUMowBEiABEiABEiABxxKgkHGs6zhxEiABEiABEiABChnGAAmQAAmQAAmQgGMJUMg41nWcOAmQAAmQAAmQAIUMY4AESIAESIAESMCxBChkHOs6TpwESIAESIAESIBChjFAAiRAAiRAAiTgWAIUMo51HSdOAiRAAiRAAiRAIcMYIAESIAESIAEScCwBChnHuo4TJwESIAESIAESoJBhDJAACZAACZAACTiWAIWMY13HiZMACZAACZAACVDIMAZIgARIgARIgAQcS4BCxrGu48RJgARIgARIgAQoZBgDJEACJEACJEACjiVAIeNY13HiJEACJEACJEACFDKMARIgARIgARIgAccSoJBxrOs4cRIgARIgARIgAQoZxgAJkAAJkAAJkIBjCVDIONZ1nDgJkAAJkAAJkACFDGOABEiABEiABEjAsQQoZBzrOk6cBEiABEiABEiAQoYxQAIkQAIkQAIk4FgCFDKOdR0nTgIkQAIkQAIkQCHDGCABEiABEiABEnAsAQoZx7qOEycBEiABEiABEqCQYQyQAAmQAAmQAAk4lgCFjGNdx4mTAAmQAAmQAAlQyDAGSIAESIAESIAEHEuAQsaxruPESYAESIAESIAEKGQYAyRAAiRAAiRAAo4lQCHjWNdx4iRAAiRAAiRAAhQyjAESIAESIAESIAHHEqCQcazrOHESIAESIAESIAEKGcYACZAACZAACZCAYwlQyDjWdZw4CZAACZAACZAAhQxjgARIgARIgARIwLEEKGQc6zpOnARIgARIgARIgEKGMUACJEACJEACJOBYAhQyjnUdJ04CJEACJEACJEAhwxggARIgARIgARJwLAEKGce6jhMnARIgARIgARKgkGEMkAAJkAAJkAAJOJYAhYxjXceJkwAJkAAJkAAJUMgwBkiABEiABEiABBxLgELGsa7jxEmABEiABEiABChkGAMkQAIkQAIkQAKOJUAh41jXceIkQAIkQAIkQAIUMowBEiABEiABEiABxxKgkHGs6zhxEiABEiABEiABChnGAAmQAAmQAAmQgGMJUMg41nWcOAmQAAmQAAmQAIUMY4AESIAESIAESMCxBChkbLouPT0d/fv3x+TJk5GUlIRmzZphwoQJKFy48A09/eMf/4D45+ojISEBr732GsaMGWP959OnT6NHjx5YvHgx8uTJgy5dumDYsGEIDAy0OTM2JwESIAESIAHzCFDI2PS5EBlTpkzBokWLULBgQXTs2BEZGRmYO3durj3t3bsXlSpVwm+//YZ7773Xat+kSRPkz58fX375pSVqmjZtildeeQVvvvlmrv2JBmJsIaiCg4MREBAgdQ4bkQAJkAAJ/EUgMzMTaWlpCA8P5x+QDg0KChmbjitTpgwGDhxorZyIY/fu3ahcuTKOHDmCkiVL3rK3v/3tb1i2bBl+//13q92ff/6J2NhY7Nu3D+XLl7f+28SJE/Hhhx9CiB6Z48qVK4iIiJBpyjYkQAIkQAI3ISBWy/PmzUs+DiRAIWPDafHx8YiKisKmTZtQs2bN7DOFkJg5cyZatGhx096Sk5MRExNj3Wrq1q2b1W7OnDno1KkT4uLiss9bv369tVpz+fLlHAWKuLUlVmGyjpSUFOTLlw+XLl1CSEiIDWv+Ws1ZsGCBNW9TbmWZaLMIChPtps3m3J5W8XVqaioiIyMhanRoaKitGsrGvkGAQsaGH8SqS+nSpXHgwAGUK1cu+0whUEaNGoV27drdtLdp06bh5ZdfxvHjxy3hIY6pU6finXfewaFDh7LPEysxFStWxIkTJ1CsWLEb+hs8eDCGDBlyw3+fNWuWdXuJBwmQAAmQgDwBcVupbdu2EH8U2v1jUH4UttRJgELGBl2xciL2xbiyIlO/fn1UrVoV48ePzx7RHSsy4q8JsSIk9snYTULxV8z8+fPRsmVLo1ZkTLM5a0XGNLsZ32atyLga36KGiv0xFDI2LoY+1pRCxqZDxB6ZQYMGoXPnztaZe/bssTbw3mqPzM6dOy0Rs3nzZtSoUSN7xKw9Mvv377f2yojj008/xciRI6X3yIgkFMuhriRh1ibl1q1bGyVkxMZsk2zOEjKm2c34NkvIuBrfKjXU5uWDzTURoJCxCVY8tSRuCS1cuNBanRF7XEQizJs376Y99erVC+vWrcOaNWtuaCOeWhL7bj7//HOcOXPGepy7e/fuEBuDZQ6VJGShZ6GXiTGntmF8M75lYlelhsr0zzb6CVDI2GQsNtv269fPeo+M2BwmHpcWTxqJ98iIfTBChIiNullHYmKitcn3448/th7Vvv64+j0yYWFh6Nq1q7UhWHbzrUoSstCz0NsMf0c1Z3wzvmUCVqWGyvTPNvoJUMjoZ6x1BJUkZKFnodcanF7unPHN+JYJQZUaKtM/2+gnQCGjn7HWEVSSkIWehV5rcHq5c8Y341smBFVqqEz/bKOfAIWMfsZaR1BJQhZ6FnqtwenlzhnfjG+ZEFSpoTL9s41+AhQy+hlrHUElCVnoWei1BqeXO2d8M75lQlClhsr0zzb6CVDI6GesdQSVJGShZ6HXGpxe7pzxrT++r6Sk4VJSGoICAxAcGIDI8BDr/+d0JKWm48ylZFxMSkVgQIDVLk9IEEpE5bnpOTcLIfF9pMTUdOvn4MBABAVkWk+OuvJaBZUa6uUQ5/D/JUAh4/BQUElCFnr9hd5Xwou+NsPXnvCzECNLdp3Coh0nsXrfWaSmZ2aHeaGIULS8szgerxWDgnlDsOyP01i66zR2nbyIuCupOaZDaHAgYqMjUKlYJJpXK46GlYsgLDgIQvis2HMGvx04b4mfhOS/RNPJi0k4EZeIhJS/hEzWcU90Bma8Yf/lnio11Ffy2/R5UMg4PAJUktATRc/X8Jpos/CBiXabZrO42P+0/STmrtqMO6tUQFTeUOQNDUZGZibSMzKRmp6B5LQMJKemIzUjE1F5QhCdLwwFI0KQkpYJsbqSlJqBstF5Ub1kFPKFBVtfhhbCYcexi/jtwDms2ncWf5y8lJ3WoUGBKBEVjvTMTKSkZeDUxeSbpnz+8GDclj8c+fOEWP0K/XMpMRWHzl+x5pd1FMgTgpqlorD+4HlcuU6sXN25EE1i8UeMm5aRiRpRqZjWm0LG12quJ+ZDIeMJyhrHoJCxB9e0i1sWHRPtNsFmIQiEuPj3b4fw8+4z1kXdHUdAAFAuOgLnE1JuWEkRgqRexSJoVrUYHqpUxLqdlHX8eTYB328+hh+2HEdyagbqVyyCxlWK4r7YwpYwyukQcz58/grW/nkOczYdw/qDF6xmQqTULlsITe64zRJA+cKDERn2lxgqmj/MWrVxR3yr1FB3sGYf6gQoZNQZerUHlSQ0odBf7xwTbeaKjLM+wfH12sPYeSIexQvksVY7MjOBI+cTceTCFYQEBaBObGHUvT0aJ+KSMGLhLqzed84Kc/FbvQrRKJh0EpWqVMWl5DQkJKcjKBAICgy09rCEBQciPCTI2pMSdyUFZxNScCEhxfrvecOCIVZYdp+8hK1H47Jv3cRE5UHlYpG4q0xBPHh7NKrFFLC9p8VOkTxy/gp2HL+Ie8oWtFaMZA6VvFapoTJzYxv9BChk9DPWOoJKEqokv1ajNHZuos0UMs4RMvFXUlHz/Z8s8SJ7iIv9a41ux+M1YxAZHgRXvzl09XjiVo8QFAUjQiFu9fj6oZLXKjXU17mYMj8KGYd7WiUJVZLfqdhMtJlCxjlCZuOh83hy/BqULxKB1jVKWKsu4ihVKA9KFcqLi0lp+HXfWfy6/5y1r+SlerHoWq8cIv5724bxbd/XKjXUqXXQ3+ZNIeNwj6okoYlFz0SbKWTsX9y8VRamrz+Mft9uw3P3lcawNnfedBpCxIiHnAOve9SZ8W3f1yo11FtxwnGvJUAh4/CIUElCE4ueiTZTyNi/uHmrLAydtxOTVv2JQa3vwIt1y9meBuPbvq9VaqhtB/EELQQoZLRg9VynKkloYtEz0WYKGfsXN89l8LUjdfxiHX7Zcwb/7nIfHqwQbXsajG/7vlapobYdxBO0EKCQ0YLVc52qJKGJRc9Emylk7F/cPJfB145Ud8QyHItLxG9vP4xiBcJtT4Pxbd/XKjXUtoN4ghYCFDJasHquU5UkNLHomWgzhYz9i5vnMvh/I4k311YdtMh6V8rWwY8gQLzMxebB+Lbva5UaatM9bK6JAIWMJrCe6lYlCU0seibaTCFj/+Lmqfy9ehzx7pZHx61GrdJRmP1KXZemwPi272uVGuqSk3iS2wlQyLgdqWc7VElCE4ueiTZTyNi/uHk2i/8a7bvfj+KNGVvw1N0lMfKpGi5NgfFt39cqNdQlJ/EktxOgkHE7Us92qJKEJhY9E22mkLF/cfNsFv812gcL/8D45fsxoEVldKtf3qUpML7t+1qlhrrkJJ7kdgIUMm5H6tkOVZLQxKJnos0UMvYvbp7N4r9G6zplg/VV6S863YNGlW9zaQqMb/u+VqmhLjmJJ7mdAIWM25F6tkOVJDSx6JloM4WM/YubZ7P4r9Eafrgc4qOLK99qaL3F15WD8W3f1yo11BUf8Rz3E6CQcT9Tj/aokoQmFj0TbaaQsX9x82gSA0hKTccdAxciNDgQO4c0u+GNvbLzYXzb97VKDZX1C9vpJUAho5ev9t5VktDEomeizRQy9i9u2hP3ugH+OHkRzUavRNUS+TH/9XouD8/4tu9rlRrqsqN4olsJUMi4FafnO1NJQhOLnok2U8jYv7h5OpPnbjmO177ZhMdrlsDodrVcHp7xbd/XKjXUZUfxRLcSoJBxK07Pd6aShCYWPRNtppCxf3HzdCZ/tHgPxizdi75NK6Fnw9tdHp7xbd/XKjXUZUfxRLcSoJBxK07Pd6aShCYWPRNtppCxf3HzdCb3nPY75m87gQkd7kazasVcHp7xbd/XKjXUZUfxRLcSoJBxK07Pd6aShCYWPRNtppCxf3FzVyZnZGRi4+EL2HT4ArYfuwixF6Zg3lDcGVMAd5YsgCrF86Ns4Qi0GrsSe05dxtI3G6B8kXwuD8/4tu9rlRrqsqN4olsJUMi4FafnO1NJQhOLnok2U8jYv7jllMlClBw8l4CT8UmoWqIACuQNyW6WmZmJcwkp1lNHEaHB1lNI4k29X/56EAfOJNyyMAQFBiA9IxMhQQHY9V4zBAcFulxIGN/2fa1SQ112FE90KwEKGbfi9HxnKkloYtEz0WYKGfsXt6xMFqLl29+P4uc/TmPniYu4kpJu/RQYAFQTqyoxBXD4/BVsPxaPC1dSswtAljgR/yE2OgKN77jNan9H8UicvZxitd92LN5ahTlw5jKS0zLwQPnC+PqlOkpFhPFt39cqNVTJWTzZbQQoZNyG0jsdqSShiUXPRJspZOxd3MRqyuKdpzBz41Gs2nsGGZl/5bb4GHW56AjcFhkO8YHHhP+KmqzMvy1/GDIzgcvJaZYwqXt7NF6sWxYNKhS55XthxGrMifhEFIkMQ1hwkFIhYXzb87WArVJDlZzFk91GgELGbSi905FKEppY9Ey0mUIm94ubuDW06UgcZm08CvEo9KWkNCuh84YGoeWdxdGmVgxqlo5C3tBg67+npmdg69F47Dpx0drjUi0mP6LyhmYXAdFfgFA+Hj4Y37n7+nqXqNRQD7uXw92EAIWMw0NDJQlNLHom2kwhc+PFTQiN8wkpOBaXiJV7z+LbjUdx4Oz/9rLUiS2EJ+8qiRZ3FkdE2F/ixQkH45tCxglx6u45Usi4m6iH+6OQsQechd5+obdHWK21WOnYefyvVY6rN9PerNeNh87jnTk7ULZwXusdLLFXPfGT5esWLVvh9OUUbDsaj/UHz2PDwfPW3pTE1L/2u2QdpQvlxRN3xVgCxtVvHalZr34249t+fKvUUHWPsQd3EKCQcQdFL/ahkoQmFj0TbXbCioxYIVm4/SRGLtptrYyIzbTVS0ahfoVoNKtWHHeUyH9NlokniD5decBqL/aYiCM4MADP3VcaNUpFYffJS9ajzjsOnUF8WiBS0/+70eW/vYi7PmKvS8mCeVCxWCQerxmD2mULeuV2kDvLB+ObQsad8eSUvihknOKpm8yTQsaeA1no7Rd6e4Rzby2Ex+Yj4t0qcbiYmIqLSWnW/pQtR+Ksk8XKyNnLydlPCIn/VrlYJFrXKGHtWTl9KRm/H7qAtX+etwTPa40qICE5DVPWHLxBsIhzxWPNMVF5UOG2SEus3FO2kPVNI9WNtblb6vkWjG/78a1SQz3vYY6YEwEKGYfHhUoSmlj0TLTZ0ysy4nHihTtO4kJCCs4npOJKShryhAQhPDQI8YmpWLX3rPW/1x9CbPytaUU8ViMGaRmZ1kvklv1xGt9vPo6TF5NuaF80Mgxj2tdCndjC1m+Hz13Bpyv3IzElA1WKR6JC0Xz4c8saPPdEK4QoPg3klDLB+KaQcUqsunOeFDLupOmFvihk7EFnobdf6O0RBtqO/xUbDl245WliRUQ8nlwkXxgiw4NRKCIU9SsWQXjIjY8fixWctQfOYfGuUwgNCrQeU74tfzjqVYi+5kmh6wc00dcm2qwq1FVqqN3cYHs9BChk9HD1WK8qSWhi0TPRZtVCbzeY735/sfWW20Gt70DRyHDrdlByWrp1q0i8KE6soAghovsw0dcm2qwa3yo1VHcMs385AhQycpx8tpVKEppY9Ey0WbXQ2wl+8TK5yu8uRGRYMLYNaWrnVLe3NdHXJtqsGt8qNdTtQcsOXSJAIeMSNt85SSUJTSx6JtqsWujtRPvBswl46MPlqHhbPvzUp4GdU93e1kRfm2izanyr1FC3By07dIkAhYxL2HznJJUkNLHomWizaqG3E+2/7j+LZz9biwYVi2BK53vtnOr2tib62kSbVeNbpYa6PWjZoUsEKGRsYktPT0f//v0xefJkJCUloVmzZpgwYQIKF/7ryYnrj9OnT6Nv376YN2+e9U2P2NhYLFiwACVKlLCaiv//7rvvYt++fYiIiMDjjz+Ojz76COHhcnsIVJLQxKJnos2qhd5Oiog35L45cwva31sKw5+obudUt7c10dcm2qwa3yo11O1Byw5dIkAhYxPbsGHDMGXKFCxatAgFCxZEx44dkVU8ru9KCJ3atWujTp06GD58OAoVKoRdu3ahVKlSyJ8/P4TIKV26tCVcevTogePHj6N58+Z49NFHIcaROVSS0MSiZ6LNqoVeJg6z2oxbthcf/rQHbzSpiNcfrmDnVLe3NdHXJtqsGt8qNdTtQcsOXSJAIWMTW5kyZTBw4EB06dLFOnP37t2oXLkyjhw5gpIlS17T28SJEzF06FAcOHAAISEhN4z0+++/4+6777ZWdsLCwqzf3377bWzbts1awZE5VJLQxKJnos2qhV4mDrPaDJi9DV+vPYyRbavjqXtK2TnV7W1N9LWJNqvGt0oNdXvQskOXCFDI2MAWHx+PqKgobNq0CTVr1sw+U9wSmjlzJlq0aHFNb+3atcOFCxesVZfZs2cjOjoaL7/8Mnr16mW1E0WnVatW1u2pV155BceOHbP6EL9369Ytx5mJW1vivKxDJKEYX4ihnMTSrcwT/cyfPx8tW7ZEYGCgDRLObWqizVmx5glfd56yAct3n8HUzrWt98R48zDR1ybarBrfooaKW/kpKSm2a6g345tj/48AhYyNaBCrLkKUiBWWcuXKZZ8ZExODUaNGQQiXq4/GjRtj6dKlGD16tCVgtm7daomWsWPHon379lbTGTNm4LXXXsO5c+cgRMpzzz2Hr7766qbCYvDgwRgyZMgNs541axaCg53zlV4b2NnUQQRGbAnCiSsB+HvNNBTN46CJc6rGEkhLS0Pbtm0pZBwcARQyNpwXFxdn7YuRXZFp06YN1q9fj6NHj2aP0rt3b2svjBAwP//8s7UC8+2336Jp06Y4e/YsXnrpJWsvjdhMnNPBFRkbDsuhKf9i1bv6VvO9xda3k3YMfgR5Qm98S6+a9+ydbaKvTbSZKzL28sIfW1PI2PSq2CMzaNAgdO7c2Tpzz549qFSpUo57ZMTKyaRJk6zfsg4hZE6cOIHp06fjww8/tG5JrV27Nvv3uXPn4oUXXrBuSckcKvd3TbyfbqLNWYVexFbr1vo+USA+3Fh10CIUzBuCTQMfkQlfrW1M9LWJNqvGt0oN1RrA7FyaAIWMNKq/GoqniaZOnYqFCxdaqzOdOnWyHqvOaXPuoUOHUKVKFYwcOdJ6Kmn79u0Qt5vGjRuHZ555BqtXr0aTJk0wZ84c63/F7SUhkBISEqxbUjKHShKaWPRMtFm10MvEoWiz7/QlNP5oBe4onh8LetWTPU1bOxN9baLNqvGtUkO1BS87tkWAQsYWLlj7WPr162fd+klOTrZuCYmnk8R7ZKZNm4bu3bvj8uXL2b0uX74cffr0sVZuxLtjxIpMz549s38Xj3KLlRkhesSGswYNGliPY4tHtGUOlSQ0seiZaLNqoZeJQ9FmxZ4zeOGLdWhcpSgmdawte5q2dib62kSbVeNbpYZqC152bIsAhYwtXL7XWCUJTSx6JtqsWuhlo376+sPo9+02vHB/Gbz3WDXZ07S1M9HXJtqsGt8qNVRb8LJjWwQoZGzh8r3GKkloYtEz0WbVQi8b9R8v3oNPlu5Fv2aV8fJD5WVP09bORF+baLNqfKvUUG3By45tEaCQsYXL9xqrJKGJRc9Em1ULvWzUvzVrC2ZsOIpP2tXEYzVjZE/T1s5EX5tos2p8q9RQbcHLjm0RoJCxhcv3GqskoYlFz0SbVQu9bNR3mLQWq/adxYzu9+PecoVkT9PWzkRfm2izanyr1FBtwcuObRGgkLGFy/caqyShiUXPRJtVC71s1DcatRwHziRg5VsNUapQXtnTtLUz0dcm2qwa3yo1VFvwsmNbBChkbOHyvcYqSWhi0TPRZtVCLxP1mZmZuGPgIiSlpWPP0OYICfL+Jy9M9LWJNqvGt0oNlckNttFPgEJGP2OtI6gkoYlFz0SbVQu9TADHXUmBeKtv0cgwrPt7Y5lTtLcx0dcm2qwa3yo1VHsQcwApAhQyUph8t5FKEppY9Ey0WbXQy0T/zuMX0WLMStQoFYXve9aVOUV7GxN9baLNqvGtUkO1BzEHkCJAISOFyXcbqSShiUXPRJtVC71M9C/ddQpdpmxA82rFML7D3TKnaG9joq9NtFk1vlVqqPYg5gBSBChkpDD5biOVJDSx6Jlos2qhl4n+qb8dwrtztqNz3XIY2PoOmVO0tzHR1ybarBrfKjVUexBzACkCFDJSmHy3kUoSmlj0TLRZtdDLRP//LfwD/1q+H++0rIKu9WJlTtHexkRfm2izanyr1FDtQcwBpAhQyEhh8t1GKkloYtEz0WbVQn919P95NgHbjsVDbO6Nu5KKQhGhaHFncbw/bydmbzqGfz57F1pWL+4TCWOir020WTW+VWqoTwQ6JwEKGYcHgUoSmlj0TLRZtdCL88Xj1V+sPogRP+5CanrmNVkTHBiAsOBAJKSk47tXHsBdpQv6RFaZ6GsTbVaNb5Ua6hOBzklQyDg9BlSS0MSiZ6LNqoU+PjEV4vMDi3acQkAA0PLO4igRlQcF8oRg54mLWLzzFFLSMqzf1g1ojCKRYT6RVib62kSbVeNbpYb6RKBzEhQyTo8BlSQ0seiZaLOrhV6swgjxIm4bHYtLRHS+UIx+phYerBB9TdoIobNo+0lEhAX7zG0lV212ej1gfLdGYKC9lzGq1FCnx4u/zJ+3lhzuSZUkNLHomWizKxf1facvYfAPO61vJ4nj/tjCGN2uJm7LH+6YjDHR1yba7Ep8Xx3EKjXUMcng5xOlkHG4g1WS0MSiZ6LNdgp9ekYmPlt5AKN+2m3thRG3id5uXhltasUgQNw7ctBhoq9NtNlOfOcUvio11EHp4NdTpZBxuHtVktDEomeizbKFXtw+emP6Zqz987y136VL3XLo1bgCIsNDHJklJvraRJtl4/tmQaxSQx2ZGH44aQoZhztVJQlNLHom2ixT6DceuoAXv1yHi0lpiInKg4/wroVkAAAgAElEQVSfqYl7yxVydHaY6GsTbZaJ71sFskoNdXSC+NHkKWQc7kyVJDSx6Jloc26FfuOh8+j4xXpcTk5D6xolMKxNNeR36CrM1elsoq9NtDm3+M6txKvU0Nz65u+eIUAh4xnO2kZRSUITi56JNt+q0G84KETMOusdMM/eVxpDH6uGwEBn7YW5WXKZ6GsTbaaQ0XZ5cUzHFDKOcVXOE6WQsedAFvq/Hk8Vb+YVL7ibtPIArqSko0Od0njvUf8RMaoXN3tR5TutGd98/Np3otFzM6GQ8RxrLSNRyNjDanqhf6RZC4z9eT+m/HrQWoURx4t1y2Jgqzsc91RSbp430dcm2qwqWlVqaG4xyN89Q4BCxjOctY2ikoQmFj0Tbc4q9N//MBdLE0ph3rYTVjw2r1YMPRvejmoxBbTFpzc7NtHXJtpMIePNLPONsSlkfMMPLs+CQsYeOpMLfY9/zsdPxwJRMG8I/t31PlQt4Z8CJisiTPS1iTZTyNirgf7YmkLG4V6lkLHnQFML/awNR/C3WVsRGhSAaS/VQe2yzn60WsbrJvraRJspZGSywb/bUMg43L8UMvYcaFKhv5SUilV7z2LZH6cxZ/Mx6029Hz1dHU/cVcoeNIe2NsnXJq9CUcg4NEHdOG0KGTfC9EZXFDL2qJtycft+8zH0nbXV+ip11tGyVDrGvtzK9kf17BH2ndam+Ppq4ibaTCHjOznnrZlQyHiLvJvGpZCxB9KEQr/uz/N4btJv1gpMvQrRaFzlNjxUMRqbVy9F69b2H0+1R9h3Wpvg6+tpm2gzhYzv5Jy3ZkIh4y3ybhqXQsYeSH8v9AfPJqDNv1bjwpVUvP5wBbzRpKIFyN/tzikKaHOgveRwcGsVX6vUUAcj86upU8g43J0qSaiS/E7F5s82xyemWiLmwJkEtKpeHGPb18p+N4w/232zWKTNFDIydUqlhsr0zzb6CVDI6GesdQSVJGSh959Cn5Cchhe+WAfx8ceapaLwn251EB4SlB179LX/+PpWBcVEP6uuOKrUUK3FnZ1LE6CQkUblmw1VktDEouePNiempOPFyevw24HziI2OwPTu96NIZNg1AeuPdueWkbTZDPFGIZNbJvj/7xQyDvcxhYw9B/rbxS0pNR0vfbUBK/eeRalCeTCj+/0oXiDPDVD8zW4Zr9NmChmZOFGpoTL9s41+AhQy+hlrHUElCVnonVvoT11MwjfrDlv/nLqYjBIFwq2VmFKF8uYYb/S1c31tp4CY6GeuyNiJEP9sSyHjcL9SyNhzoJMLfVp6Bn7ZcwYzNhzBkl2nkZ6RaRlfo1QUPnmmJspGR9wUhpPttufh/7WmzWaINwoZVzPEf86jkHG4Lylk7DnQiRc38YbeL1cfxNTfDuHMpWTL4LDgQDxaowQ61CljCZncDifanZtNuf1OmylkcosR8btKDZXpn230E6CQ0c9Y6wgqSchC79uF/kpKGqb8eggTV+xH3JVUK46qlyyAp+4phUerl0CBvCHSsUVf+7avpR2ZS0MT/cwVGXdFj3P7oZBxru+smVPI2HOgUwp9RkYmnvl0DdYfvGAZ2LBSEfRpUhHVS+a++pITEafYbc+bt25Nm80QbxQy7swaZ/ZFIeNMv2XPmkLGngOdcnGbveko+kzfgpioPBjTvhbuLlPQnqHXtXaK3UpG0mYj3+BMIePOrHFmXxQyzvQbhYyLfnPCBV3cUmr04S84eTEJEzrcjWbVirlo7f9Oc4LdykZSyFDIuPAtMZU/Bt0ds+zPNQIUMq5x85mzVJKQFzfPLb1nZmZi3tYT2HIkDomp6dY/wYEBiAwPQWR4MKqWKIDGVYpanxT4ePEefLJ0L+rEFsI3L9XJ/syAStDR157ztYqfVM810c9ckVGNGuefTyFj04fp6eno378/Jk+ejKSkJDRr1gwTJkxA4cKFc+zp9OnT6Nu3L+bNm2ftZ4mNjcWCBQtQokQJq31aWhref/99q7+zZ8+iWLFiGDduHJo3by41MwoZKUzZjbxV6L9acxADv99xy8nWKFkAXevFou+sLUhOy8C81x60BI47Dm/Z7Y65u9oHbTZDvFHIuJoh/nMehYxNXw4bNgxTpkzBokWLULBgQXTs2DF7Off6roTQqV27NurUqYPhw4ejUKFC2LVrF0qVKoX8+fNbzbt27YodO3bgyy+/RKVKlXDixAmkpKSgbNmyUjOjkJHC5FUhs2b/OXT4fC0yMjPRp3FFa9+L+A5SemYmLiamIu5KCqZvOIIj5xOz59mudimMeLK6PeNu0ZoXdTMu6ib6mULGbWXCsR1RyNh0XZkyZTBw4EB06dLFOnP37t2oXLkyjhw5gpIlS17T28SJEzF06FAcOHAAISE3Piqbda4QN6IPVw4KGXvUPF3oj5y/gkfHrcKFK6no27QSeja8PccJJ6elY9pvhzF22V4EBQbgx171b/hekj1Lr23tabtV5uquc2mzGeKNQsZdGePcfihkbPguPj4eUVFR2LRpE2rWrJl9ZkREBGbOnIkWLVpc01u7du1w4cIFlC5dGrNnz0Z0dDRefvll9OrVy2onbkn169cPQ4YMwahRo6y9EK1bt8YHH3yAfPny5TgzcWtLFOisQwgZMb5Y/clJLN3KPNHP/Pnz0bJlSwQGmlH0PGmz2LTbdsJv+OPkJbS6szg+aVcj1/0uQtCkpmciX1iwjcjMvakn7c59Np5pQZvNyOksIeNqLRM1NDw83FoJt1tDPRPJHCU3AhQyuRG66nex6iJEiVhhKVeuXPYvMTExlhARwuXqo3Hjxli6dClGjx5tCZitW7dae2rGjh2L9u3bW6s17777rnWeWL1JSEjAE088gerVq1v/ntMxePBgS/hcf8yaNQvBwe69+NlAw6bXEcjMBKbtC8T6s4GIyZuJ3tXSERpETCRAAr5GQOxTbNu2LYWMrznGxnwoZGzAiouLs/bFyK7ItGnTBuvXr8fRo0ezR+nduzeOHz+OGTNm4JNPPoH497179+L22/+65TBnzhx069YNYpNwTgdXZGw4LIemnvor/T/rj2DA7O3WE0lzX62L0jf5mKOaNfJne8pu+Rnpb0mbuSIjE2VckZGh5NttKGRs+kfskRk0aBA6d+5snblnzx5rk25Oe2TEysmkSZOs37IOIVzEht7p06fjl19+wUMPPYR9+/ahfPny2UKme/fuOHXqlNTMuEdGClN2I0/sm9h+LB5PjP8VKWkZ+PT5u/FIVfX3wNiz8sbWnrBbdY7uPp82myVk5s6da92at3ubXKWGujtm2Z9rBChkbHITTy1NnToVCxcutFZnOnXqZD1WLR6vvv44dOgQqlSpgpEjR6JHjx7Yvn07xO0m8Xj1M888Y+11EXttsm4liVtLYhVH/Pv48eOlZqaShCz07i/04gOPrcauwqFzV9CtfiwGtKgi5Ufdjehr9/tat89c6d9EPwtOKnar1FBXfMRz3E+AQsYmU3FrR2zQFe99SU5ORtOmTa39LOI9MtOmTYNYTbl8+XJ2r8uXL0efPn2slRvx7hixItOzZ8/s34XYEftnVqxYgQIFCuDJJ5+0HtUWG3hlDpUkVEl+mbn5YhudNouX3vWevhnfbz6Oe8oUxDfd6iAkyDcuoDrt9kU/q17cfNWm3OZlop9Vfa1SQ3PzB3/3DAEKGc9w1jaKShKaWPR02vztxqN4c+YW5A8Pxo+961vvi/GVQ6fdvmLj9fOgzb4hoj0RHyq+VqmhnrCNY+ROgEImd0Y+3UIlCVWS36eh3GJyumw+eDYBLcesREJKOsY/dxea31ncpxDpstunjLxuMrSZQkYmPlVqqEz/bKOfAIWMfsZaR1BJQhZ69xT6Cwkp6PjlOmw9Go/295bC8Cfc90ZedwUPfe0eX7vLH7r6MdHPvLWkK5qc0y+FjHN8leNMKWTsOdCdhf7UxSR8tuIAvl53GFdS0lG+SATmvvYg8ob63vt83Gm3PeLea02bzRBvFDLeyzFfGZlCxlc84eI8KGTsgXPHxU1s6p3y60H848c/rEesAwKAFncWR/9mlVHKy++LuRkNd9htj7T3W9NmChmZKFSpoTL9s41+AhQy+hlrHUElCVno7Rf6i0mp6P/tVizYdtLy65N3lcQrDcujfJGcPymh1fk2Oqev7fvaBl6faWqin7ki4zPh57WJUMh4Db17BqaQscdRpdDvPXUJL321AQfPXUHBvCH4+JmaeKhSUXsT8FJrFbu9NGXlYWmzGeKNQkY5VRzfAYWMw11IIWPPga5e3NYeOGeJmItJadY7YsY+WwvFC/jO49W5UXDV7tz69eXfaTOFjEx8qtRQmf7ZRj8BChn9jLWOoJKELPQ3L/SXk9MQGACEBwfhx+0n0Wf6ZqSkZ+CJu2LwwZPVfeZFd7LBRV+bcVE30c9ckZGtAv7bjkLG4b6lkLHnQJlCP3bpXoxavOeGjl9teDvefKQiAsTuXocdMnY7zKRcp0ubzRBvFDK5poLfNzBKyKxevRolS5aE+PCj+Lr0W2+9heDgYIwYMQLR0dGOdDaFjD235XZx+3X/WTw3aS2EVCmYNxRJqekICQ5E36aV8Nx9ZewN5kOtc7Pbh6bqtqnQZgoZmWBSqaEy/bONfgJGCRnxMcbvvvsOt99+O1588UUcPXoU4eHhyJs3r/U1aiceKknIQn9toT+fkILmn6zAqYvJGNCiMrrV/+uL5P5w0NdmXNRN9DNXZPyhQqnZYJSQEV+rvnDhAsR7QIoWLYodO3ZYIiY2NtZaoXHiQSFjz2s3K/QiJsRm3iW7TqN+xSKY3Kk2AsUmGT85TLzA0WYzxBuFjJ8UKQUzjBIy4vbRkSNHsGvXLnTs2BHbtm2zPv8uvjp96dIlBYzeO5VCxh77m13c/v3bIbwzZzui84Xhx171UCQyzF7HPt6aF3UzLuom+plCxseLjwemZ5SQefrpp5GYmIhz587h4Ycfxvvvv4/du3ejVatW2Lt3rwdwu38IChl7THMq9OLtvPX+b5l1S+nLF2ujoUPeDWPHchMvcLTZDPFGIWOnEvhnW6OETFxcHEaOHInQ0FBro2+ePHkwb9487N+/H7169XKkhylk7Lktp4vbzA1H0HfWVtQqHYXvXn7AkU8l5UaBF3UzLuom+plCJrfs9//fjRIy/uhOChl7Xr2+0Iu9MU1Hr8CeU5cxocNdaFatuL0OHdLaxAscbTZDvFHIOKQIaZym3wuZ9957TwrfwIEDpdr5WiMKGXseuf7i9vMfp/Hi5PUoWzgvlr75EIL8aIPv1WR4UTfjom6inylk7NVAf2zt90KmSZMm2X4Tf32vWLECxYoVs94lc+jQIZw8eRINGjTA4sWLHelfChl7bru+0Lf7dA1+O3AeQx+vhg51nPuemNwomHiBo81miDcKmdyy3/9/93shc7UL33jjDevFd2+//Xb2Pojhw4fj7NmzGDVqlCO9TSFjz21XX9y2H7+IR8etRuGIUKzu3wjhIUH2OnNQa17Uzbiom+hnChkHFSJNUzVKyBQpUgQnTpyw3uabdaSlpVkrNELMOPGgkLHntasL/WvfbMb8bSfQp3FF9GpcwV5HDmtt4gWONpsh3ihkHFaMNEzXKCFTqlQpzJ07FzVr1sxGuWnTJrRu3dp6y68TDwoZe17LurhVuvchNB+zCnlCgrC6XyMUjAi115HDWvOibsZF3UQ/U8g4rBhpmK5RQkbcRvrkk0/QvXt3lC1bFgcPHsSnn36K1157DQMGDNCAV3+XFDL2GGcV+sUJpTBv6wn0aFAe/ZtXtteJA1ubeIGjzWaINwoZBxYkN0/ZKCEj2H311VeYOnUqjh07hpiYGDz//PN44YUX3IzVc91RyNhjLS5un02fixFbgxEeHIRV/RqicD7/eotvTkR4UTfjom6inylk7NVAf2xtjJBJT0/HrFmz8PjjjyMszH8uXBQy9tJSFPqnPpqPjWcD0b1+LN5uUcVeBw5tbeIFjjabId4oZBxalNw4bWOEjGAWGRnp2G8q3cznFDL2smHfqYto8vEKhIcEY2W/hta3lUw4eFE346Juop8pZEyoYLe20Sgh06hRI4wePRrVq1f3G89TyMi78o+TF/H2t9uw6Ugcuj5YDu+0ukP+ZIe3NPECR5vNEG8UMg4vTm6YvlFCZujQofjss8+szb7ihXgBAQHZCJ999lk34PR8FxQyuTM/fSkJHy7ajVkbjyIjEygQmomf3nwYtxXIk/vJftKCF3UzLuom+plCxk+KlIIZRgmZcuXK5YhKCJoDBw4oYPTeqRQyt2Yv3ubcYswq7DpxEWHBgejyYDmUSdiNp9q0RmCgGRc31ULvvehWG9nEi7qJNqvGt0oNVYtQnu0uAkYJGXdB86V+VJLQhKL36/6zePaztShVKA9mdL8ft0WGWe8SEu8OopDxpUh2/1xMiO/rqZloM4WM+3PHaT1SyDjNY9fNl0Lm1g58ZdpGLNh2Eu+0rIKu9WLBQm+OgDPR1ybaTCHj8IuYG6ZvlJBJTEyE2CezdOlSnDlzBuK2Q9bBW0v+d5vlZHwS6n6wDMGBAVg74GFE5Q2lkDFoJcrEi7qJNlPIuEEJOLwLo4RMjx49sGrVKrz88svo168fPvjgA4wbNw7PPfcc3nnnHUe6kisyN3fbx4v34JOle/HU3SUx8qkaVkMWeq7IODLRJSfN+LYf3yo1VNItbKaZgFFCRrzJd+XKlYiNjUVUVBTi4uKwc+dO6xMFYpXGiYdKEvpz0UtNz0DdEctw+lIy5r76IO4sWYBCxrC9Qf4c3zerVSbarPoHikoNdeI1wx/nbJSQKVCgAOLj4y0/Fi1a1PpQZGhoKPLnz4+LFy860r8qSejPRW/BthN4ZdrvqFEqCt/3rJvtW3+2+VYBbKLdtNn/bhfrEHAqNdSRFw0/nLRRQkZ89fqbb75BlSpVUL9+fYh3x4iVmb59++LIkSOOdK9KEvpzoX/2s9/w6/5z+PCpGmh7d0kKmYwM457W8uf41nFBd2QB/O+kVXytUkOdzMyf5m6UkJk+fbolXJo2bYrFixejTZs2SE5Oxvjx49G1a1dH+lUlCVWS35dhXU5OQ/XBixAWHIRNA5sgPCSIQoZCxpdD1m1z89eczg2Qit0qNTS3efF3zxAwSshcj1QEcEpKCiIiIjxDW8MoKkmokvwaTHFblyv2nMELX6zDA+UL4+uX6lzTr7/anBs8E+2mzby1lFteiN9VaqhM/2yjn4BRQkY8pfTII4+gVq1a+sl6aASVJPTXQv/RT7sxZtk+9Hq4Avo0qUghY+jTWv4a37cqLSbaLHio2K1SQz1U5jlMLgSMEjKPPvoofvnlF2uDr/iAZOPGjdGkSROULVvWsYGikoQqye/LwNp9uga/HTiPf3e5Dw9WiKaQUSz0vuxrXtSvJeCvOZ1bDKrYrVJDc5sXf/cMAaOEjECanp6OtWvXYsmSJdY/69atQ6lSpbB3717PEHfzKCpJqJL8bjbDbd2lpGWg+pBFSE3PxNZBjyAiLJhChkLGmM9R+GNOyxQHFbtVaqjM3NhGPwHjhIxAum3bNvz000/Wht81a9agWrVqWL16tRRtIYT69++PyZMnIykpCc2aNcOECRNQuHDhHM8/ffq09VTUvHnzrHux4h02CxYsQIkSJa5pLx4Fr1q1KooUKYJ9+/ZJzUU0UklCleSXnqCHG/5++AKe+NevqFGyAL5/9cEbRvdHm2UQm2g3beYeGZncUKmhMv2zjX4CRgmZ559/3lqFKViwoHVbSfzTsGFDREZGSpMeNmwYpkyZgkWLFln9dOzYMfv+7PWdCKFTu3Zt1KlTB8OHD0ehQoWwa9cuawVIvLvm6kMIIpFQhw4dopCR9saNDT9dsR//WPCH9ZXrd1vdQSHzXwK8qJtxUTfRzyLEVeymkFEouD5yqlFCJm/evChZsiSEoBEi5r777rO95FymTBkMHDgQXbp0sVy4e/duVK5c2XoPjej76mPixInWt53Ed5xCQkJu6vLPPvsMs2fPxtNPP22154qM69nRdcoGLNl1ChM63I1m1YpRyFDIGPWlc5ULuutZ5/0zVeymkPG+/1RnYJSQEY9ai28tZe2P2b9/P+rVq2dt+O3Zs2euLMVbgcV7aDZt2gTxcr2sQzy+PXPmTLRo0eKaPtq1a4cLFy6gdOnSllCJjo62vvPUq1ev7HaHDx9G3bp1rVtcYl65CRlxa0skbdYhklCML1Z/biWWcjJO9DN//ny0bNnStqDLFZYXGmRkZKL2P5biwpVUrBvQCNH5wnIUMv5ksyxmf/O1jN202YxVqKwVGVfzWtTQ8PBw61UcdmuoTByyjX4CRgmZq3GKlZQZM2Zg1KhRuHTpkrUJOLdDrLoIUSJWWMqVK5fdXHzDSfQjhMvVh1j1Ed9wGj16tCVgtm7dau2pGTt2LNq3b281FSKqbdu26N69u7XvJjchM3jwYAwZMuSGqc6aNQvBwddubM3NHn/7/eQVYPiWYBQNz8Tfa+XuT3+zn/aQAAnYJ5CWlmbVYAoZ++x85QyjhIx4s6/Y4Cv+OXXqlHVr6eGHH7bExP3335+rT8RHJsW+GNkVGfHm4PXr11vfdMo6evfujePHj1siStx6EnMSYicgIEBKyHBF5uZu+mbdYfx9zg48fU9JjHjizhwbmvhXuupfrLkmho82MNHXJtqsGt9ckfHRBLYxLaOETPXq1bM3+TZo0MClN/qKPTKDBg1C586dLcx79uxBpUqVctwjI1ZOJk2adM13nISQOXHihCVgHn/8cfz888/IkyeP1VdiYiISEhKsW1Diyaa77rorV1eq3N9Vua+c68S80KDP9M2YvenYDd9Xunoq/mazLGYT7abNZt1amuvi191Vaqhs/rGdXgJGCRl3oBRPLU2dOhULFy60Vmc6depkPW0kHq++/hBPIIkPVI4cORI9evTA9u3bLSE1btw4PPPMMxArPGJvS9YhxI24DSX2y4jHuWXu16okoT8V+rT0DNT/v59xPD4JK/o2ROnCeW+6IuNqwXNH/HirD3/ytSxD2kwhIxMrKjVUpn+20U/AOCEjNvt+9dVX1qqIuKBt3LjRWgURX8OWOcStnX79+lm3gcQHJ8UHKMUtIiE8pk2bZu11uXz5cnZXy5cvR58+fayVG/HuGLEic7ONxTJ7ZK6fo0oS+lOh/2zFAQxbsAsViubDT33qW7fqcjr8yWaZeM1qY6LdtJlCRiZHVGqoTP9so5+AUULm66+/xquvvooOHTpY74IRTyH9/vvveOONNyAEhxMPlST0l0J/6FwCmo5egaTUDMzofj/uLVfopq70F5vtxqqJdtNmChmZPFGpoTL9s41+AkYJGfHmXCFg7rnnHuu2kHg0WuxUF08dnTlzRj9tDSOoJKE/FPrMzEw8+9larDlwDh3qlMbQx3Pe5GvyyoSw3R98bTd9aDOFjEzMqNRQmf7ZRj8Bo4RMlngRWMVbds+fP28VeLG5Vvx/Jx4qSegPhf4/6w6j/3fbULxAuHVLKTL85i8eNPWCbqrd/hDfdmuSiTarxrdKDbXrH7bXQ8AoISNWYsaMGYMHHnggW8iIPTPiW0hig60TD5UkdHrRS0xJx33/WIKLSWn4otM9aFT5tlxd6HSbczXwJg1MtJs2c0VGJl9UaqhM/2yjn4BRQmbOnDl46aWXrDfrfvDBBxAvlxNPCX366ado3ry5ftoaRlBJQqcX+qW7TqHLlA3WnhixN0bmcLrNMjbm1MZEu2kzhYxMvqjUUJn+2UY/AWOEjHjaSLz9VrzOXzxl9Oeff6Js2bKWqBEvxHPqoZKETi/0A2Zvw9drD+OdllXQtV6slAudbrOUkTk0MtFu2kwhI5MvKjVUpn+20U/AGCEjUIqvXIvPEfjToZKETi70YpPv/cOX4eTFJCx7swFii+STcquTbZYykLeWsgmY6GsTbRYOV7FbpYaq5CTPdR8Bo4RMo0aNrFtJ4g2//nKoJKFK8nub347j8Wg5ZhXKRUfg5789JD0dJ9ssbSRXZCwCJvraRJtVfa1SQ1Vykue6j4BRQkZ8kPGzzz6zXlonPjVw9UvTnn32WfdR9WBPKkno5KI3ZulefLR4D7o+WA7vtLpDmriTbZY2kkKGQqZ1a7/4or1szKvktUoNlZ0f2+klYJSQufqL1VdjFYJGfNHaiYdKEqokv7dZPfbP1dhyJA5fv3QfHigfLT0dJ9ssbSSFDIUMhYx0uqjUUOlB2FArAaOEjFaSXupcJQmdelE/cykZtYctQWRYMH4f2AQhQfKbGp1qs2p4mWg3bZbPC9X48vb5Kr5WqaHetpvj/0WAQsbhkaCShCrJ701sMzYcwVuztqJl9eL457O5fyH86rk61WZV3ibaTZspZGTyRqWGyvTPNvoJUMjoZ6x1BJUkdGqh7zF1IxbuOImPnq6BJ+4qaYuvU222ZSRvLfHWEm8tSaeMSg2VHoQNtRKgkNGKV3/nKknoxIt6Umo67n5/MRJT07HhnSYoFBFqC7ITbbZl4E0am2g3beaKjEzuqNRQmf7ZRj8BChn9jLWOoJKETiz009Yewt9nb8f9sYXxTbc6ttk60WbbRnJFhisyXJGRThuVGio9CBtqJUAhoxWv/s5VktBpF/W09Aw0GvULDp+/gi9frI2GlYraBuw0m20byBWZbAIm+tpEm4XDVexWqaHuyk/2o0aAQkaNn9fPVklCleT3huHfbz6GXv/ZjDuK58f81x+85j1AsvNxms2yduXWzkS7aTNvLeWWF+J3lRoq0z/b6CdAIaOfsdYRVJLQSYVefJKg+Scr8cfJSxjbvhZa1yjhElcn2eySgVyR4YrM3LlozVtL0umjUkOlB2FDrQQoZLTi1d+5ShI66aK+7I9T6Dx5A8oWzoulbz6EoMAAl+A6yWaXDKSQoZChkLGVOio11NZAbKyNAIWMNrSe6VglCZ10UW87/ldsOHQBw5+4E5FzTTUAACAASURBVO3vLe0yXCfZ7LKROZxoot20mbeWZHJIpYbK9M82+glQyOhnrHUElSR0SqHfefwiWoxZiaKRYVjZryHCgoNcZuoUm102kCsyXJHhioyt9FGpobYGYmNtBChktKH1TMcqSeiUi/qEX/ZjxI9/4MW6ZTGodVUlsE6xWclIrshYBEz0tYk2q/papYa6O0/Zn2sEKGRc4+YzZ6kkoVOKXodJa7Fq31l83vEePFzlNiX2TrFZyUgKGQoZbvaVTiGVGio9CBtqJUAhoxWv/s5VktAJF3XxJt/qQ35CRkYmNg96BPnCgpWgOsFmJQN5a4m3lnhryVYKqdRQWwOxsTYCFDLa0HqmY5UkdMJFfdXes+jw+VrcW7YQZvS4XxmqE2xWNpIrMlyR4YqMdBqp1FDpQdhQKwEKGa149XeukoROuKgP/3EXJv5yAG80qYjXH66gDNQJNisbSSFDIUMhI51GKjVUehA21EqAQkYrXv2dqyShEy7qLcesxI7jF/Htyw/g7jIFlYE6wWZlIylkKGQoZKTTSKWGSg/ChloJUMhoxau/c5Uk9PWL+rnLybh76BJEhgdj07tNEByk/l4MX7dZV8SYaDdtVs8XXfHo7n5VfK1SQ91tB/tzjQCFjGvcfOYslSRUSX5PAJi75The+2YTHrnjNnz6wj1uGdLXbXaLkVyR4YoMV2SkU0mlhkoPwoZaCVDIaMWrv3OVJPT1i3q/WVsxfcMRvPdYVbxwf1m3wPR1m91iJIUMhQyFjHQqqdRQ6UHYUCsBChmtePV3rpKEvnxRFx+JfPCDn3EsLhHL3myA2CL53ALTl212i4E36cREu2kzby3J5JRKDZXpn230E6CQ0c9Y6wgqSejLhf7AmctoNOoXxETlwap+DREQ4NpHIq+H78s26wwUE+2mzRQyMjmlUkNl+mcb/QQoZPQz1jqCShL6cqH/as1BDPx+B565pxQ+aFvdbQx92Wa3GclbS7y1xFtL0umkUkOlB2FDrQQoZLTi1d+5ShL68kX9pa82YPHOUxjbvhZa1yjhNpC+bLPbjKSQoZChkJFOJ5UaKj0IG2olQCGjFa/+zlWS0Fcv6mnpGaj13mJcTknDxneaoFBEqNtA+qrNbjPwJh2ZaDdt5q0lmbxSqaEy/bONfgIUMvoZax1BJQl9tdBvPHQeT45fg2ox+THvtXpu5eerNrvVSK7IcEWGKzLSKaVSQ6UHYUOtBChktOLV37lKEvrqRX30kj0YvWQvejQoj/7NK7sVoq/a7FYjKWQoZChkpFNKpYZKD8KGWglQyGjFq79zlST01Yt62/G/YsOhC5jW9T7UvT3arRB91Wa3GkkhQyFDISOdUio1VHoQNtRKgEJGK179naskoS9e1C8lpaLme4sRHBiALYMeQXhIkFsh+qLNbjXwJp2ZaDdt5h4ZmdxSqaEy/bONfgIUMvoZax1BJQl9sdCLJ5XEE0v1KkRjapf73M7OF212u5FckeGKDFdkpNNKpYZKD8KGWglQyGjFq79zlST0xYv6oO+3Y8qaQxjQojK61S/vdoC+aLPbjaSQoZChkJFOK5UaKj0IG2olQCFjE296ejr69++PyZMnIykpCc2aNcOECRNQuHDhHHs6ffo0+vbti3nz5kEkTGxsLBYsWIASJUpgz549GDBgANasWYOLFy+idOnS6NOnD7p27So9K5Uk9MWLeqNRy3HgTAIWvF4Pd5TIL81BtqEv2iw7d5V2JtpNm3lrSSZnVGqoTP9so58AhYxNxsOGDcOUKVOwaNEiFCxYEB07dkRWwby+KyF0ateujTp16mD48OEoVKgQdu3ahVKlSiF//vxYu3YtNmzYgDZt2qB48eJYuXIlWrduja+++gqPPfaY1MxUktDXCv3xuEQ8MGIZovOFYt2AxggMdM9nCa4G6Ws2SznZDY1MtJs2U8jIpI5KDZXpn230E6CQscm4TJkyGDhwILp06WKduXv3blSuXBlHjhxByZIlr+lt4sSJGDp0KA4cOICQkBCpkYSoKVeuHD766COp9ipJ6GuF/svVf2LI3J14vGYJjG5XS8p+u418zWa783e1vYl202YKGZl8UamhMv2zjX4CFDI2GMfHxyMqKgqbNm1CzZo1s8+MiIjAzJkz0aJFi2t6a9euHS5cuGDdMpo9ezaio6Px8ssvo1evXjmOmpCQgNtvvx0jRoywVnpyOsStLVGgsw6RhGJ8sfojK5ayzhX9zJ8/Hy1btkRgoHeLnvjadcuxq/HHyUv4ouM9eKhSERuekW/qSzbLz1q9pYl202bv5rR61Mr3oOJrUUPDw8ORkpJiu4bKz5AtdRKgkLFBV6y6CFEiVljEqknWERMTg1GjRkEIl6uPxo0bY+nSpRg9erQlYLZu3WrtqRk7dizat29/Tdu0tDS0bdsWcXFxWLJkCYKDg3Oc2eDBgzFkyJAbfps1a9ZNz7FhoteaHr4MjNoWjKjQTAy6Kx0a7ip5zTYOTAIk4LsEsmovhYzv+ii3mVHI5Eboqt+FyBD7YmRXZMRtovXr1+Po0aPZvfTu3RvHjx/HjBkzsv+bSCAhgs6cOWNtBI6MjLzprPx1ReadOdvx9bojeK1hefRpUtGGV+w1VfnLzd5IvtXaRLtpM1dkZLKQKzIylHy7DYWMTf+IPTKDBg1C586drTPFk0eVKlXKcY+MWDmZNGmS9VvWIYTMiRMnMH36dOs/JSYm4oknnrCWNX/44QfrNpGdQ+X+rq/sIUhMSce9w5bgUnIaVr7VEKUK5bWDwFZbX7HZ1qTd0NhEu2mzWUJm7ty51sMSdm+Tq9RQN6Qmu3ADAQoZmxDFU0tTp07FwoULrdWZTp06WY9Vi8errz8OHTqEKlWqYOTIkejRowe2b98Ocbtp3LhxeOaZZ3D58mW0atUKefLksfbQiPu0dg+VJPSVQv/d70fxxowtePD2aPy7q/tfgnc1U1+x2a6fVdubaDdtppCRyRuVGirTP9voJ0AhY5OxuLXTr18/6z0yycnJaNq0KcTTSeI9MtOmTUP37t0tgZJ1LF++3Ho3jFi5Ee+OESsyPXv2tH4Wj3ELISSEzNV/RXTo0MF6N43MoZKEvlLon564Buv+PI8x7Wvh0RolZMx2uY2v2OyyAS6eaKLdtJlCRiZdVGqoTP9so58AhYx+xlpHUElCbxX6X/efRY+pG5EnNAjF8odjy9F4FMgTgrUDHnb7t5Wuh+8tm7UGgUTnJtpNmylkJFLDWlEPDQ3lU0sysHy0DYWMjzpGdloqSeitQt/rP5vw/ebj15j4Ur1y+HvLO2TNdrmdt2x2ecJuOtFEu2kzhYxM+qjUUJn+2UY/AQoZ/Yy1jqCShN4o9OkZmbhn6GJcuJKKRb3rQ/x7XGIK7i5TEGHB7v3SdU7gvWGz1gCQ7NxEu2kzhYxMeqjUUJn+2UY/AQoZ/Yy1jqCShN4o9JsOX0Cbf/2KysUisbB3fa1sKGT+R8Abvva4c68bkDZTyMjEoEoNlemfbfQToJDRz1jrCCpJ6I1C//HiPfhk6V50bxCLt5tX0cqGQoZCxtVHcj0emG4a0Bs57aapK3WjYrdKDVWaNE92GwEKGbeh9E5HKkmokvyuWvvYP1djy5E4fPNSHdxfPucvhrvat8x53rBZZl6625hoN23mioxMXqnUUJn+2UY/AQoZ/Yy1jqCShJ4u9OcuJ+OeYUsQERqMTQObICTI84XW0zZrdb6Nzk20mzZ7Pr9shKRbm6r4WqWGutUIduYyAQoZl9H5xokqSaiS/K5YP2fTMfSevhlNq96Gic/f40oXyud42mblCbupAxPtps0UMjLpo1JDZfpnG/0EKGT0M9Y6gkoSerrQ9/7PJszZfBzDn7gT7e8trZXLzTr3tM1eMTKHQU20mzZTyMjkn0oNlemfbfQToJDRz1jrCCpJ6MlCnyEeux62BOcTUvBr/0YoEZVHKxcKmWsJeNLXXnEsxZtFwEQ/q9qtUkN9JdZNnweFjMMjQCUJPVn0xAZfsdG34m358FOfBl6j7kmbvWYkL+rGXtQZ3/xopC/VHU/NhULGU6Q1jeMUITPixz8w4Zf96FY/FgNaeP6x6yz8LPT2C72m0NXerYm+NtFmrshoTyWfH4BCxudddOsJOkHIxF9JxYMfLMOl5DT82KseqhTP7zXqLPQUMl4LPg8MzPi2H98qNdQDLuUQEgQoZCQg+XITlST0VNHLegmeN59W4opMBvhyOF/OZPfMzVM57Z7Zuq8XFbtVaqj7LGBPKgQoZFTo+cC5Kkmokvyypl9MSkXdEctwKSkN8157ENViCsieqqWdJ2zWMnHFTk20mzbzqSWZtFGpoTL9s41+AhQy+hlrHUElCT1R6Mcs3YuPFu9B4ypFMaljba0sZDr3hM0y8/B0GxPtps0UMjJ5plJDZfpnG/0EKGT0M9Y6gkoS6i70l5LE3pifEZ+Yih9erYvqJaO0spDpXLfNMnPwRhsT7abNFDIyuaZSQ2X6Zxv9BChk9DPWOoJKEuou9OOX78cHC/9Aw0pF8OWL92rlINu5bptl5+HpdibaTZspZGTyTKWGyvTPNvoJUMjoZ6x1BJUk1FnoMzMz8dCHy3Ho3BVM71YH98V6/gOROYHXabNWRyt2bqLdtJlCRiZtVGqoTP9so58AhYx+xlpHUElCnYV+/cHzeGrCGpSLjsCyNxsgICBAKwfZznXaLDsHb7Qz0W7aTCEjk2sqNVSmf7bRT4BCRj9jrSOoJKHOQv/WrC2YseEo+jathJ4Nb9fKwE7nOm22Mw9PtzXRbtpMISOTZyo1VKZ/ttFPgEJGP2OtI6gkoa5CfyUlDbWHLsGV1HSs7ue97yrx1tL/COjytdbgVuycNlPIyISQSg2V6Z9t9BOgkNHPWOsIKkmoq9B/9/tRvDFjC+pViMbULvdptd9u57pstjsPT7c30W7aTCEjk2cqNVSmf7bRT4BCRj9jrSOoJKFKof9y9Z/49vejKF0oL2Kj86FqifxofMdtCAkKRPtPf8OaA+fwSbuaeKxmjFb77XauYrPdsXypvYl202YKGZkcVKmhMv2zjX4CFDL6GWsdQSUJXS304omk2sOW4OzllGtsE6KmQ53S+MeCPxAZHoz1f2+M8JAgrfbb7dxVm+2O42vtTbSbNlPIyOShSg2V6Z9t9BOgkNHPWOsIKknoaqE/dC4BDUYuR0xUHgxqfQf2n0nAvK3HseP4xWxbn72vNP7R5k6ttrvSuas2uzKWL51jot20mUJGJgdVaqhM/2yjnwCFjH7GWkdQSUJXC/23G4/izZlb8HjNEhjdrpZln1ilWbLrNEYv2YMDZxIwu+cDqFzMe1+5vhl0V23W6kQPdG6i3bSZQkYmtVRqqEz/bKOfAIWMfsZaR1BJQlcL/YDZ2/D12sN4/7GqeP7+sjfYl56RiaBA33hvzPWTc9VmrU70QOcm2k2bKWRkUkulhsr0zzb6CVDI6GesdQSVJHS10Df9eAV2n7qEBa/Xwx0lfG/V5VbAXbVZqxM90LmJdtNmChmZ1FKpoTL9s41+AhQy+hlrHUElCV0p9OIDkDXf+wkRocHYMugRn1154a2lawm44mutgeuBzmkzhYxMmKnUUJn+2UY/AQoZ/Yy1jqCShK4U+l/2nEHHL9b55DtiZEC7YrNMv77exkS7aTOFjExeqtRQmf7ZRj8BChn9jLWOoJKErhT6j37ajTHL9qHXwxXQp0lFrbbp6NwVm3XMw9N9mmg3baaQkckzlRoq0z/b6CdAIaOfsdYRVJLQlUL/3KTfsHrfOUztci/qVSii1TYdnbtis455eLpPE+2mzRQyMnmmUkNl+mcb/QQoZPQz1jqCShLaLfRp6RmoPuQnJKWmW/tjIsNDtNqmo3O7NuuYgzf6NNFu2kwhI5NrKjVUpn+20U+AQkY/Y60jqCSh3UK//Vg8Wo1dhSrF8+PHXvW02qWrc7s265qHp/s10W7aTCEjk2cqNVSmf7bRT4BCRj9jrSOoJKHdQj/l14MY9MMO6zMEQx/3vbf2yoC2a7NMn05oY6LdtJlCRiY3VWqoTP9so58AhYx+xlpHUElCu4X+9W824YctxzH6mZp4vJZvfQxSFrJdm2X79fV2JtpNmylkZPJSpYbK9M82+glQyOhnrHUElSS0U+gvJKTgoQ+XQ7xHZuVbDVGqUF6tdunq3I7NuubgjX5NtJs2U8jI5JpKDZXpn230E6CQ0c9Y6wgqSWin0Gd9lqDJHbfhsxfu0WqTzs7t2KxzHp7u20S7aTOFjEyeqdRQmf7ZRj8BChn9jLWOoJKEsoV+y5E4PP6v1QgNCsSSNxo4djVGOELWZq1O80LnJtpNmylkZFJNpYbK9M82+glQyOhnrHUElSSUKfQZGZlo86/V2HI0Hm82qYjXHq6g1R7dncvYrHsO3ujfRLtpM4WMTK6p1FCZ/tlGPwEKGf2MtY6gkoQyhf6bdYfx9nfbUKZwXizqXR/hIUFa7dHduYzNuufgjf5NtJs2U8jI5JpKDZXpn230E6CQsck4PT0d/fv3x+TJk5GUlIRmzZphwoQJKFy4cI49nT59Gn379sW8efMgEiY2NhYLFixAiRIlrPb79u1Djx49sGbNGhQsWBB/+9vf0Lt3b+lZqSRhboU+JS0DD4xYirOXU/Dli7XRsFJR6Xn5asPcbPbVeavOy0S7aTOFjEzeqNRQmf7ZRj8BChmbjIcNG4YpU6Zg0aJFlvDo2LFj9r6L67sSQqd27dqoU6cOhg8fjkKFCmHXrl0oVaoU8ufPDyGKqlWrhiZNmmDEiBHYuXOnJYwmTpyIJ598UmpmKkmYW6FftOMkuk/diJqlojCnZ12p+fh6o9xs9vX5uzo/E+2mzRQyMvmiUkNl+mcb/QQoZGwyLlOmDAYOHIguXbpYZ+7evRuVK1fGkSNHULJkyWt6E4Jk6NChOHDgAEJCbnyd/88//4yWLVtCrNrky5fPOvftt9/Ghg0bsHjxYqmZqSRhboX+pa82YPHOUxjWphqeu6+M1Hx8vVFuNvv6/F2dn4l202YKGZl8UamhMv2zjX4CFDI2GMfHxyMqKgqbNm1CzZo1s8+MiIjAzJkz0aJFi2t6a9euHS5cuIDSpUtj9uzZiI6Oxssvv4xevXpZ7UaPHm3dotq8eXP2eaKfnj17WuImp0Os4ogCnXWIJBTji9WfnMTSrcwT/cyfP98SU4GB1xa9c5eTcf+InxEYGIB1bzdC/jzO+65STrbfymYboeC4pibaTZvNEjI3q2W5JauooeHh4UhJSbFdQ3Prm797hgCFjA3OYtVFiBKxwlKuXLnsM2NiYjBq1CgI4XL10bhxYyxdutQSLELAbN261bp1NHbsWLRv3x7vv/8+lixZgl9++SX7NLES07p1a0uY5HQMHjwYQ4YMueGnWbNmITg42IY1t266/EQAZh8MQq3CGehU8X/CyW0DsCMSIAES8AECaWlpaNu2LYWMD/jC1SlQyNggFxcXZ+2LkV2RadOmDdavX4+jR49mjyI28h4/fhwzZszw6RWZ1uNWY8fxi/iy0z1oULGIDUq+3dTEv9KFR0y0mzZzRUamGnFFRoaSb7ehkLHpH7FHZtCgQejcubN15p49e1CpUqUc98iIlZNJkyZZv2UdQsicOHEC06dPR9YemTNnzli3h8QxYMAAS/x4c4/MrhMX0fyTlSgaGYY1bz+MoMAAm5R8t7mJ+yayhMzcuXOt1b7rbyP6rrfUZmair020WTW+uUdGLc984WwKGZteEE8tTZ06FQsXLrRWZzp16mQ9Vi0er77+OHToEKpUqYKRI0daj1hv374d4nbTuHHj8Mwzz2Q/tdS0aVPrqSbxRJP4/+PHj7eWOmUOlSS8WdEbOm8nJq36E90bxOLt5lVkpuGYNiz0FDKOCVYXJsr4th/fKjXUBRfxFA0EKGRsQhWbbfv162dt0k1OTraEh3g6SbxHZtq0aejevTsuX76c3evy5cvRp08fa+VGvDtGrMiIzbxZh3iPjDjn6vfIiPayh0oS5lT0ftpxEm/O3IJLSWlY3Kc+KtwWKTsVR7Rjobdf6B3h2BwmaaKvTbSZKzJOzVD3zZtCxn0svdKTu4RMfGIaBs/dge83H7fsaFMrBh8/878ns7xinIZBWegpZDSElc90yfi2H98qNdRnHG/4RChkHB4AKkmYVfQaN22BZmNW4sj5RESGB2NgqzvQ9u6SCAjwn70xWW5mobdf6J2aIib62kSbuSLj1Ax137wpZNzH0is9uUPIJBavif7fbUfVEvnxecfaKFYg3Cu2eGJQFnoKGU/EmbfGYHzbj2+VGuotP3PcawlQyDg8IlSSMKvofX6kMLYejceEDnehWbXiDidy6+mz0Nsv9E4NCBN9baLNXJFxaoa6b94UMu5j6ZWeVIXMv76Ziw+3BVuPWq/u3wghQf797gkWegoZrySqhwZlfNuPb5Ua6iG3cphcCFDIODxEVJJQFL3nRs/HmtOBeL3R7XjjkUoOp5H79Fno7Rf63Kn6ZgsTfW2izVyR8c388+SsKGQ8SVvDWCpCJu5KMu4duhhpmQFY2a8RYqLyaJihb3XJQk8h41sR6d7ZML7tx7dKDXWv99ibqwQoZFwl5yPnqSTh5NV/YvDcnXi4clF83qm2j1ikdxos9PYLvV6P6OvdRF+baDNXZPTlkFN6ppBxiqduMk9XhUxmZiaajl6BPacu4/OOd+PhKsUcTkJu+iz0FDJykeLMVoxv+/Htag11ZoT456wpZBzuV1eTcMPB82g7YQ0KhWVi7bvNERIc5HASctNnobdf6OXI+l4rE31tos1ckfG93PP0jChkPE3czeO5KmQSU9Lxw5Zj2LZlM97r3IofEnSzX3ytOxMvcLTZv59AvDrHVHztag31tRw3eT4UMg73vkoSqiS/U7GZaLPqX6z0tXMIML7trziq1FDnRIZ/z5RCxuH+VUlCE4ueiTZTyNi/uDm1LDC+7ftapYY6NU78bd4UMg73qEoSmlj0TLSZQsb+xc2pZYHxbd/XKjXUqXHib/OmkHG4R1WS0MSiZ6LNFDL2L25OLQuMb/u+VqmhTo0Tf5s3hYzDPaqShCYWPRNtppCxf3FzallgfNv3tUoNdWqc+Nu8KWQc7lGVJDSx6JloM4WM/YubU8sC49u+r1VqqFPjxN/mTSHjcI+qJKGJRc9Emylk7F/cnFoWGN/2fa1SQ50aJ/42bwoZh3tUJQlNLHom2kwhY//i5tSywPi272uVGurUOPG3eVPIONyjKkloYtEz0WYKGfsXN6eWBca3fV+r1FCnxom/zZtCxuEeVUlCE4ueiTZTyNi/uDm1LDC+7ftapYY6NU78bd4UMg73aEpKCsLCwpCQkICQkBBb1oiiN2/ePLRqZdYnCkyzOUvImGY349usTxS4Gt9CyERERCA5ORmhoaG2aigb+wYBChnf8IPLs7hy5YqVhDxIgARIgARcJyD+GMybN6/rHfBMrxGgkPEaevcMLP7qTEpKQnBwMAICAmx1mvWXiCurObYG8qHGJtos8JtoN222t0LrQ2lqeyoqvs7MzERaWhrCw8ON+XiubcA+fgKFjI87SOf0TLw3bKLNWUJGLJuLW5F2b0HqjEGdfZvoaxNtNjW+deaO0/qmkHGax9w4XxOLnok2m1roTfS1iTabGt9uvBQ4visKGce70HUDTCx6JtpsaqE30dcm2mxqfLte+f3vTAoZ//OptEXp6el4//338e677yIoKEj6PCc3NNFm4S8T7abNZuS0qfHt5Drs7rlTyLibKPsjARIgARIgARLwGAEKGY+h5kAkQAIkQAIkQALuJkAh426i7I8ESIAESIAESMBjBChkPIaaA5EACZAACZAACbibAIWMu4k6pD+xEbJ///6YPHmy9UK9Zs2aYcKECShcuLBDLLj1NPv162d9fuHw4cPInz8/WrRogQ8++ACFChXKPvGrr77CkCFDcOLECVSvXt2yv2bNmn5hvzDiiSeewOzZs7Fy5Uo8+OCDll0LFy7Em2++iQMHDqB8+fL45JNP8PDDD/uFzUuWLME777yD7du3Wy83e/rpp/Gvf/3Lss0ffX3y5En06tULy5Yts17oVqNGDXz00Ue46667/Mbm//znP/jnP/+JLVu2QLzFXNh59ZFbPO/btw89evTAmjVrULBgQfztb39D7969/SLeacT/CFDIGBoNw4YNw5QpU7Bo0SIrwTt27IisD875A5IBAwbgqaeeQrVq1XDhwgV06NDB+pSDuLCLY9WqVWjatCm+//571KtXD6NGjcLYsWOxd+9e5MuXz/EIvv76a3z++efWRS5LyAjxInh89tlnFhtxkXjllVewa9culCpVytE2L1++HI8//jgmTZqE1q1bQ7ytdefOndZF3V993aZNG1y+fBnTp0+3YlY8fTht2jQcOXIEq1ev9ov4FvXp/PnzSExMRLdu3a4RMrnFs/hjTcR7kyZNMGLECCsexB9sEydOxJNPPunoeOfkryVAIWNoRJQpUwYDBw5Ely5dLAK7d+9G5cqVrSJYsmRJv6MiBMuLL75oFUVxZAm3qVOnWv8uRJy4mP/f//0fnnvuOUfbf+rUKdx7770QF/fY2NhsITNo0KBsYZNl4P333299NPTvf/+7o20WdjRo0MC6YF1/+KuvxSriq6++al3gr87hM2fOWKtuIqb9Jb5FLDdu3PgaIZNbPP/8889o2bIlTp8+nf3Hydtvv40NGzZg8eLFjo53Tp5CxvgYiI+PR1RUFDZt2nTNrRSxYjFz5kzrNoy/Ha+//jq2bdsGUdzEIW4hderU6ZplZlH0qlataokZJx9iZaJRo0YQNovvb2WtyIj/XrZsWYwePTrbvJ49e0Jc+GbMmOFYk8W3wsTtQyHMxe3EP//8E3feeae1yiZWZPzV10Kk/Pvf/7ZWYcSKjBCjv/32m7Ua42825yRkcotnEefipPVW1gAADoBJREFU1vnmzZuzY1vUNxHzQtzw8B8CXJHxH19KWyJWXUqXLm3tkyhXrlz2eTExMVbxb9eunXRfTmgolt5feukl64Iu9hGIQ+wPEfspxCpN1iFWYiIjI629Mk49xIVt/Pjxlq2BgYHXCBmxF0bslRH7grIOcfHbuHGjtXfGqcfRo0et1bQSJUrgxx9/tFYWP/zwQ2v/j1hpvPvuu/3S1yJ/xWrM0qVLrRdaipVUcSumUqVKfhffOQmZ3OJZvOxT7Jv65ZdfskNbrMSIW49iXyAP/yFAIeM/vpS2JC4uztoXY8KKzDfffGP9BTZnzhzUr18/m5G//cUqDBObP8VFW+yLERczcZiwIpO1wihE2dChQy27xR4ZsbFbrFaI/VL+tvombhvdfvvt1j4YcTstT5481oZmwUCsPD7yyCN+ZTNXZKTLu5ENKWSMdDsg9siIe8ydO3e2COzZs8e6+PnTHhmx2fWtt97C/PnzUadOnWs8LfZNiIudKP5ZFz7xV714ssmpe2SEWBNP6lz9ZJbYLyP+vWvXrtaTPOLW2ooVK7JZPPDAA9Y+AqfvkREri2JDt/gr/GohIzY9i03N/ubrs2fPokiRItYG1ipVqmT7Uzx1+MUXX+C7777zK5tvtkfmVvGctUdG3DoVt83FIUTt+vXruUfGz657FDJ+5lBZc8RTS+Ieu7ilIFZnxF+s4oNzYo+BPxxjxozBe++9Zy21i1WK6w/xJIt4guGHH35A3bp18fHHH1u3Ipz81FJycrL1hNbVR/Hixa0nsx566CFrL4zYOyIudOKpDXHLTTyaKi6G4lajk4+RI0da/vvpp59QsWJF6zFk4dM//vjDWqHwN18LX4k/PMQTOUJ8h4WFWaJc+FPYfPz4cb+wWTx5JOqSEN9i7554Skscwl5xa+1W8Zz11JJYtRo+fLj1dJ74/+LWa9u2bZ0c7pz7dQQoZAwNCZHk4l0rYjOcuACKBBePJfrLe2TELZXg4GCr4F19ZBVC8d9E4R88ePA175GpVauWX0XE1beWhGHXv3dDbIgUT4M4/RArLmKF8dNPP7X2Pwg/CiGT9V4gf/S1uDD37dvX2uArLvbiVpPY8PzYY49Z7vQHm0V9unofW1acig3dYuN6bvEs3iPTvXv3a94j06dPH6eHO+dPIcMYIAESIAESIAES8BcCXJHxF0/SDhIgARIgARIwkACFjIFOp8kkQAIkQAIk4C8EKGT8xZO0gwRIgARIgAQMJEAhY6DTaTIJkAAJkAAJ+AsBChl/8STtIAESIAESIAEDCVDIGOh0mkwCJEACJEAC/kKAQsZfPEk7SIAESIAESMBAAhQyBjqdJpMACZAACZCAvxCgkPEXT9IO4wmIz0yItxlPmjTJqyxSUlLw/PPPW58LEF9lFt8FkjnE6/XF/MeNGyfTnG1IgARIwCJAIcNAIAE/IeArQkZ8cVx8nG/79u3ZH+u7HrF4vbz4UrX40KMvHDl9lNAX5sU5kAAJ5E6AQiZ3RmxBAo4g4G4hI77fExISYtt2IVCEMFiyZMlNz6WQsY2VJ5AACdyEAIUMQ4MENBAQF+pu3bph6dKlWLt2LcqUKYMJEyagXr161mg5iQ7x0b933nnH+k18LE8IgldffRUffvgh4uPjrY/f/X97dxbq4/bHcXzlwjwlSoYLZCaSOWUuMpRkLopcIGRIJBcSilwJF5IylExJhtyQKUWm5MKYkDFRphvDv/e3nt3+s7H97eN/Fu9Vp3Nsv/0863mtVevTd63feZYsWZKmT58eIYE3W2/ZsiX16dOn5JqEj0qVKsUbrxs0aJCWLVsW1yva6dOn4xq88Zq3ns+cOTPNnz8/8XLJoirBvXn54NOnT9Pbt2+/0nn37l1cY//+/en9+/dxf942zhu02R7i7dqfPn1KVatWjbcMc73SbcSIEenw4cOpcuXKsZXUu3fv2Ib60oQ+sc20devWeHNxp06d4o3de/fujbdb0zfut2rVqpLLUwVasGBBunjxYqpevXqaNGlSvAWdQMaWF54HDhyIF0s2bNgwfpf7t2jRIn5Wo0aNuNaGDRvSlClT0v3798Pn7Nmz8XP6vm7dulSrVq34M33k5ZQ84507d1LXrl3T5s2b4wWOtF27dqXly5enhw8fRn+GDh36lcc/MP28pAJ/lYBB5q8abh/2dwkQZIpA0a5du3jT+L59+xJv7S1vkCGw8HuEiuvXr6cePXqkjh07pvXr18d/L126NK5569atkmvu3LkzFv7x48en48ePp5EjR8a/Way5Rs+ePdOOHTvS8OHD4/dYWFloJ0+eHEGmf//+acKECWnTpk2x+LP4ftkIVFeuXIkgU7du3TR37tx04cKFdOnSpTgTwxvFz5w589MVmbKCTPfu3SO41KtXLw0bNiwCAc9GQCOM4UC/eb5nz56ltm3bRjjhjcnPnz+PN0FjgCFvxua5CIG85f3Bgwfp9evXifEpa2uJYNOhQ4c0ceLECG78mWBEACKsFUGGex48eDA1btw4Qs/JkyfTtWvX4q3yderUSceOHUsDBgyI4IVREWZ/11z0Pgr86QIGmT99hH2+/4sAQYZqx6JFi+L+N27cSG3atImDryyi5anIzJkzJ718+TLCAY1FvVu3blEtoLGQt2/fPr169SoWTK5JVYCqS9FYeKkysIhTjaCaUizCfIbqwtGjR2NxL4IMVYimTZuW6UalheuxcA8ePDg+8+bNmwgaLOC9evWq0CCze/fuNGbMmLjPxo0b0+LFi78y4RkJU1Sujhw5EsGtaAQ9wuDt27ejErJy5cp4fvpJNahoZQUZAhS/i2nRqPQQmnBkXKjIcLh62rRp8RHCCpUurte5c+dUv3796BfhCyObAgpUvIBBpuJNvaIC6cszIFQSCAdUZPi78gQZtpZYgIvWr1+/NGjQoNh+ot27dy81a9YsKgtNmjSJa378+DFt37695Hf4LFUAFngqGizyVapUKfl7ggn9olrD4jtw4MC4xrca201UJOgX2zFF4/5s94wdO7ZCgwyhrNg6K7bbvmUya9asCBXVqlUr6dfnz5/jeQhbHz58iOC2Z8+eqEbxrGvWrIltoLKCzNq1a+PQcrHdVFyUygzhhgoMQYYQyLXKsuC6uPAczZs3j20vKjw2BRSoOAGDTMVZeiUFSgR+FGSojrx48SLxDR8aiy3bNGwblT4j87NB5nsVGRZ6WlHR+XK4yvPNHYIP202HDh2KUEX7XyoyLOqcXSn9raWytpZ+JsgQPHgGzt/8qFHFYgyoPp06dSr+YfuHsFM0Ag/bZIS8b7XvVWSo3BSN8aWKNXr06AhRpUPgj/rq3yugwPcFDDLOEAX+AYEfBRmqC2w7cRC4UaNGsahTHeCg6K8EGc7IbNu2LbZjWNQ5C0PFgKoGB2H79u0bWyxDhgyJasLNmzfjLAk/L0+QgYpDzJwBYduG8DVv3rx07ty5dPny5XKfkWGRZ2uK8zlF+9Ug8+TJkzgQvHr16qh6cJiYqhXPyPNSjaK/nDMikLF1R6jg53ymdevW6e7du1HlorF9xPYQ/Zo9e3aqWbNmevToUTp//nwaNWpUfAZDtvc4XM04Lly4MK6HNduInBXiOWvXrp1OnDgRlRvuwfywKaBAxQgYZCrG0aso8F8CPwoyfLtoxowZEQaocHAWg2/+fPmtpZ+tyJT+1hJncTgUO3Xq1JK+ETi4x9WrV2MxZ1uFQMW3i8obZDgHwlkVDvtyoJVQQt+Lxbk8h33Z6iIcUJXivArndH41yPCQnBuib4QNvlFFnziczHklql8rVqyIKgwhhzNHVMBatmwZPlSsOJODIT/nf+rHth0HfQkhHAwmrIwbN64kgBXfWuKANQGlS5cuEUZbtWqVHj9+HIeDCXhUetjC41pc16aAAhUnYJCpOEuvpIACf5kAQab09tdf9vg+rgL/CgGDzL9iGOyEAgrkKGCQyXHU7POfJmCQ+dNG1OdRQIHfJmCQ+W3U3kiBbwoYZJwcCiiggAIKKJCtgEEm26Gz4woooIACCihgkHEOKKCAAgoooEC2AgaZbIfOjiuggAIKKKCAQcY5oIACCiiggALZChhksh06O66AAgoooIACBhnngAIKKKCAAgpkK2CQyXbo7LgCCiiggAIKGGScAwoooIACCiiQrYBBJtuhs+MKKKCAAgooYJBxDiiggAIKKKBAtgIGmWyHzo4roIACCiiggEHGOaCAAgoooIAC2QoYZLIdOjuugAIKKKCAAgYZ54ACCiiggAIKZCtgkMl26Oy4AgoooIACChhknAMKKKCAAgookK2AQSbbobPjCiiggAIKKGCQcQ4ooIACCiigQLYCBplsh86OK6CAAgoooIBBxjmggAIKKKCAAtkKGGSyHTo7roACCiiggAIGGeeAAgoooIACCmQrYJDJdujsuAIKKKCAAgoYZJwDCiiggAIKKJCtgEEm26Gz4woooIACCihgkHEOKKCAAgoooEC2AgaZbIfOjiuggAIKKKCAQcY5oIACCiiggALZChhksh06O66AAgoooIACBhnngAIKKKCAAgpkK2CQyXbo7LgCCiiggAIKGGScAwoooIACCiiQrYBBJtuhs+MKKKCAAgooYJBxDiiggAIKKKBAtgIGmWyHzo4roIACCiiggEHGOaCAAgoooIAC2QoYZLIdOjuugAIKKKCAAgYZ54ACCiiggAIKZCtgkMl26Oy4AgoooIACChhknAMKKKCAAgookK2AQSbbobPjCiiggAIKKGCQcQ4ooIACCiigQLYCBplsh86OK6CAAgoooIBBxjmggAIKKKCAAtkKGGSyHTo7roACCiiggAIGGeeAAgoooIACCmQrYJDJdujsuAIKKKCAAgoYZJwDCiiggAIKKJCtgEEm26Gz4woooIACCihgkHEOKKCAAgoooEC2AgaZbIfOjiuggAIKKKCAQcY5oIACCiiggALZCvwHUubwgCHuHM4AAAAASUVORK5CYII=\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8d940d2e80> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8dc134a5f8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006365627 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0378      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0051      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032633513 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.476      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0641      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.604       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034693487 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.17       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0698      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0378      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.606      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 201        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04002426 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.465     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0603     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0229     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.607       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029365933 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0538      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0144      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.613       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027726144 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.505       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0259      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0101      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019373255 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0772      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00806     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015355974 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00722     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009740907 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00626     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.624       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 201         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009541893 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0371      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00609     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.629       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009406346 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0778      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00591     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009039444 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0502      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0058      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.633       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009475094 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0641      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00575     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.636        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063322126 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0413       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0246      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00512      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008937001 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0591      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00524     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009291222 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0626      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.64         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 204          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072204424 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0607       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00515      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011285245 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.065       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00474     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.646      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00805054 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.838      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0491     |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0052     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.648        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 201          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074118166 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0553       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00471      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.651        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068108765 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0654       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00486      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007865369 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0672      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00477     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.655       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009314701 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0887      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00469     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027735978 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.071        |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0244      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00459      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.659        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075268834 |\n",
      "|    clip_fraction        | 0.332        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0676       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0254      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00442      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055933953 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0719       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00455      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009925294 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.091       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00459     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077251345 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0371       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0256      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00446      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.662       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004445684 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.056       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00421     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.663        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050566616 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.059        |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00414      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007718733 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.051       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00416     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.663         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 211           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013876855 |\n",
      "|    clip_fraction        | 0.339         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.871         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0604        |\n",
      "|    n_updates            | 620           |\n",
      "|    policy_gradient_loss | -0.0272       |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.0042        |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.664        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064074188 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0431       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00417      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007404676 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0592      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00395     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008397022 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0344      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00409     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004732102 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0519      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006041235 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00368     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007111603 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0592      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054844827 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0877       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0266      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00377      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050089536 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0787       |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00383      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063805105 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0779       |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00371      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054797963 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.049        |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00352      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005155808 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0586      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00368     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005201557 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025882989 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.085        |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00358      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005924487 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008378643 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00372     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007880524 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0552      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004336402 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0759      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00344     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "seed 2: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8dc134a5f8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8d940c7710>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064620106 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0518       |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00376      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013300797 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0718      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0053      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004888651 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0254      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00465     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033572495 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0523       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00455      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061625643 |\n",
      "|    clip_fraction        | 0.335        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0534       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00443      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042661964 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0599       |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00459      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005239102 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0044      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004542336 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0729      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00463     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004183191 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0978      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00455     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005896914 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0508      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007797292 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0508      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00424     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070629627 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0458       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00437      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004228103 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.037       |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00419     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007447398 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0557      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00429     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093474295 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0242       |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.0322      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00445      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057852594 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0368       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00411      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010791781 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0586      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00399     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056168735 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0618       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00425      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006389572 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00409     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008675342 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0416      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033726036 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0572       |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00392      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005786815 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0917      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00386     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029380382 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0664       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00381      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006341234 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0815      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075651556 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0507       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00371      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068704723 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0486       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0039       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "seed 2: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8dbc1a04a8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8d8431f748>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009452152 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0602      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013318785 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0697      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00636     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.696      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00788534 |\n",
      "|    clip_fraction        | 0.361      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.822      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0589     |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0309    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00612    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004236904 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0381      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00619     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007941792 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0331      |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00586     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034662008 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.816        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0756       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00623      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005303964 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00574     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009375056 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00609     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021544783 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0528       |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00588      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064157126 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0392       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00577      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.695      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00820533 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.831      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0423     |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00582    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037664443 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0318      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00557      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011765948 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0571      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00565     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008241418 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0565      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00553     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006017572 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0395      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00563     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008215222 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0614      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00583     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.695      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00705764 |\n",
      "|    clip_fraction        | 0.367      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.828      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0472     |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0311    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00586    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007300797 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0515      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00581     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011441985 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0359      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00573     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007107547 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0235      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00557     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007239872 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0449      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0056      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012008363 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.063       |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00552     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024127483 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0368       |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00557      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007022613 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00568     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005084306 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00551     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007646254 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0636      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00589     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8d842d9e48> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8d843020b8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.599       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008940351 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0681      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00554     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008611363 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0645      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0827      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.603       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036471147 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.39       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0906      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0336      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.605     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 209       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 12        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0380391 |\n",
      "|    clip_fraction        | 0.38      |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | -0.375    |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0909    |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | -0.0214   |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.0203    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.609      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 205        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03127714 |\n",
      "|    clip_fraction        | 0.378      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.335      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0802     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0126     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.609       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023681339 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.574       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0301      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00905     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.611     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 207       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 12        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0186771 |\n",
      "|    clip_fraction        | 0.343     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.725     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0562    |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | -0.023    |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00684   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.615       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014332309 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00616     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011970257 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0357      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00588     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.618      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00867331 |\n",
      "|    clip_fraction        | 0.345      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.83       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0539     |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00542    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.622        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 201          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112716975 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0608       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0231      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00508      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.622       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008531893 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0873      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00503     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.623        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048460127 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0378       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0259      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00503      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.624       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011932664 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00498     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.625        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0128508685 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0674       |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0044       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.626       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005663827 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.629        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 203          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067400276 |\n",
      "|    clip_fraction        | 0.334        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0621       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0246      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00455      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.632       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012890858 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0638      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00404     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011975783 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00426     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.64         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 202          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044877767 |\n",
      "|    clip_fraction        | 0.327        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0331       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0244      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00401      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009833497 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0757      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.641      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00793908 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0493     |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0264    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00382    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008342582 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0405      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.644        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 201          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075665624 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0725       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0253      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00406      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.646      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01218692 |\n",
      "|    clip_fraction        | 0.338      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0854     |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00384    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.647       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005613193 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0626      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.649        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057478277 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0617       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00356      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005415781 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0453      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00333     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.652       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003361559 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0661      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00346     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.655       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010581171 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0535      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00362     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.656       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005057818 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0447      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00353     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 204          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007322341 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0822       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00333      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007673341 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0606      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051002414 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0404       |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.0259      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00347      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.662        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038924187 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0474       |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0031       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002762404 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00334     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007065469 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.045       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00329     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.665      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01055531 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.903      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0535     |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00318    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009018359 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0694      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011550871 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0637      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007347724 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0561      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00337     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063210875 |\n",
      "|    clip_fraction        | 0.329        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0419       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.024       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056840717 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0728       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00315      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063381107 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.031        |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00323      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023716062 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0624       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00326      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.669         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 213           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 11            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039167405 |\n",
      "|    clip_fraction        | 0.36          |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.912         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0605        |\n",
      "|    n_updates            | 920           |\n",
      "|    policy_gradient_loss | -0.0278       |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.003         |\n",
      "-------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008696845 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0382      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005884564 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.074       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "seed 3: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8dbf6f2f28> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8d842c5160>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006482959 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.682       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011201417 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0402      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00464     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069970964 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.052        |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00454      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073840143 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0424       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00435      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039914576 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0582       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00423      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004259902 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0424      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00435     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063505173 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0266       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00399      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039018423 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0463       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00408      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007733971 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0439      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00402     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006734091 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00385     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010906875 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0569       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00378      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054474473 |\n",
      "|    clip_fraction        | 0.323        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0533       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00361      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066095768 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0251       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00383      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011676857 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0602      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00377     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010357303 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.073       |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007724309 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0666      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00354     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048668953 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0555       |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00346      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008758736 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0712      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00392     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009891232 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0459      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00371     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008837387 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0461      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00362     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008392924 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.063       |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00347     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007732168 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0547      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00345     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004297477 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00365     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005652383 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0459      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031028837 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0542       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0036       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006304562 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0674      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0035      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "seed 3: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8d940e3cc0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8dc134aa90>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003141269 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0774      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007816973 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0828      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00544     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005027652 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0833      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00561     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.696      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00857569 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.844      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0554     |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0293    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00513    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045167417 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0408       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00531      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037886947 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0733       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00554      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037104934 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0688       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00495      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008478555 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0764      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00538     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008654761 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0829      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00531     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053928467 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0487       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00533      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062617273 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0521       |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0315      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00537      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006865865 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0668      |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00508     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007288146 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0355      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00511     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006712529 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0525      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00521     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009023385 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0901      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00503     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059639425 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0517       |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00505      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060277134 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0886       |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00495      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035125404 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0506       |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.0308      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00501      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060676546 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0466       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00467      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002654037 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0564      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00505     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007015267 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0676      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00472     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008096391 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0798      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00479     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009150118 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00506     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049787164 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0491       |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00505      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049493224 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0694       |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00459      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007226789 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0592      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00497     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=25,\n",
    "                                grid_fidelity_factor_array =[0.25, 0.5, 1.0],\n",
    "                                episode_limit_array=[25000, 25000, 25000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
