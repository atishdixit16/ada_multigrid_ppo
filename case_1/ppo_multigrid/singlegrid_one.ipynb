{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_1_singlegrid_one'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 3e-6,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.1,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[150,100,80], log_std_init=-2.9),\n",
    "                verbose = 1,\n",
    "                target_kl = 0.05,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fa00817ef60> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fa0081703c8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.599    |\n",
      "| time/              |          |\n",
      "|    fps             | 80       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.604       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019692665 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.234      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0946      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0924      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.606       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030110842 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.23       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0948      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0424      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033001117 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.228      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.087       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0269      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020800646 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.189       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.108       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0189      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021035159 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.418       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0144      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.619       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015440675 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.572       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0741      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0118      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.617       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015906557 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0102      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.622       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011005143 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0368      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00986     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.624       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011346236 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0784      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0089      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.626       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008259425 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.1         |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00918     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.629        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096230535 |\n",
      "|    clip_fraction        | 0.335        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.745        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0535       |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00899      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.634       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007744372 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0788      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00846     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.635       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009052331 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0579      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00813     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006740627 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0518      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00807     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.642        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083576115 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.782        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0438       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00747      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.645       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006277847 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0552      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00728     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.645       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008537429 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0847      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00758     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.649       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008118319 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0902      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00689     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006722218 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0314      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00693     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.656      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00828067 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.793      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.046      |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00693    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054000975 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.812        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0598       |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00656      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.659        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074881287 |\n",
      "|    clip_fraction        | 0.329        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.8          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0656       |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00695      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009063328 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0489      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00644     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.661        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059479354 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.812        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0417       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00631      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.662       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007150608 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0637      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00612     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.662       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004958558 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0363      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00621     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.662       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006569171 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0725      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00636     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.664        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068674595 |\n",
      "|    clip_fraction        | 0.334        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.041        |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0062       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009449178 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0658      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00593     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065443935 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0546       |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00587      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076306225 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0615       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00582      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008352945 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0451      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00554     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007624647 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0497      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00573     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.673       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008236286 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0349      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00562     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008113262 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.048       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00607     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006878489 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0707      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00572     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.676       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006730446 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00503     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.677      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00868828 |\n",
      "|    clip_fraction        | 0.342      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.837      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0765     |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00554    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010457012 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0707      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00561     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020114183 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.062        |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0315      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0055       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005715984 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0629      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00555     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056356816 |\n",
      "|    clip_fraction        | 0.335        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0358       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00539      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007206845 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.062       |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0055      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.682       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005511889 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0688      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00525     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062056123 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0549       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00539      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007812944 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0354      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00507     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059488565 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0649       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00516      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005445206 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00532     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058897138 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0454       |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00508      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005322361 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0574      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00511     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066025825 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0729       |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00507      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062562795 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0491       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00537      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005755541 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0579      |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00504     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067082793 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0397       |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00524      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047488986 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0665       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.005        |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006323376 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0669      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00519     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.691      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00541268 |\n",
      "|    clip_fraction        | 0.368      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.855      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0394     |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00505    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009415021 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00493     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010646892 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0258      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00484     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.693        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061689047 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0485       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00481      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007608113 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0484      |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00503     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022947968 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.054        |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00491      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008256441 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0622      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0047      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058835833 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0478       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0048       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007283074 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0681      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0047      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.694      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00538933 |\n",
      "|    clip_fraction        | 0.343      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.869      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0652     |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00452    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008373943 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00474     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004722965 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0334      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00496     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008870864 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.078       |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00483     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061967284 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0635       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0319      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00463      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052028624 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0594       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00485      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060028853 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0356       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00497      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004612854 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0428      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00451     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.694      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01022554 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.861      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0874     |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0321    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.0048     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005880383 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0465      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00494     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008599276 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0273      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00472     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031511039 |\n",
      "|    clip_fraction        | 0.324        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.039        |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00491      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077814134 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0543       |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.032       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00476      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078214435 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0446       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00482      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008389485 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0845      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00497     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072387517 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.08         |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00453      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008580023 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0458      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00473     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009366739 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0379      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00463     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004819411 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0478      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0047      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007924924 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0395      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00445     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080977855 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0441       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00458      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008222791 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0893      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00454     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006719926 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0347      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00452     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009494552 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0303      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0339     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006854981 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0604      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00448     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009040216 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0632      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00479     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071878107 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0715       |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.0317      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00487      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007203296 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0514      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00471     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009729972 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0229      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0044      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009697938 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0619      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00448     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010997256 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0594      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00444     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004196805 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00465     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041003553 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0446       |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0315      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00446      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006542927 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0722      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00464     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 88           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071639298 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0381       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00436      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006604296 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0501      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00445     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006253937 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0313      |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00432     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069619627 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0553       |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00443      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005584398 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00437     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008993214 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00422     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008686895 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0646      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00436     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009517503 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0561      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00416     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071997764 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0546       |\n",
      "|    n_updates            | 2160         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00439      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065848054 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0498       |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00399      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007389939 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0673      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00411     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00542295 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.864      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0465     |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00449    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005241382 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0559      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00398     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008157283 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0517      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005468935 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0438      |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00418     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008269509 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.05        |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005854973 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.077       |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0043      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00428572 |\n",
      "|    clip_fraction        | 0.369      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.879      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0608     |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0041     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006476426 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0549      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007615441 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0403      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00406     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.697     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 86        |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 29        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0078769 |\n",
      "|    clip_fraction        | 0.362     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.9      |\n",
      "|    explained_variance   | 0.873     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0588    |\n",
      "|    n_updates            | 2400      |\n",
      "|    policy_gradient_loss | -0.0296   |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00422   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007531807 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0516      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007216224 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0456      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00413     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009747649 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.041       |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00421     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060080467 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.054        |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0043       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010053429 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0418      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00401     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055939644 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0814       |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00413      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006571305 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0574      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00398     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008215895 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0441      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00422     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 88           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075154724 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.071        |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00421      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005461213 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0461      |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055733025 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.109        |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00401      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072289584 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0413       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00411      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059842407 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0478       |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00411      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057884576 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0767       |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00434      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003336817 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0625      |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009615225 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0464      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00407     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075871795 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0335       |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.0308      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00392      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005242097 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0367      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008241418 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0629      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00398     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034889996 |\n",
      "|    clip_fraction        | 0.384        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0516       |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.0315      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00422      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044308305 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0358       |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00398      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.696     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 85        |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 29        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0067287 |\n",
      "|    clip_fraction        | 0.345     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.9      |\n",
      "|    explained_variance   | 0.885     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0521    |\n",
      "|    n_updates            | 2840      |\n",
      "|    policy_gradient_loss | -0.0285   |\n",
      "|    std                  | 0.0549    |\n",
      "|    value_loss           | 0.00396   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036044181 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0731       |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00383      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006853664 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.073       |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.0041      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056215883 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.061        |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00405      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007564741 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0374      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f9f8c0ca9b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f9f783b6080>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.605       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004729694 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0661      |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00405     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.606       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033112563 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.464      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0611      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0682      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.611       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037100445 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.04       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0793      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0404      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.613       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038673725 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.423      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0752      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0263      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.617       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029616648 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0609      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0169      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.62       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02610824 |\n",
      "|    clip_fraction        | 0.384      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.465      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0624     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0123     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.622      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01764677 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.597      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0582     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0287    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0104     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.625       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016591148 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0565      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00961     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.628       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014184684 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0531      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00836     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.631       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013578141 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.048       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00804     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.635       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007953671 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0622      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00794     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.637       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007949179 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.055       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00731     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.641       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005488187 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0357      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00767     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.642        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055488376 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.771        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0551       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00769      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010050791 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0756      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00719     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.644       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009599492 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.059       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00694     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.647       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011487303 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0793      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00712     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009296668 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.038       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00672     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.655       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007059911 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0538      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00691     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.656        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074871806 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.807        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0432       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00657      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.659        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060763983 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.807        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0638       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00663      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.662       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008415377 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0385      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00654     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010264501 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0578      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0066      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.665        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032002062 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.816        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0736       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00639      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007658729 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0709      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00636     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074376343 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0447       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00629      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008146202 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0723      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00616     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067068934 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0361       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00637      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028990537 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0639       |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00598      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048514428 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0601       |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00611      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009227445 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.057       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00612     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002598056 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0555      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00624     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.676       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005193436 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00633     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069709746 |\n",
      "|    clip_fraction        | 0.332        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.043        |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00577      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049558757 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0498       |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00601      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073967366 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0665       |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0058       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005691531 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0469      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00562     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008417574 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0637      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00566     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076696454 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0864       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00558      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006114206 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0963      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00568     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008485404 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00554     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007973252 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0475      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00536     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077907084 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0351       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0308      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00566      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052472623 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0437       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00554      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005725497 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0621      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00532     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006186408 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0377      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00566     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007149103 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00577     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006467542 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0699      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00556     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006344557 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0849      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00549     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008136248 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0565      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00582     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011908618 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0727      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00566     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045544235 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.029        |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0315      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00557      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.693      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00639497 |\n",
      "|    clip_fraction        | 0.343      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.856      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0477     |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00504    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006652807 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0671      |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00529     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006535682 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0412      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00512     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.693      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00874578 |\n",
      "|    clip_fraction        | 0.343      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.85       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.069      |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.03      |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00527    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006100881 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0373      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00534     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006812674 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0458      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00545     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027608573 |\n",
      "|    clip_fraction        | 0.38         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0555       |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.0329      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00529      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069140405 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0423       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.0321      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0052       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006081945 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0403      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0051      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006581721 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0834      |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00524     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009873515 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0377      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00492     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007237175 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0549      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00522     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.693        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040760785 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0518       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0049       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006423074 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0381      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00523     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005850366 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0311      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00508     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008373161 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0466      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00495     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.693        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063076257 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0409       |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00497      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007063286 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0334      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00492     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005645764 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0687      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00517     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037822216 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0689       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0051       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053093107 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0544       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00512      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.696      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00503529 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.865      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0479     |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0327    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00455    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009526491 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0918      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0052      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008521003 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0706      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00504     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005644637 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0477      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00482     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00714857 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.86       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0851     |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0302    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00493    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007325211 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0529      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00464     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047236593 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0446       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00457      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004631457 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0632      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00471     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048467875 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.045        |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00473      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003246072 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0381      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00484     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037316293 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0563       |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00483      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075494736 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0046       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009283548 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0824      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00463     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067331404 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0632       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0322      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00453      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007345402 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0791      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00456     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049306513 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0506       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00446      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008122584 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0301      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00451     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007148093 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0369      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00455     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007131809 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005878699 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0741      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00469     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007943082 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0815      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00453     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044332654 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0651       |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0044       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006585458 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.028       |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00458     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007907823 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0429      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0043      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009161768 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0477      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00459     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052292286 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.041        |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00446      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003176725 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.053       |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00445     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064475564 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0427       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00438      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008517313 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0621      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00443     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032462166 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0453       |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00418      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 78           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057673664 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0457       |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00438      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008025694 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0441      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00456     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.698      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 74         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 34         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00609923 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.873      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0792     |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0306    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00438    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006726551 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0516      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00415     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010522353 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0521      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00426     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009412101 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0484      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00456     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010298511 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0385      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00435     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037630827 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0345       |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00433      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047367127 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0346       |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00449      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.698     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 83        |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 30        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0100268 |\n",
      "|    clip_fraction        | 0.372     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.876     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0604    |\n",
      "|    n_updates            | 2240      |\n",
      "|    policy_gradient_loss | -0.0311   |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.00425   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035472244 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0706       |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00437      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007898703 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0713      |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0042      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010373265 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.047       |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00419     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010607392 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0637      |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00415     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008171782 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0678      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00427     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004266423 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0527      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0043      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065468787 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0829       |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00426      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008283958 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0437      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00433     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065484405 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0635       |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00434      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004124181 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0735      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00429     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009195685 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0487      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00444     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010903096 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0896      |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00432     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076836227 |\n",
      "|    clip_fraction        | 0.374        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0432       |\n",
      "|    n_updates            | 2500         |\n",
      "|    policy_gradient_loss | -0.0318      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00434      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007676664 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0658      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00442     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009529283 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0776      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00423     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041592093 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.078        |\n",
      "|    n_updates            | 2560         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0041       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057152747 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0475       |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00419      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008002281 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0418      |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004849297 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00416     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007469782 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0628      |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00422     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005549145 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0782      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00413     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003652236 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0513      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00447     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005965209 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0702      |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00386     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051862923 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.065        |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00418      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070416704 |\n",
      "|    clip_fraction        | 0.387        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0457       |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.0318      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00437      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006948945 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0615      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00425     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028406053 |\n",
      "|    clip_fraction        | 0.387        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0679       |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00421      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005339411 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0546      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00435     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043285578 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0418       |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00409      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007967329 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0523      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00405     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010884762 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0398      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00421     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070198565 |\n",
      "|    clip_fraction        | 0.374        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.047        |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00406      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065224557 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0464       |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00429      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063046217 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0377       |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00427      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fa009d9eb38> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fa008170e48>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.603        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075347004 |\n",
      "|    clip_fraction        | 0.382        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.063        |\n",
      "|    n_updates            | 2940         |\n",
      "|    policy_gradient_loss | -0.0316      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0042       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.602       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010022998 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0928      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.603      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03967239 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -1.4       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0859     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0368     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.606      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03587713 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.419     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0834     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0235    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0237     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.61       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02718707 |\n",
      "|    clip_fraction        | 0.363      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.239      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0548     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0157     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.611       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020361401 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0367      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0121      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.612       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015754467 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.644       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0585      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00946     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014667165 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0411      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00881     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010049412 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.71        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0504      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00862     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.619       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011326176 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.059       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00802     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.623        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0107380245 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.783        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0562       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00748      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.624       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007996416 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0557      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00717     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.625        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075350376 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.8          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0712       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00689      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012416193 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0553      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00698     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.633       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011676535 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0558      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00694     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.634       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003978285 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.05        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00659     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.638        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066608014 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0357       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0266      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0066       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.641       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009494764 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.074       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00646     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.644       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006715554 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0398      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00649     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.645       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006714949 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0568      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00614     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.648        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007415563 |\n",
      "|    clip_fraction        | 0.316        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0396       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00602      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.649       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004430893 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0611      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00611     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.652       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007615608 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.037       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0056      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.655        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066967667 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0468       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00579      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.657      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00877876 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.823      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0861     |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0283    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00596    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.657       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009575325 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0671      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0057      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062420964 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0448       |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00551      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.661        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070579886 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0659       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00559      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.664        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070977346 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.03         |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00544      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006676921 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0922      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00528     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010206235 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0497      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00562     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069821686 |\n",
      "|    clip_fraction        | 0.336        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0331       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00528      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005642521 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0707      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00559     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067064376 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0414       |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00522      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045918496 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0511       |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00542      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008052269 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0465      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00523     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003907734 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0505      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00573     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007689768 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0602      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00534     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.679      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00799101 |\n",
      "|    clip_fraction        | 0.343      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.847      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0596     |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00513    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007797512 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0831      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00547     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012488911 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.06        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00489     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007936338 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0508      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00529     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.682       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008394301 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0392      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00501     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007061702 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0455      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00507     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009378195 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0561      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0052      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005098036 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0657      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00513     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023866117 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0512       |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00498      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.688      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00489766 |\n",
      "|    clip_fraction        | 0.34       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.849      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0465     |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0289    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.0051     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007242352 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0464      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00499     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005977583 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0626      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00499     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006654769 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0379      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00484     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007423681 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0399      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00502     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033292354 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0684       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00513      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004810822 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0448      |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00497     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007282159 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.089       |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00482     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042442917 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0774       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00474      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056981593 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0565       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0049       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077144084 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0284       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00474      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034812242 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0731       |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00496      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005828136 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0824      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00484     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013752443 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0456      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00455     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.691      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00834766 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.864      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0586     |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0303    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00473    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071587143 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0653       |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00456      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055392804 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0537       |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00461      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043312134 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0795       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00475      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006192514 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0047      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009528476 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0412      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00482     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.694      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00848428 |\n",
      "|    clip_fraction        | 0.343      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.867      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.058      |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00458    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010438487 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00467     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066655814 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0623       |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00481      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061832517 |\n",
      "|    clip_fraction        | 0.322        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0566       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0046       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064040096 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.033        |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00472      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035557866 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0561       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00452      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042217285 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0505       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00443      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054597645 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.115        |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00472      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010235861 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0457      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00461     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081170825 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0559       |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00442      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011322695 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0328      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00448     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004667443 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0449      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0044      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005707604 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0824      |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00451     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070402147 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0711       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00468      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004502228 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0405      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00462     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060875295 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0561       |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00431      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.695      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00722321 |\n",
      "|    clip_fraction        | 0.345      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.867      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0494     |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00463    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050414563 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0696       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0317      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00441      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055253776 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.057        |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00442      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008361447 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0587      |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00432     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006489274 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.058       |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00432     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043393164 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0566       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00448      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059312833 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0511       |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00408      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006933945 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0866      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00448     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006357676 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0285      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004249236 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0481      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00419     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062428713 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0572       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00433      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075651556 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0575       |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00434      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008300433 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0399      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00406     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008912978 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.071       |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00427     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011607322 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.063       |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008244455 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0549      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071549653 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0601       |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00433      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006627622 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0487      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064195693 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0401       |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00411      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067607374 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0369       |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0321      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00394      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.698      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00616734 |\n",
      "|    clip_fraction        | 0.346      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.878      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0433     |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00416    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002268222 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00386     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010211637 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008259112 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0429      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00388     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009468955 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0516      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00409     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009173512 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.037       |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00397     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008517548 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0888      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00409     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061659724 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0546       |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00408      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045717717 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.071        |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00401      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.699      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00873563 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0658     |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00396    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007341847 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0858      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00411     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.699      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00621863 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0496     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0298    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00395    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005760121 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0477      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00406     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036216856 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0508       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00403      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006416264 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00373     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023015172 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0485       |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00392      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075180503 |\n",
      "|    clip_fraction        | 0.39         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0497       |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.032       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0038       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052012233 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0339       |\n",
      "|    n_updates            | 2400         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.004        |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069242446 |\n",
      "|    clip_fraction        | 0.384        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0531       |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00375      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008634875 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0482      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00383     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010670116 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0559      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008694857 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0342      |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00385     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065458566 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0503       |\n",
      "|    n_updates            | 2500         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00389      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.699         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 82            |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 30            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0005427405 |\n",
      "|    clip_fraction        | 0.38          |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.894         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0539        |\n",
      "|    n_updates            | 2520          |\n",
      "|    policy_gradient_loss | -0.0304       |\n",
      "|    std                  | 0.055         |\n",
      "|    value_loss           | 0.00378       |\n",
      "-------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007973021 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0618      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055874498 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0501       |\n",
      "|    n_updates            | 2560         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00377      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008330596 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0425      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077838064 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0645       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00376      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009177444 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0695      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054530143 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0611       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0037       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036973774 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0468       |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00352      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009515715 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0854      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00376     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006369406 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0677      |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00367     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067432136 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0519       |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00348      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.699      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00517056 |\n",
      "|    clip_fraction        | 0.363      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.902      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0501     |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0283    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00345    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008168289 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0565      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0037      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044012456 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0479       |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00345      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004628843 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0554      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00353     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006947735 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00368     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086894985 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0624       |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00359      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039201886 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0616       |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00355      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070595876 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0718       |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066906125 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0657       |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00378      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.699      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00306651 |\n",
      "|    clip_fraction        | 0.386      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.892      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0797     |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0303    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0037     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[1.0],\n",
    "                                episode_limit_array=[75000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
