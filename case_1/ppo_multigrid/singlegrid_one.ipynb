{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_1_singlegrid_one'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 3e-6,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.1,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[150,100,80], log_std_init=-2.9),\n",
    "                verbose = 1,\n",
    "                target_kl = 0.05,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/coarse_grid_functions.py:51: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1mFirst-class function type feature is experimental\u001b[0m\u001b[0m\n",
      "  for j in range(len(p_1)-1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8de177c208> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8d566bb978>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.599    |\n",
      "| time/              |          |\n",
      "|    fps             | 63       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 70 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.603       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019837633 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.234      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0995      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0924      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.605      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 98         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03140346 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -1.23      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.103      |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0423     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.607       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032862615 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.226      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0845      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0269      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 62 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.609       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020418609 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.108       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0189      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.617       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020815674 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.424       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0397      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0144      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014895743 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.576       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.078       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0118      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016295096 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0479      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0102      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010414368 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0394      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00983     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.622       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011166987 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0735      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00887     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.625       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008980376 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.103       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00916     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 67 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.627     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 95        |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 26        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0087329 |\n",
      "|    clip_fraction        | 0.334     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.746     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.055     |\n",
      "|    n_updates            | 220       |\n",
      "|    policy_gradient_loss | -0.0282   |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.00897   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.633      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 99         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00747596 |\n",
      "|    clip_fraction        | 0.328      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.757      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0835     |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00844    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.635      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 95         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00880369 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.767      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0511     |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0081     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.641       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006301433 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00805     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 65 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008575514 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0424      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00746     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.646       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007315792 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0702      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00727     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.646       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008172962 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0755      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00757     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.649        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072818547 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.794        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0914       |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00688      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.652        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066266744 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.805        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0309       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00691      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.655       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008400643 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0506      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00692     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.656       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005382444 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0625      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00655     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.657       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007668647 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0775      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00694     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.658       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008078384 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0518      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00643     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005307895 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0405      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0063      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007303393 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0635      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00612     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005904752 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.04        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00621     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.661        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074821054 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0743       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0308      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00635      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.662        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057255924 |\n",
      "|    clip_fraction        | 0.335        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.812        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0525       |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00619      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009768376 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0659      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00593     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006901513 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00588     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074645877 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0642       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0058       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008501294 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0483      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00555     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008218849 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00573     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007975328 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0369      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00563     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009575811 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.047       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00608     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071715387 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0661       |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0319      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00573      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.674        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064795376 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0442       |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00504      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.676       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009007963 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.083       |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00555     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010771778 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0573      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0056      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014136195 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.052        |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0316      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0055       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066726804 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0606       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00555      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062483847 |\n",
      "|    clip_fraction        | 0.336        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0411       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00537      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069081276 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0682       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0323      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0055       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053803683 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0693       |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0315      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00526      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070652394 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0516       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0308      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00539      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066869766 |\n",
      "|    clip_fraction        | 0.331        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0365       |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00506      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060228435 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0606       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00515      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055031627 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0962       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00532      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008118639 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0439      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00508     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059388964 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0629       |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.0318      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00511      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062529384 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0773       |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00507      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066094995 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0463       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00536      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057857335 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0552       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0315      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00505      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061218827 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0411       |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00524      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055537224 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0598       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.005        |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061881454 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0597       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0315      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00518      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005781689 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0412      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00504     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009526628 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0412      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00493     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010494751 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0253      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00484     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005195266 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0477      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0048      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.693        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067679705 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0471       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0324      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00501      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027160286 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0474       |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0049       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.693      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 95         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00789513 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.864      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0578     |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0297    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00467    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.693        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061581107 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0473       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00478      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007271701 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0703      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00468     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041371165 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0666       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0045       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.694      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 96         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00970309 |\n",
      "|    clip_fraction        | 0.353      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.862      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0588     |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0311    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00472    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.694      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 96         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00410904 |\n",
      "|    clip_fraction        | 0.381      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.857      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0276     |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0329    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00493    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008713955 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0783      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00481     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063591124 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0578       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0319      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00461      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068906574 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0552       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00481      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070072054 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0312       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00495      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004671505 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0379      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0045      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010235691 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.097       |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00477     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.693        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066909967 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0547       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00489      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009953233 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0274      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00469     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030588328 |\n",
      "|    clip_fraction        | 0.325        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0362       |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00489      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008676404 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0547      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00474     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078102946 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0504       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00481      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007784766 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0822      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00495     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068011046 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0787       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00451      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008679154 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0405      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00471     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010138327 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0353      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00461     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046711145 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0507       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0316      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00469      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007962513 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.05        |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00444     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069221645 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0424       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00457      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.695      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 95         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00786834 |\n",
      "|    clip_fraction        | 0.369      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.865      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0853     |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0321    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00454    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006796119 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0414      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00451     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010541022 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0279      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00427     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066224905 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0608       |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00449      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008849004 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0589      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00479     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007980975 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0749      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00487     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008852938 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0598      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00468     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009401729 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0241      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010056257 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0602      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00444     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010229265 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0517      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004108685 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0451      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0046      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045449557 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0441       |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0315      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00443      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006581539 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0823      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0046      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.696      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 96         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00755786 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.875      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0451     |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00433    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005860275 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0511      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00443     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068960786 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0308       |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00431      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006755936 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0044      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005138588 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0411      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00434     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010220888 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.11        |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00419     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008694251 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0683      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00432     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009502682 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0576      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00415     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068095475 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.054        |\n",
      "|    n_updates            | 2160         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00436      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071555763 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0533       |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00396      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075732083 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0719       |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0041       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005653742 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0487      |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00446     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049786717 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0542       |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00397      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008653412 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0548      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00432     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053881137 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0426       |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00419      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00762288 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0506     |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00407    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065323533 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0716       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00429      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044296207 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0691       |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0041       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006070596 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0558      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00426     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078035174 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0427       |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00404      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008944901 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0563      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0042      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073593496 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0635       |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00428      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008598524 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0449      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00412     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009894612 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0437      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0042      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053317784 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0555       |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00427      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010157457 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0458      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00398     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006578827 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0796      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00414     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071372776 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.058        |\n",
      "|    n_updates            | 2540         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00396      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006486842 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00421     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 96         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00861902 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.875      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.078      |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00422    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060789166 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0522       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00432      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031817586 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.117        |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00402      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059200553 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.038        |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00413      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006688401 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0504      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00412     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.696      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 99         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00468114 |\n",
      "|    clip_fraction        | 0.375      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.87       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0738     |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0309    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00435    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026565343 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0639       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00382      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008899855 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006990093 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0295      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00392     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042129396 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0378       |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00372      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007785848 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0554      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00401     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003443441 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0525      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00425     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004277286 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.034       |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00402     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006194609 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0596      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00397     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.696      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 99         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00411278 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.889      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.077      |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.03      |\n",
      "|    std                  | 0.0549     |\n",
      "|    value_loss           | 0.00385    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005953175 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0724      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00411     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005207804 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0618      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00407     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074101477 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0347       |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00388      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8d56428400> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8d56392240>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.605       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004231694 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0671      |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00407     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 81 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.606      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 28         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03332468 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.464     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0639     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0682     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 68 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.61       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 28         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03688842 |\n",
      "|    clip_fraction        | 0.343      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -1.03      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0798     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0404     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.613       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037361644 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.421      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0775      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0263      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.616      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 94         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 27         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02969462 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.168      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0682     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0262    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0168     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.619      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 96         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02558952 |\n",
      "|    clip_fraction        | 0.384      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.462      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0615     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0293    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0123     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.622       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017059745 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.595       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0561      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0104      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.625       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016525133 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.653       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0552      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00963     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.627       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013538072 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0559      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00837     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012995457 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0468      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00802     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.634       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007733983 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0606      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00794     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.635       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008011279 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0514      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00731     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.639        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059089335 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.776        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0352       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00767      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.641        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052662343 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.772        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0556       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00768      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008619604 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0773      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00718     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.644       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009258553 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.059       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00693     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.647       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010535034 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0735      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00711     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.651      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 95         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01004458 |\n",
      "|    clip_fraction        | 0.326      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.8        |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0383     |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0273    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00671    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.656       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007298705 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.801       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.053       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00691     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.657       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007539007 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0428      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00657     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065704673 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.806        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0547       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00663      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.662      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00954113 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.809      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0468     |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00654    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 63 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011210188 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0584      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00661     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003271365 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0721      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00639     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008170026 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0718      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00636     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007096055 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0449      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00629     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009130865 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0689      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00618     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073517114 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0385       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00638      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026994497 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0608       |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00599      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.673       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004961687 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0675      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00613     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.674     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 96        |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 26        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0088512 |\n",
      "|    clip_fraction        | 0.335     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.822     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.05      |\n",
      "|    n_updates            | 600       |\n",
      "|    policy_gradient_loss | -0.0294   |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.00613   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026076555 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.816        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0666       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00625      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.676       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004820061 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0469      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00635     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006453541 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0464      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00579     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047974763 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.822        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.049        |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00603      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008053789 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0616      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00581     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054166405 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0455       |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00564      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090871155 |\n",
      "|    clip_fraction        | 0.335        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0596       |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00566      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075235693 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0911       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00559      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072851805 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.104        |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00569      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009477729 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0479      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00554     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065122456 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0451       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00538      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008173752 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0366      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00565     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 65 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005049801 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0397      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00554     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061258883 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0667       |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00533      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054269345 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0395       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00568      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007036364 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0679      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00577     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070566656 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0691       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00558      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064792456 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0828       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00551      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 67 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008264494 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00586     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011353606 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.07        |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00568     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006040451 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0267      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0056      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 61 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005745247 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0499      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00506     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076503726 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0586       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00532      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006894344 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0469      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00516     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009098944 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0728      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00528     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005904317 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0365      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00537     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006905371 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00548     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027064532 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0605       |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.0329      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00533      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 67 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066824467 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0516       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.032       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00522      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060146777 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0343       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00512      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006995669 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0769      |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00525     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009565717 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0365      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00494     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007819271 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0591      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00524     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039875656 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.057        |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00491      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007082629 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.047       |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00524     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067530153 |\n",
      "|    clip_fraction        | 0.328        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0333       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0051       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008606875 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00497     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007031885 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0379      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.005       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068464177 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0387       |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00496      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005312562 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0765      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00521     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004264784 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0672      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00511     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004607162 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.052       |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00515     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005617338 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00458     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008984536 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0855      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00524     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008984623 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.067       |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00507     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 93           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056331353 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0511       |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00484      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070815296 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.086        |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00496      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060152113 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0568       |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00465      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046566846 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0491       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00458      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005476949 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0574      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00474     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045662434 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0463       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00474      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 67 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023674518 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0357       |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.0315      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00485      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038470565 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0548       |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00485      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069817724 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0426       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00462      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0089842025 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0801       |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00464      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 62 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006028354 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0606      |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00453     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 64 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008592774 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0787      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00458     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061766803 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0493       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00446      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007983381 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0267      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0045      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068324446 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0355       |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00456      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070630996 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.105        |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00438      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059751244 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0691       |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00469      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008317518 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0827      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00454     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040151863 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.075        |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00441      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006386402 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0284      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00458     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008485216 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0423      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009105638 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00457     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004992917 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00443     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 93           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027228415 |\n",
      "|    clip_fraction        | 0.383        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0594       |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.0317      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00442      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048424243 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0408       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00436      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007955095 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.057       |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002489175 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0423      |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00415     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.699      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 94         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 27         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00541555 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.872      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0448     |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0308    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00433    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 63 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008350646 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0346      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00454     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 62 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006951773 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0806      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00437     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065239533 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0492       |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00414      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010649502 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0436      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00453     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010296768 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0385      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00432     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031562925 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0343       |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00431      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053504915 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0401       |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00447      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113722505 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0629       |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00423      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039416077 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0763       |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00435      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074065356 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0747       |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00417      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010064525 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0396      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00415     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 63 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010142127 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0578      |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00412     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 61 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008377614 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0703      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00425     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037857562 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0468       |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00428      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006477383 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0813      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00422     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.699      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00733816 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.872      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0482     |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0306    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00431    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006382674 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0684      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0043      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030253767 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0673       |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00424      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 61 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008769306 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0517      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00442     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011057785 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0861      |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008059487 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.037       |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00433     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072631687 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.065        |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00442      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010773649 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0705      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00421     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003687647 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0799      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054953517 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.045        |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00418      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081486795 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0388       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00429      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056147007 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0483       |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00416      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 65 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071520535 |\n",
      "|    clip_fraction        | 0.388        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0625       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0321      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00421      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005365914 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0795      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0041      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044134827 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0459       |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00445      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058294833 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.059        |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00383      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055228323 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0648       |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.0308      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00417      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062572137 |\n",
      "|    clip_fraction        | 0.384        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0453       |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.0317      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00438      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064409496 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0554       |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00425      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016614109 |\n",
      "|    clip_fraction        | 0.388        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0595       |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00419      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004905933 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0653      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00434     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004814139 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0464      |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007706681 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0533      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00403     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011868751 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0395      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0042      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008154693 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0433      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00404     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 64 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006911981 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0465      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 99         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00713394 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.876      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0369     |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00422    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjIAAAHUCAYAAAAgOcJbAAAAAXNSR0IArs4c6QAAIABJREFUeF7snQmYTmX/x7+GYey77GtZIokslciWvVIqSvEikRatVEJvvOqVNhVKC1JvKApFCCFZolBe+5qdMRhmxoz5X7/7/T/TYMyc8zznOec+9/M91+Uq5l5+9/f7O+f+zH3uc0621NTUVPCgAlSAClABKkAFqIAPFchGkPGhawyZClABKkAFqAAVUAoQZJgIVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZJgDVIAKUAEqQAWogG8VIMj41joGTgWoABWgAlSAChBkmANUgApQASpABaiAbxUgyPjWOgZOBagAFaACVIAKEGSYA1SAClABKkAFqIBvFSDI+NY6Bk4FqAAVoAJUgAoQZHyeA+fPn0dCQgJy5MiBbNmy+Xw0DJ8KUAEq4K4CqampSE5ORkxMDKKiotztnL05ogBBxhEZvWvkzJkzyJs3r3cBsGcqQAWogAEKxMfHI0+ePAaMJPKGQJDxuedJSUnIlSsX5CSMjo62NRpZzZk9ezY6dOhgxG8ipo1HzDRtTKaNx0SPTBxTZnl37tw59ctgYmIicubMaesaysJ6KECQ0cOHoKOQk1BOPgGaYEBm1qxZ6NixozEgY9J4AhOKSWOSCcWk8ZjokYljyizvQrmGBn3hZkVHFSDIOCqn+42FchKaNqmYNp5Im1DcP3uc6ZF554yO4WyFIBNOdb1vmyDjvQchRUCQ+Vs+TighpZIrlemRKzKH3IlpPhFkQk4JrRsgyGhtT9bBEWQIMllniT4lTJsgTVw1M3FMBBl9rgHhiIQgEw5VXWyTIEOQcTHdQu6KIBOyhK40YJpPBBlX0sazTggyNqVPSUnBoEGD8Omnn6r3t7Rp0wbjxo1D0aJFL2npX//6F+RP+kOeLnrsscfwzjvvqH8+fPgw+vbti/nz5yN37tzo1asXRowYYXnzLUGGIGMzhT0tbtoEaeLqhYljIsh4etqHvXOCjE2JBTImTpyIefPmoXDhwujevbt6RFaexMjq2Lp1K6pVq4ZffvkFDRo0UMVbtWqFAgUK4JNPPlFQ07p1azzyyCN4+umns2pO/ZwgQ5CxlCiaFCLIaGJEFmGY5hNBxh95F2yUBBmbylWoUAFDhgxRKydybN68GdWrV8fevXtRtmzZTFt75pln8OOPP2Lt2rWq3M6dO1G5cmVs27YNVapUUf82fvx4vP766xDosXIQZAgyVvJElzKmTZAmrl6YOCaCjC5XgPDEQZCxoWtcXBwKFSqEdevWoU6dOmk15WVK06ZNQ7t27S7bmrxsqUyZMupWU58+fVS5mTNnokePHjhx4kRavdWrV6vVmtOnT2f4xl65tSUnZeAIvMxJbnMF8x6ZOXPmoH379pZvZdmQy/WiootJ4wlMKCaNiR65floE1aFpPmU2HrmGyucJgnkXV1DispLjChBkbEgqqy7ly5fHjh07UKlSpbSaAiijR49Gly5dLtvalClT0K9fP+zfvx/58uVT5SZPnozBgwdj9+7dafVkJaZq1ao4cOAASpYseUl7w4YNw8svv3zJv0+fPl19b4kHFaAC3iiQlALEJgEp54HsUUCRXEA0P93jjRk2epXvLHXu3JkgY0Mz3YoSZGw4Iisnsi8mmBWZJk2aoGbNmhg7dmxaj1yRsSG+haKm/RbJFRkLpntYRD42uO3waSzafARfr9iEnaez41xKalpE8g3XcoVzo2nV4mhZ4wo0rFQEOXNEQeodOZWoyuXNlUP90e0w7VziioxuGeZsPAQZm3rKHpmhQ4eiZ8+equaWLVvUBt7M9sj8+eefCmJ+++03XHvttWk9BvbIbN++Xe2VkeODDz7AqFGjuEfGpi+BSZ+vvw9COBer+GGPjIBG+i/JC3R8tGwnvli1B6cSziF7VDb1c/nWfGLy37d5o7Nnw5Ul8iMmOgoJ585jz7F4xMsyzf8f+XPlQN0KhbHl0CkciEtI+/d6FQqjV+NKOHQyATPX/YXzqcD1FQujfsUi6r8l8sdk6pDEtGzrUazceVxBksDSoZOJ2H/iLGqVKYhHbqmCEgUyb+PiDvzgk5205R4ZO2r5ryxBxqZn8tSS3BKaO3euWp2RPS5yj1U+vni544knnsCqVauwYsWKS4rIU0uy7+ajjz7CkSNH1OPcDz/8MGRjsJWDm33/Vsm0i6+JcKabRwnnUrB482HMXn8Aa3fH4sTZc0g+n4r6FQujdtlC2LAvDqt2HkeS3C8CkCtHFM6npirYSDmfinJFcqPpVcWR+8ROPN6lDfLn/vujgwIV24+cxg9/HsL8Pw9h3Z6/98JdUSAXcuXIjtj4JJxKTM70VJeyJQvmRp7o7Dgu5RPOIUrBFHDybDJOJpxD6t8LQZe0JWAlsHQ2KQUViuZVYHPVFfkz7VM3n6xcCzMrQ5AJVUG96xNkbPojm20HDhyo3iMjG3jlcWl50kjeIyP7YARCZKNu4Dh79qza5Pvmm2+qR7UvPtK/R0a+Yt27d2+1ITgqytrNdYIMQcZmCntaPFwTpEDDjqPxWL/vBPafSMDBuAQcPJmAw6cSUSxvTlQvlR/R2aNw9HQi8sdE49qyhbDj6Gl8vGwnjp5OukCTqGxQoBI4ckRlQ8drS18WAKyO6fDJBKzfF4dqJfOjXJE8qvnE5BR8+9t+TPt1H4rly4l765dHwdzRWLPrOFbvOo41u2JxLP7C+C42ME/O7Gr1pknV4pBVH2mzWL5cKJY/l1pFmrHurwtARwCo9dUlcdNVxVC7TEGULBiDonlzIods7Pn/w+qYPE0mG50TZGyI5cOiBBkfmpY+ZIIMQcYPKXwmKRnLtx3DrqOn8dOvfyKq4BU4EHdWrWoIYDSrXgK9G1dC0Xy5shyOQMvmQ6ewelcs1u89gd3HzmDr4VOIPXMuy7oZFZDVlzuuK4MW1a9QMCGrL7/sOIY//jqJmmUKoEGlosiXyT6WcE76MlYZl9x2OpMkgJITBWKikZKaquAkf0wOxERnz3Tce4+fweFTCWoFaM6GA/h0+S6cPff3LS+pnDN7FK4pWxDVS+ZXt70Oxp1FofNx6NWmPq4pWwjF8+W64HabxCWwJ9CX/jZcUAYEUUn6l9WoE2eTULZwHnW7L7ODIBOEyD6qQpDxkVkZhUqQIcjonsLLtx3FwK/WY1/s2UxDzR2dHZWK5VW3bmQvyYAWV12yt0Mm5Me/WIdfdhy/pK0KRfOgXvnC6vZJyYK5cEWBGLW/5NCpBGw+eEq1WyxvLhyNT1S3eaS/HjdVRN3yhUOSMJwgE1Jgl6l87HQiftp6BKt2xmLzwZNqRUqgMv1G5YxWfRpUKoJbry6JnUdPY8a6/Wp1Sw5ZscqRPRsK58mJ2mULqlty1a7If8HKU6A9ub2182i8AqvDJxOxcX8cft97Qu01EliU/T1Rsv/o/wFJ2hafBFQEvk4nJCuoE9gKwFiRvDnRrFoJtK9dEs2rX5HhqAky4cgkfdokyOjjRVCREGQIMkEljguVdh+Lx1sLtqpbG3JcW64QbqhcBLF7t6LDLY1QrkheROeIgkysHy7didnr919wC0RumdxSrTiSkmUDazYFJbKiIJtvZd9IixpXKAipUjwvKhbNi8J5/96f4sLw0rrwG8hkpI3sFZLbXvIUVpnCuVEkTw5MmLUUR6NLYOeReBw4mXDJPhwBD1lRkz/J589fcDsu0Efl4nnRsFJRyG21rYdPY2/smUz381j1TUBHbp/lzZkdu46dUdXa1y6F9+6rS5CxKqJB5QgyPjeTIEOQ0S2Fk1POY9S8zZiwbKea5OQ36ufaVEP3GyoCSFWf8+jYseMl+8AEUGQjq9zaGb9kRxoAXTy+ljVKYPTddVAwT7QWQzcBZC4W8uIxnU5MxqL/HlZ/iufPhbvqlUXVizYMy76k3/bGYsNf/wMiWfWSPUrpD4HTKsXzoVTBGLWHp2qJfGr1TfYFnUpIVt7LLbPArSvJpYTkFLVaJHkkj6rLnp4S+XOpW5Jy/HXiLBZuOoQri+fDjVcWI8hocVa4GwRBxl29He+NIEOQcTypQmhQJrzHPl+r3q0i+y7ua1gejzSrkvYIsZ1JXx5Tlv0dsrdDVgxk867cvmhbq6R6akeXw86YdIk5qzicGNP586nq1pGATZlCudWTUqUKxHjiHW8tZeW4v39OkPG3f/xoZDr/nLj46pYOfhrTiu3H8OLMDdhxJF7d+vmoe331HpP0h5/GYzUXOCarSnlXjiDjnfZu9EyQcUPlMPbBFRmuyIQxvbJsWn7rlid8pqzco/avyHFt2YIY/8D16hbAxQcn/Swl1aKAaT4RZLRIq7AFQZAJm7TuNEyQIci4k2mX9iJPnzw0aY3aDyGHbP58+taqeKBRhQveScIVGa8cCr5fgkzw2rGm+woQZNzX3NEeCTIEmcslVFLyeSzdegQLNh1Sr8yXt9KqP9HZ0aJ6CTSsXDTDqlJP4ETeKRLYiyL/Jo/GBg558dw/PlmtXtZWuVhe3Fu/nNoAKk+SZHaYNkHKWDkmRy9pYWmMKzJhkVWbRgky2lgRXCAEGYLMxZkjG2Mn/rwLY5dsx4nLvCRO3s8xpXfDS2AmPjEZ3T9ehTW7Y9UGzZuvKobf9p5QL6Dr27QKnmtdDcu2HcXDk39VL2hrd01JvHlvHbUh18rBSd+KSt6XMc0ngoz3ORXOCAgy4VTXhbYJMgSZgAJxZ8/h67X71AcOAy+fq1u+kHq9fqmCudWr6xPPnce6vSfUq+vltfSzHmuM0oVyqybkZWU9PlmlPj4oqy+yCnPxIa/BX7H9qHocVm4hDbutZpZvVU3fhmkTJFdkXLjIOdAFQcYBETVugiCjsTlWQiPIRCbIrNxxDF+u3ovYM0nqQ4dxZ86p92kEvsZcp1whDG5fA9dXLHJJGskG3X5TfsW8Pw6pVZc28jhzNmDW7wfUI87li+TBlw83wrHTSVi7JxbXlCmobk31mbQm7QOHT7WqiseaX2n79fQEGStntfdlTPOJION9ToUzAoJMONV1oW2CTOSBjNzquXf8ijRoCSggr3GXvS/dGlVQt4Qy+waOvO+l6we/qHd8pD8EWsZ2q6u+X3PxsfGvOLz6/X9xW53SuOf6ckFlt2kTJFdkgkoD1ysRZFyX3NUOCTKuyu18ZwSZyAKZ/SfO4vb3lqvX9MvL5jrWLo1CeaLVH3lZXFYfEEyfgfLWXYGT5duPqltOba8pieolCzifpOlaJMiEVV7HGjfNJ4KMY6mhZUMEGS1tsR4UQSZyQEa2rNw19me1itKyxhUY/0A9W/tTrGdV+EqaNkFyRSZ8ueJkywQZJ9XUry2CjH6e2IqIIBM5IPPavM3qG0RXlciHmf1vUt+d8dtBkPGHY6b5RJDxR94FGyVBJljlNKlHkIkMkCl69Y144ONViI6KwjeP3oQapcJ7Cyhc6W3aBMkVmXBlirPtEmSc1VO31ggyujliMx6CjPkgM23GLLy1JR8OxCXgpQ5Xo1fjSjazRJ/iBBl9vMgsEtN8Isj4I++CjZIgE6xymtQjyJgPMj3emYOfDkbhhspF1UvsdPrys93TwLQJkisydjPAm/IEGW90d6tXgoxbSoepH4KM2SCzbs9x3Pn+z4jOkR3zBjRBpWJ5w5RJ7jRLkHFH51B7Mc0ngkyoGaF3fYKM3v5kGR1BxlyQOZOUjM5jf8afB07hqZZX4fGWVbPMB90LmDZBckVG94z7X3wEGX/4FGyUBJlgldOkHkHGTJCR7yX1mrgay7cdQ8ncqVj0fGvkzhmtSdYFHwZBJnjt3Kxpmk8EGTezx/2+CDLua+5ojwQZ80DmVMI5PPr5OizZckR9QqB35VPo3rkjoqL+/vq0o0nkYmOmTZBZ/bbvorSOdmWaTwQZR9NDu8YIMtpZYi8ggoxZICOfH3j8i3XYc/wMShWMwRcPNcTvyxeiY0eCjL0zw73Spk36JsIZQca988GLnggyXqjuYJ8EGXNAZu7Gg3jsi7Xqy9L1KxbGW12uQ6kCuTBr1iyCjIPnjNNNEWScVtT59ggyzmuqU4sEGZ3cCCIWgowZIPPDHwfxyJS1SD6fikduqQL5unSO7FGZblIMIl08r8JJ33MLLAVgmk8EGUu2+7YQQca31v0vcIKMf0EmNTUVy7Ydxddr/8Ls9fvVSsyzrauhf7Mr0wYVSROKX09F0zzirSW/ZmLkxk2Q8bn3BBl/gkzcmXN4etrvWLDpkBpAVDbg6VsvhJhIm1D8eioSZPR3jisy+nsUSoQEmVDU06AuQcZ/IPPn/pPoM3kN9sWeRbF8OdG3aRXcdm1plCgQc0lGmTZJmjYeE2HTxDERZDSYrMIYAkEmjOK60TRBxl8gs2rncfT6dDVOJSajYaUiGNP1ugwBJjAq0yZ+08Zj4qRv4pgIMm7MRt71QZDxTntHeibI+Adklm09ql5yl5h8HvdeXw4jOtVSG3ozO0yb+E0bj4mTvoljIsg4Mt1o2whBRltrrAVGkPEHyCQmp6D560vw14mzeLhJZQxqWx3ZsmXL0mTTJn7TxmPipG/imAgyWV5qfF2AIONr+/jUUnr7dJ4kJ63YhSHf/IFryxXCzEdutAQxkTah+PVU1DnvgtXUtDERZILNBH/UI8j4w6fLRskVGf1XZM4mpaDJqEU4cioRn/VqiMZXFbOcdZE0oVgWRbOCpnkUaQAdyjVUs1SM2HAIMj63PpST0LQLsK7jeWfhVrwxfwtuqFwUnz/U0PJqTKRNKH49FXXNu1D0NG1MXJEJJRv0r0uQ0d+jTCMkyOi7IiMvvHtzwVYIyMjxVb8bUa9CYVsZF0kTii1hNCpsmkeRBtChXEM1SsOIDoUg43P7QzkJTbsA6zSehHMpeGba75i9/gCis2fDq3fWxl31ytrONp3GZDv4DCqYNh4TJ30Tx8QVGSfOXn3bIMjo642lyAgy+q3IHD6VgD6TfoV8ybpwnmiMf+B6NKhUxJKfFxcybeI3bTwmTvomjokgE9TlxzeVCDK+sSrjQAky+oDM9F/34Zvf/oK89E7eFVOleF583KM+KhTNG3SWmTbxmzYeEyd9E8dEkAn6EuSLigQZX9h0+SAJMu6CjOx7mf/nISzafAT/PXgS7WqVQu+bK2Hiz7swbNafKpjsUdnQqsYVeK1zbRTMHR1Shpk28Zs2HhMnfRPHRJAJ6TKkfWWCjPYWZR4gQcZdkJmycjdenLHxAlOaVC2OZVuPIBXAyE7XoH3tUsgfExrABDowbeI3bTwmTvomjokg4/OJLovwCTI+95cg4x7InEs5j1tGLVZv5x3Q8ipULp4PL87YgFMJySqIwe1roPfNlR3NKNMmftPGY+Kkb+KYCDKOXpa0a4wgo50l9gIiyLgHMrIHRp5Eql22IL7pf5N6H8zWQ6cw6OsN6h0xT99a1dY7Yqw4bdrEb9p4TJz0TRwTQcbK1ca/ZQgy/vVORU6QcQdkUs6notWbS7DjSDzGdauHNrVKupI5pk38po3HxEnfxDERZFy5XHnWCUHGM+md6ZggE36QiTt7Dm8t2IJPlu/ClSXy4YcBTRAVlfUHH51w2LSJ37TxmDjpmzgmgowTVyN92yDI6OuNpcgIMuEFmZ+3H8UjU9bixJlzkI9Vj+9WD7fWdGc1JtImFEsJr2EhwpmGplwUEkFGf49CiZAgY1O9lJQUDBo0CJ9++ikSEhLQpk0bjBs3DkWLFs2wpcOHD+PZZ5/F7Nmz1W2gypUr47vvvkPp0qVVefn/l156Cdu2bUPevHlxxx134I033kBMTIylyAgy4QMZedS6zVtLsfnQKTSrVhwD21ZH9ZIFLPniVCHTJknTxmMibJo4JoKMU1ckPdshyNj0ZcSIEZg4cSLmzZuHwoULo3v37gicJBc3JaBTv359NGrUCCNHjkSRIkWwadMmlCtXDgUKFIBATvny5RW49O3bF/v370fbtm1x2223QfqxchBkwgcyS7YcQfePV6FC0Tz48elb1Pth3D5Mm/hNG4+Jk76JYyLIuH3lcrc/goxNvStUqIAhQ4agV69equbmzZtRvXp17N27F2XLXvgtnfHjx2P48OHYsWMHoqMvfa/I2rVrUa9ePbWykytXLtXe888/jw0bNqgVHCsHQSZ8INNtwkos23YUr9xeEw/cUNGKHY6XMW3iN208Jk76Jo6JIOP4pUmrBgkyNuyIi4tDoUKFsG7dOtSpUyetptwSmjZtGtq1a3dBa126dEFsbKxadZkxYwaKFSuGfv364YknnlDl5OTq0KGDuj31yCOP4K+//lJtyM/79OmTYWRya0vqBQ4BGelfYCgjWMpseNLOnDlz0L59e0RFRdlQQs+iTo7nz/0n0eHd5epbScuea4bcObN7Mmgnx+TJAC7q1LTxBM5jk84jE8eUWd7JNVRu5SclJdm+hupwTjEGgCBjIwtk1UWgRFZYKlWqlFazTJkyGD16NARc0h8tW7bEwoUL8dZbbymAWb9+vYKWMWPGoGvXrqro1KlT8dhjj+HYsWMQSLn//vsxadKky4LFsGHD8PLLL18S9fTp05EjRw4bo2HRzBSYsi0Kq45EoXXZ82hX7m9wpGpUgAqYpUBycjI6d+5MkPGxrQQZG+adOHFC7YuxuiLTqVMnrF69Gvv27UvrZcCAAWovjADMokWL1ArMV199hdatW+Po0aN46KGH1F4a2Uyc0cEVmcsb5tRv+wnnUtDgXwsRn5SCnwc2wxUFrG28tpFKlos6NSbLHYa5oGnjMXH1wsQxcUUmzCe2x80TZGwaIHtkhg4dip49e6qaW7ZsQbVq1TLcIyMrJxMmTFA/CxwCMgcOHMCXX36J119/Xd2SWrlyZdrPZ82ahQcffFDdkrJycI/M3yo5tf9i7sYD6PvZWvW23i/6NLJiQ9jKODWmsAVos2HTxhOY9OW87dixoxG3aE0cE/fI2DxRfVacIGPTMHmaaPLkyZg7d65anenRo4d6rDqjzbm7d+9GjRo1MGrUKPVU0saNGyG3m959913ce++9WL58OVq1aoWZM2eq/8rtJQGk+Ph4dUvKykGQcR5k+k9ZizkbDmBEp1q4v2EFKzaErYxpE79p4zFx0jdxTASZsF2itGiYIGPTBrm1M3DgQHXrJzExUd0SkqeT5D0yU6ZMwcMPP4zTp0+ntbp48WI8+eSTauVG3h0jKzL9+/dP+7k8yi0rMwI9suGsadOm6nFseUTbykGQcRZkTicm4/rh83GumYuwAAAgAElEQVQuJRWrX2yJInlzWrEhbGVMm/hNG4+Jk76JYyLIhO0SpUXDBBktbAg+CIKMsyDzzW9/4Yn//IamVYtjYs8GwRvjUE3TJn7TxmPipG/imAgyDl2QNG2GIKOpMVbDIsg4BzLnz6ei64e/YOXO43j97mvRud6F7wWy6omT5Uyb+E0bj4mTvoljIsg4eVXSry2CjH6e2IqIIOMcyLy3aBtGzduMEvlzYeHTTZE/5tKXGNoyx4HCpk38po3HxEnfxDERZBy4GGncBEFGY3OshEaQcQZklm87igc+Wols2bLhi4caoUGlIlbkD3sZ0yZ+08Zj4qRv4pgIMmG/VHnaAUHGU/lD75wgEzrIHIxLQPt3luJYfBJeaFcdfZpUCd0Yh1owbeI3bTwmTvomjokg49AFSdNmCDKaGmM1LIJMaCBzLuU8un7wC9bsjkXrmldgXLd6alVGl8O0id+08Zg46Zs4JoKMLle08MRBkAmPrq61SpAJDWSGz/4TE5btVF+4nvVYYxTQYF9M+uQxbeI3bTwmTvomjokg49qU5ElHBBlPZHeuU4JM8CDz3YYDeGTKWuTKEYUZj9yEq0sXcM4Yh1oybeI3bTwmTvomjokg49AFSdNmCDKaGmM1LIJMcCCz/chp3DZmmfqe0r8718Y911t7AaFVX5wqZ9rEb9p4TJz0TRwTQcapK5Ke7RBk9PTFclQEGfsgE5+YjDveW46th0+ja4NyGHlnbct6u13QtInftPGYOOmbOCaCjNtXLnf7I8i4q7fjvRFk7IFMamqqenPvt7/vR60yBTC9742Iic7uuC9ONWjaxG/aeEyc9E0cE0HGqSuSnu0QZPT0xXJUBBl7IDNpxS4M+eYPFMwdjdmPNUa5Inksa+1FQdMmftPGY+Kkb+KYCDJeXL3c65Mg457WYemJIGMdZH7ZcUy99E4+CPlJj/poVr1EWDxxslHTJn7TxmPipG/imAgyTl6V9GuLIKOfJ7YiIshYA5lth0/hzvd/xsmEZAxoeRUGtKxqS2evCps28Zs2HhMnfRPHRJDx6grmTr8EGXd0DlsvBJmsQSbuzDm0H7MU+2LP4s7rymD0Pddq9dK7zJLDtInftPGYOOmbOCaCTNimIC0aJshoYUPwQRBksgaZj5btxCuz/0SDikXwWe+GyJkjKnjBXa5p2sRv2nhMnPRNHBNBxuULl8vdEWRcFtzp7ggyWYPM7e8uw+/74vDpP+rjlmr674tJnyOmTfymjcfESd/EMRFknJ559GqPIKOXH7ajIchkDjK7jsbjltcXo0jenFj5QgtEZ/fPakykTSi2k1+TCoQzTYzIJAyCjP4ehRIhQSYU9TSoS5DJHGTeWbgVb8zfggcaVcArd9TSwDF7IZg2SZo2HhNh08QxEWTsXXf8Vpog4zfHLoqXIPO3IHuPx2PUfxaie7ubcF35IpCPWLd8Ywm2H4nHtL43oH7FIr5z27SJ37TxmDjpmzgmgozvLn22AibI2JJLv8IEmf95cjYpBbIXZsvh0+rvRfPmRMmCMfhj/0mULhiDZQObIyoqm34GZhGRaRO/aeMxcdI3cUwEGd9d+mwFTJCxJZd+hQky//PkhRkb8PnKPSgek4pCBfKr7ygFjsdbXIWnWvnjvTEXZ5hpE79p4zFx0jdxTAQZ/eYuJyMiyDippgdtEWSAuRsPou9nvyJPzux48upE9LqnI46fOYdjp5OQcC4FNUsXQA6fbfINpJJpE79p4zFx0jdxTAQZDyYnF7skyLgodji6IsgA94xbgVW7jmPknbWQ58Bv6NixI6Ki/PV00uVyw7SJ37TxmDjpmzgmgkw4Zh992iTI6ONFUJFEOsgkp5zHNcN+QGJyCtYPbYWF874nyASVSe5UIsi4o3OovZjmE0Em1IzQuz5BRm9/sowu0kFm04GTaPv2UlQvmR/fPd4Ys2bNIshkmTXeFTBtgjRx9cLEMRFkvDvn3eiZIOOGymHsI9JB5j+r9mDQ1xvQpX45/KtTLYJMGHPNiaYJMk6oGP42TPOJIBP+nPGyB4KMl+o70Hekg8zzX6/HF6v2YuSd1+De68sSZBzIqXA2YdoEaeLqhYljIsiE86z2vm2CjPcehBRBpIOM3FaS20vfP3Ezql2RjyATUjaFvzJBJvwaO9GDaT4RZJzICn3bIMjo642lyCIZZM4kJauNvjmzR2HDsFsh77vjHhlLaeNZIdMmSBNXL0wcE0HGs1PelY4JMq7IHL5OIhlkVu86jrvHrUCDikUwte8N4CQZvjxzqmV65JSS4W3HNJ8IMuHNF69bJ8h47UCI/UcyyExYugPD52zCQzdXwovtrybIhJhLblQ3bYI0cfXCxDERZNw4u73rgyDjnfaO9BzJIPPo52sxe/0BvHvfdehQuzRBxpGMCm8jBJnw6utU66b5RJBxKjP0bIcgo6cvlqOKVJA5cSYJN736I+KTUvDzoOYoXSg3QcZy1nhX0LQJ0sTVCxPHRJDx7px3o2eCjBsqh7GPSAWZN+ZvwTsLt6JtrZIY262eUpiTZBgTzaGm6ZFDQoa5GdN8IsiEOWE8bp4g47EBoXYfiSBzMuGcWo05lZCMOY83Rs3SBQkyoSaSS/VNmyAJ0C4lTojdEGRCFFDz6gQZzQ3KKrxIBJkxC7di9PwtaFmjBCZ0r58mESfJrLLF+5/TI+89sBKBaT4RZKy47t8yBBn/eqcijzSQSTiXghtf/RHH45PwTf+bcG25QgQZH+WwaRMkV2T8kXwEGX/4FGyUBJlgldOkXqSBzJer92DgVxtwQ+Wi+KJPowtc4CSpSVJmEgY90t8jE+GMIOOPvAs2SoJMsMppUi+SQCY1NVV96fq/B0/hgwfq4daaJQkymuSh1TAIMlaV8racaT4RZLzNp3D3TpAJt8Jhbj+SQObn7Udx34crUa5Ibix+phmyyzcJ0h2mXXwj7TfjMJ8qYWueeRc2aR1rmCDjmJRaNkSQ0dIW60FFEsj0nrgGCzYdwuD2NdD75sqXiMQJxXreeFWSHnmlvL1+TfOJIGPPf7+VJsj4zbGL4o0UkDkQd1Zt8s0dnR2/vNACBWKiCTI+zF3TJkgTV81MHBNBxocXCxshE2RsiKVj0UgBmfcXb8O/527G3fXKYtTd12ZoBSdJHTP0wpjokf4eEWT84RGj/FsBgozPsyESQEY2+bZ4Ywl2HInHl30aoWHlogQZn+YtQcYfxpnmE1dk/JF3wUZJkLGpXEpKCgYNGoRPP/0UCQkJaNOmDcaNG4eiRTOeXA8fPoxnn30Ws2fPVu98qVy5Mr777juULl1a9ZycnIxXXnlFtXf06FGULFkS7777Ltq2bWspskgAmXV7YtHp/Z/VJt8lzzRD1EWbfANCmXbxjbTfjC0lvIaFmHcamnJRSAQZ/T0KJUKCjE31RowYgYkTJ2LevHkoXLgwunfvnvaNn4ubEtCpX78+GjVqhJEjR6JIkSLYtGkTypUrhwIFCqjivXv3xh9//IFPPvkE1apVw4EDB5CUlISKFStaiiwSQObFGRswZeUeDGh5FQa0rHpZXTihWEoZTwvRI0/lt9y5aT4RZCxb78uCEQUyy5cvR9myZVGhQgXISslzzz2HHDly4NVXX0WxYsUsGSh1hwwZgl69eqnymzdvRvXq1bF3717Vdvpj/PjxGD58OHbs2IHo6Es3pwbqCtxIG8EcpoNMYnIK6g9fgJMJyVj6XDOUK5KHIBNMomhSx7QJ0sRVMxPHRJDR5AIQpjAiCmRq166Nr7/+GldeeSX+8Y9/YN++fYiJiUGePHnw5ZdfZilxXFwcChUqhHXr1qFOnTpp5fPmzYtp06ahXbt2F7TRpUsXxMbGonz58pgxY4aCpX79+uGJJ55Q5eSW1MCBA/Hyyy9j9OjRyJYtGzp27IjXXnsN+fLlyzAeubUlJ2XgEJCR/mX1JyNYymxQ0s6cOXPQvn17REVFZTl+Lwqs3HEMXSeswnXlCuGrfjdkGoIfxmNXQ9PGZNp4ApO+7ucR8+7y1zq5hso8ICvhdq+hdnVl+fAoEFEgI7eCBCxk82iJEiXULR2BGNm3Iis0WR2y6iJQIisslSpVSitepkwZBSICLumPli1bYuHChXjrrbcUwKxfv17tqRkzZgy6du2qVmteeuklVU9Wb+Lj43HnnXdCgEv+ntExbNgwBT4XH9OnT1erS6Yd3++Nwtx9Ubi1zHm0L/83wJk2To6HClABbxSQfYqdO3cmyHgjvyO9RhTIyIqIwIjcypG9LRs2bFCrGwULFsSpU6eyFPTEiRNqX4zVFZlOnTph9erVauUncAwYMAD79+/H1KlT8fbbb0P+vnXrVrVKJMfMmTPRp0+fy4JVpK3IdPngF6zaFYvJPevjpiszv/3H3/azTGHPC9Ajzy2wFIBpPmU2Hq7IWEoJrQtFFMjcc889OHv2LI4dO4YWLVqop4Vkn0qHDh0UTFg5ZI/M0KFD0bNnT1V8y5YtapNuRntkZOVkwoQJ6mfpQUY29MqtrCVLluCWW27Btm3bUKVKlTSQefjhh3Ho0CEr4Rj99Wv50nXtYT8gFalYP7Q1cufMnqkm3H9hKWU8LUSPPJXfcuem+cQ9Mpat92XBiAIZWVEZNWoUcubMqTb65s6dWz0WvX379rR9K1m5KE8tTZ48GXPnzlWrMz169FAwIe1cfOzevRs1atRQffbt2xcbN26E3G6Sx6vvvfdetRoke20Ct5Lk1pKs4sjfx44dm1Uo6ucmb/b9edtR3DdhJRpULIKpfTPfHyNamHbxNXFM9MjSae15IdN8Ish4nlJhDSCiQMYJJeXWjmzQlfe+JCYmonXr1mo/i7xHZsqUKZDVlNOnT6d1tXjxYjz55JNq5UbeHSO3kvr375/2c4Ed2T/z008/qVtcd911l3pUWzbwWjlMBpnRP2zGmB+34fHmV+KpW6tlKYdpF1+CTJaWa1GAeaeFDUGv1oZyDdV/5JERofEg889//tOSk/JItR+PUE5C3S/Ad4/7Gat3xeLzhxrixipZPx6v+3iCyS/TxmTaeEyETRPHxBWZYK4+/qljPMi0atUqzQ15WklWPuTtubLXRVZDDh48iKZNm2L+/Pn+cS1dpKaCzNmkFNR+eZ56JH390FsRE535/hgTL74mjokg44/LjGk+EWT8kXfBRmk8yKQX5qmnnlLvcnn++efVBCmH3MaRTwPI49N+PEwFmVm/78djX6zDDZWL4os+jSxZY9rFlyBjyXbPCzHvPLcgywAIMllK5OsCEQUyxYsXV58ASP++FXmHgKzQCMz48TAVZDqP/RlrdsfizXuvRafrLnxj8uV84oSifwbTI/09ijSADuUa6g83zY8yokBGvnE0a9asC97KK++Ekbfppn/Xi59sD+Uk1HVS2fhXHDqMWYZi+XJi+aDmyJUj69tKJl58TRyTrjkXyjnPMYWinjt1uSLjjs5e9RJRICO3keQldPJkkXyUcdeuXfjggw/w2GOP4YUXXvDKg5D6NRFknpn2O6b/us/y00oBATmhhJRKrlSmR67IHHInpvlEkAk5JbRuIKJARpyYNGmSeg/MX3/9Bfm0wAMPPIAHH3xQa5MyC840kDken4RGIxfi/PlUtRpzRYEYy96YdvHlioxl6z0tyLzzVH5LnRNkLMnk20IRAzLy/hf5HtEdd9yBXLly+dawiwM3DWRmrNuHJ7/8HW1qlsS4B+rZ8okTii25PClMjzyR3XanpvlEkLGdAr6qEDEgI67kz5/f0jeV/OSgaSAzYs6f+HDpTgxuXwO9b65sywrTLr5ckbFlv2eFmXeeSW+5Y4KMZal8WTCiQKZ58+bqS9TyCQBTDtNA5v4Jv2D5tmOWX4KX3kdOKPpnNT3S36NIA+hQrqH+cNP8KCMKZIYPH44PP/xQbfaVF+IF3iUjNt93332+dDuUk1C3SUVeWFj3lfmIPXMOvw1phUJ5ctryRLfx2Ar+MoVNG5Np4zFx0jdxTFyRceJqpG8bEQUylSpVytAJAZodO3bo61ImkZkEMgfjEtRG39IFY/Dz8y1s+8FJ0rZkrlegR65LHlSHpvlEkAkqDXxTKaJAxjeu2AjUJJD58b+H0PPTNWhRvQQ+6lHfhgr/K2raxdfEMdEj22ntSQXTfCLIeJJGrnVKkHFN6vB0ZBLIvLdoG0bN24zHml+Jpy187fpiRU27+BJkwnPOON0q885pRZ1vjyDjvKY6tRhRIHP27FnIPpmFCxfiyJEjkD0ZgYO3lqI8z8v+U9ZizoYDGHt/XbS9ppTteDih2JbM9Qr0yHXJg+rQNJ8IMkGlgW8qRRTI9O3bF8uWLUO/fv0wcOBAvPbaa3j33Xdx//33Y/Dgwb4xLX2gJq3INH99MXYcjcfiZ25BxWJ5bfth2sWXKzK2U8CTCsw7T2S31SlBxpZcviscUSAjb/JdunQpKleujEKFCuHEiRP4888/1ScKZJXGj4cpIBOfmIxaw+YhT3R2bBjWGlFR//s6uZ2DE4odtbwpS4+80d1ur6b5RJCxmwH+Kh9RIFOwYEHExcUph0qUKKE+FJkzZ04UKFAAJ0+e9Jdz/x+tKSDz6+5Y3DX2Z1xfoTCm97sxKC9Mu/hyRSaoNHC9EvPOdcltd0iQsS2ZrypEFMjUqVMHX3zxBWrUqIEmTZqod8fIysyzzz6LvXv3+sq4QLCmgMxnv+zG4Jkb8UCjCnjljlpBecEJJSjZXK1Ej1yVO+jOTPOJIBN0KviiYkSBzJdffqnApXXr1pg/fz46deqExMREjB07Fr179/aFYRcHaQrIvDBjAz5fuQcj77wGXRuUD8oL0y6+XJEJKg1cr8S8c11y2x0SZGxL5qsKEQUyGUFAUlIS8ua1v7FUF5dNAZlO7y/Huj0n8E3/m3BtuUJBycsJJSjZXK1Ej1yVO+jOTPOJIBN0KviiYkSBjDyldOutt+K6667zhTlWgjQBZFLOp6LW0HlITE7Bn/9sg5jo7FaGfkkZ0y6+XJEJKg1cr8S8c11y2x0SZGxL5qsKEQUyt912G5YsWaI2+MoHJFu2bIlWrVqhYsWKvjItfbAmgMz2I6fRYvQSXFkiHxY81TRoLzihBC2daxXpkWtSh9SRaT4RZEJKB+0rRxTIiBspKSlYuXIlFixYoP6sWrUK5cqVw9atW7U3K6MATQCZ2ev349HP1+G2a0vjna7Br5aZdvHliow/Tknmnf4+EWT09yiUCCMOZESsDRs24IcfflAbflesWIFatWph+fLloejoWV0TQObfc/+L9xdvx8A21dHvlipBa8kJJWjpXKtIj1yTOqSOTPOJIBNSOmhfOaJA5oEHHlCrMIULF1a3leRPs2bNkD9/fu2NulyAJoDMPz5ZhUWbj2BizwZoWrV40F6YdvHlikzQqeBqReadq3IH1RlBJijZfFMpokAmT548KFu2LARoBGIaNmyIqCjvvzEUSraYADIN/7UAh04mYtWLLVAif0zQcnBCCVo61yrSI9ekDqkj03wiyISUDtpXjiiQkUet5VtLgf0x27dvx80336w2/Pbv3197szIK0O8gc+x0IuoNX4Bi+XJhzeCWIXlg2sWXKzIhpYNrlZl3rkkddEcEmaCl80XFiAKZ9I5s3rwZU6dOxejRo3Hq1Cm1CdiPh99BZtnWo+j20Uo0qVock3o2CMkCTighyedKZXrkiswhd2KaTwSZkFNC6wYiCmTkzb6ywVf+HDp0SN1aatGihVqRueGGG7Q26nLB+R1kxi/ZjpHf/xcPN62M59vWCMkD0y6+XJEJKR1cq8y8c03qoDsiyAQtnS8qRhTI1K5dO22Tb9OmTX39Rt9AdvkdZHpPXIMFmw5h7P110faaUiGdNJxQQpLPlcr0yBWZQ+7ENJ8IMiGnhNYNRBTIaO1EkMH5GWTkjb51/vkDTiUkY91LrVA4b84gVfhfNdMuviaOiR6FlOKuVTbNJ4KMa6njSUcRBzKy2XfSpEk4cOAAZs2ahV9//RXx8fHqa9h+PPwMMuv3ncBt7y5HjVIF8P0TN4csv2kXX4JMyCnhSgPMO1dkDqkTgkxI8mlfOaJA5vPPP8ejjz6Kbt26YeLEiYiLi8PatWvx1FNPYfHixdqblVGAfgaZcUu249Xv/4tejSvhpQ5Xh6w/J5SQJQx7A/Qo7BI70oFpPhFkHEkLbRuJKJCpWbOmApjrr79evRQvNjYW8kh2mTJlcOTIEW1NyiwwP4PMgx+vwk9bjmDCg9ej5dVXhKy/aRdfrsiEnBKuNMC8c0XmkDohyIQkn/aVIwpkAvAirhQpUgTHjx9X+yqKFSum/t+Ph19BJin5vNofk3AuBb8NvRUFYqJDlp8TSsgShr0BehR2iR3pwDSfCDKOpIW2jUQUyMhKzDvvvIMbb7wxDWRkz8yzzz6rvrnkx8OvILNm13F0HrcC15YtiG8ebeyI9KZdfLki40hahL0R5l3YJQ65A4JMyBJq3UBEgczMmTPx0EMP4YknnsBrr72GYcOG4a233sIHH3yAtm3bam3U5YLzK8i8vWAr3lywxZH3xwS04YSifwrTI/09ijSADuUa6g83zY8yYkBG3tw7ffp09e6Y8ePHY+fOnahYsaKCGnkhnl+PUE5CryaVuDPn0Gz0YhyPT8KXfRqhYeWijsjv1XgcCf4yjZg2JtPGY+Kkb+KYuCITzquU921HDMiI1PKVa/kcgUmHH0Hm5Vl/4JPlu9C8egl83KO+Y3ZwknRMyrA1RI/CJq2jDZvmE0HG0fTQrrGIApnmzZurW0nyhl9TDr+BzNZDp9Dm7aWIygb88GRTVCqW1zErTLv4Rtpvxo4lgssNMe9cFjyI7ggyQYjmoyoRBTLDhw/Hhx9+iIcffhgVKlRAtmzZ0qy67777fGTb36H6DWQemrQG8/885OjemIAanFD0T2F6pL9HkQbQoVxD/eGm+VFGFMhUqlQpQ0cFaHbs2OFLt0M5Cd2eVPYeP4MmoxYhT3R2rHihhSOPXKc3ze3xuJEwpo3JtPGYOOmbOCauyLhxtfKuj4gCGe9kDl/PfgKZkd9twvifduCBRhXwyh21HBeFk6TjkjreID1yXNKwNGiaTwSZsKSJNo0SZLSxIrhA/AIy8uK7RiMX4sSZc5j/ZBNcdUX+4AacSS3TLr6R9pux4wnhUoPMO5eEDqEbgkwI4vmgKkHGByZlFqJfQGbqmr14bvp63FilKD5/qFFYVOeEEhZZHW2UHjkqZ9gaM80ngkzYUkWLhgkyNm2Q99EMGjQIn376KRISEtCmTRuMGzcORYtm/C6Uw4cPqzcHz549GwIdlStXxnfffYfSpUtf0PO+ffsg34IqXrw4tm3bZjkqP4BMamoq2r2zDJsOnMS4bnXRplYpy+OzU9C0iy9XZOy4711Z5p132lvtmSBjVSl/liPI2PRtxIgR6sOT8+bNUx+e7N69u/pe06xZsy5pSUCnfv36aNSoEUaOHKk+i7Bp0yaUK1cOBQoUuKC8AJFAye7du40DmQV/HkLvSWtQvkge/Ph0U+TIHmVTdWvFOaFY08nLUvTIS/Wt922aTwQZ6977sSRBxqZr8tj2kCFD0KtXL1Vz8+bNqF69Ovbu3YuyZcte0Jq8QVge+ZYnoqKjL/9RRHkkfMaMGbjnnntUeZNWZGQ15o73f8bve0/g1TuvQZcG5W0qbr24aRdfrshY997Lksw7L9W31jdBxppOfi1FkLHhXFxcHAoVKoR169ahTp06aTXlswfTpk1Du3btLmitS5cuiI2NRfny5RWoyFe2+/Xrpz6LEDj27NmDm266SX20csGCBVmCjNzakpMycMgqjvQvqz+ZwVJGw5R25syZg/bt2yMqKjyrJEu3HkX3T1ajVMEYLHq6KXLmCE8/gUk/3OOxkS6OFHXDI0cCtdiIaeNh3lk03uNimeWdXENjYmKQlJRk+xrq8bDY/f8rQJCxkQqy6iJQIiss6d9JU6ZMGYwePRoCLumPli1bYuHCheptwgIw69evV3tqxowZg65du6qi8p2nzp07q5f0yb6brFZk5EOXL7/88iVRy3ekcuTIYWM07hQd80cUtp2MQudKKbi5ZKo7nbIXKkAFqIBFBZKTk9U1mCBjUTANixFkbJhy4sQJtS/G6opMp06dsHr1ashG3sAxYMAA7N+/H1OnTlUfr/zyyy8V7MhL+ayAjJ9WZBKTU1Br2Hxkj8qG315qiZjo7DbUtl+Uv+3b18ztGvTIbcWD6880n7giE1we+KUWQcamU7JHZujQoejZs6equWXLFlSrVi3DPTKycjJhwgT1s/Qgc+DAAQUwd9xxBxYtWoTcuXOrH589exbx8fHqFpQ82VS3bt0so9P5qaXNB0+h9Vs/oUapAvj+iZuzHEuoBbhXIVQFw1+fHoVfYyd6MM0n7pFxIiv0bYMgY9MbeWpp8uTJmDt3rlqd6dGjh3raSB6vvviQJ5Bq1KiBUaNGoW/fvti4cSPkdtO7776Le++9F7LCI3tbAofAjdyGkv0y8ji3lT0vOoPM7PX78ejn63DbtaXxTtfrbCptv7hpF19RwLQxmTYeEz0ycUwEGfvXUz/VIMjYdEtu7QwcOFDdBkpMTETr1q3VLSIBjylTpqi9LqdPn05rdfHixXjyySfVyo28O0ZuLfXv3z/DXq3cWrq4os4g8+b8LXh74VY83aoqHmtxlU2l7RfnJGlfM7dr0CO3FQ+uP9N8IsgElwd+qUWQ8YtTl4lTZ5DpP2Ut5mw4ENaX4KWXxbSLb6T9ZuzXU5F5p79zBBn9PQolQoJMKOppUFdnkLn1zSXYcug0Fj7dFFWK5wu7WpxQwi5xyB3Qo5AldKUB03wiyLiSNp51QpDxTHpnOtYVZM6lnMfVQ+aqQW76Z5uwvc2XKzLO5JFbrZg2QZq4ambimAgybp3h3vRDkPFGd8d61RVkth0+jZZvLEG1K/Jj3pNNHBtvZg1xknRF5pA6oUchyQoyzDgAACAASURBVOdaZdN8Isi4ljqedESQ8UR25zrVFWTmbjyAvp+tRfvapfDefVk/Ru6EIqZdfCPtN2MncsCLNph3Xqhur0+CjD29/FaaIOM3xy6KV1eQGbNwK0bP34IBLa/CgJZVXVGZE4orMofUCT0KST7XKpvmE0HGtdTxpCOCjCeyO9epriDz+Bfr8O3v+9VqjKzKuHGYdvHliowbWRN6H8y70DUMdwsEmXAr7G37BBlv9Q+5d11Bpu3bS7HpwEn88GQTVL0if8jjtNIAJxQrKnlbhh55q7/V3k3ziSBj1Xl/liPI+NO3tKh1BJm4M+dQ/18LkHI+VT2xFM4vXqe3z7SLL1dk/HFyMu/094kgo79HoURIkAlFPQ3q6ggyg75aj/+s3otWV1+BDx+83jWVOKG4JnXQHdGjoKVztaJpPhFkXE0f1zsjyLguubMd6gYyq3Yexz3jVyB3dHbMf6oJyhbO4+yAM2nNtIsvV2RcS52QOmLehSSfK5UJMq7I7FknBBnPpHemY51AJjnlvPra9fYj8RjcvgZ631zZmUFabIUTikWhPCxGjzwU30bXpvlEkLFhvg+LEmR8aFr6kHUCmfX7TuC2d5ejcvG8+GFAE1fe5pteC9MuvlyR8cfJybzT3yeCjP4ehRIhQSYU9TSoqxPIfPbLbgyeuRH3NyyPEZ2ucV0dTiiuS267Q3pkWzJPKpjmE0HGkzRyrVOCjGtSh6cjnUAmsMn31TuvQZcG5cMz4ExaNe3iyxUZ11MoqA6Zd0HJ5molgoyrcrveGUHGdcmd7VAnkGn/zlL8sf8kZj/WGLXKFHR2oBZa44RiQSSPi9Ajjw2w2L1pPhFkLBrv02IEGZ8aFwhbF5BJTE5BraHzkA3ZsPHl1q69Oya9faZdfLki44+Tk3mnv08EGf09CiVCgkwo6mlQVxeQCWz0vbZsQXzzaGNPlOGE4onstjqlR7bk8qywaT4RZDxLJVc6Jsi4InP4OtEFZKas3I0XZ3i30dfE1QsTx2TaBGmiRyaOiSATvjlIh5YJMjq4EEIMuoDM81+vxxer9sKrjb4mXnxNHBNBJoST3cWqpvlEkHExeTzoiiDjgehOdqkLyHQYsxQb//Juo6+Jk76JYzJtgjTRIxPHRJBxctbRry2CjH6e2IpIB5DRYaOviRdfE8dEkLF1entW2DSfCDKepZIrHRNkXJE5fJ3oADK/7T2BO95bjtplC+Jbjzb6mjjpmzgm0yZIEz0ycUwEmfDNQTq0TJDRwYUQYtABZN5btA2j5m1Gr8aV8FKHq0MYTWhVOUmGpp8btemRGyqH3odpPhFkQs8JnVsgyOjsjoXYdACZ+z78BT9vP4ZP/lEfzaqVsBB1eIqYdvGNtN+Mw5MV4W+VeRd+jUPtgSATqoJ61yfI6O1PltF5DTJnk1Jw7cs/qDh/G9oKeXLmyDLmcBXghBIuZZ1rlx45p2U4WzLNJ4JMOLPF+7YJMt57EFIEXoPMki1H0P3jVWhUuQj+0+eGkMYSamXTLr5ckQk1I9ypz7xzR+dQeiHIhKKe/nUJMvp7lGmEXoPMv77bhA9+2oFnbq2KR5tf5amanFA8ld9S5/TIkkyeFzLNJ4KM5ykV1gAIMmGVN/yNew0ybd9eik0HTmJm/5tQp1yh8A84kx5Mu/hyRcbTdLLcOfPOslSeFSTIeCa9Kx0TZFyROXydeAkyR04lov6IBSiYOxprX2qF7FHZwjdQCy1zQrEgksdF6JHHBljs3jSfCDIWjfdpMYKMT40LhO0lyEz/dR+emfY72tYqibHd6nmupGkXX67IeJ5SlgJg3lmSydNCBBlP5Q975wSZsEsc3g68BJken6zC4s1H8HaXOri9TpnwDtRC65xQLIjkcRF65LEBFrs3zSeCjEXjfVqMIONT47xekTken4QGIxYgR/Zs+HVwK+TN5d1j1wEtTLv4ckXGHycn805/nwgy+nsUSoQEmVDU06CuVysyn6/cgxdmbED7a0rhvfvraqAEwAlFCxsyDYIe6e9RpAF0KNdQf7hpfpQEGZ97HMpJGMqk0vWDX7BixzGM61YXbWqV0kLFUMajxQAyCMK0MZk2HhMnfRPHxBUZXa9wzsRFkHFGR89a8QJkDp9MQMORC5E3Zw6sGdwSMdHZPRt/+o45SWphA1dk9LchywhNO5cIMlla7usCBBlf2wd4ATKTV+zCS9/8gU7XlcGb99bRRkHTLr6R9puxNolkMxDmnU3BPChOkPFAdBe7JMi4KHY4uvICZHpPXIMFmw7hvfvqon1tPW4rmTjpmzgmTvrhuAo436ZpPhFknM8RnVokyOjkRhCxuA0y51LO47p/zkd8UjLWDm6FwnlzBhF1eKqYdvElyIQnT5xulXnntKLOt0eQcV5TnVokyOjkRhCxuA0ya3YdR+dxK1C7bEF8+2jjICIOXxVOKOHT1qmW6ZFTSoa3HdN8IsiEN1+8bp0g47UDIfbvNsi8OX8L3l64FY/cUgXPtakeYvTOVjft4ssVGWfzI1ytMe/Cpaxz7RJknNNSx5YIMjq6YiMmt0HmrrE/49fdsfj8oYa4sUoxG5GGvygnlPBrHGoP9ChUBd2pb5pPBBl38sarXggyXinvUL9ugszJhHNqf0zO7FH4bWgr5Mqhx2PXASlNu/hyRcahkyTMzTDvwiywA80TZBwQUeMmCDIam2MlNDdBZt4fB/Hw5F/RtGpxTOzZwEp4rpbhhOKq3EF1Ro+Cks31Sqb5RJBxPYVc7ZAg46rcznfmJsi8NHMjJv+yG4Pb10Dvmys7P5gQWzTt4ssVmRATwqXqzDuXhA6hG4JMCOL5oCpBxqZJKSkpGDRoED799FMkJCSgTZs2GDduHIoWLZphS4cPH8azzz6L2bNnq5fXVa5cGd999x1Kly6NLVu24IUXXsCKFStw8uRJlC9fHk8++SR69+5tOSo3QabZ64ux82g85g64GdVLFrAco1sFOaG4pXTw/dCj4LVzs6ZpPhFk3Mwe9/siyNjUfMSIEZg4cSLmzZuHwoULo3v37mkfK7y4KQGd+vXro1GjRhg5ciSKFCmCTZs2oVy5cihQoABWrlyJNWvWoFOnTihVqhSWLl2Kjh07YtKkSbj99tstReYWyOyLPYPGry1C8fy5sOqFFsiWLZul+NwsZNrFlysybmZP8H0x74LXzq2aBBm3lPamH4KMTd0rVKiAIUOGoFevXqrm5s2bUb16dezduxdly5a9oLXx48dj+PDh2LFjB6Kjoy31JFBTqVIlvPHGG5bKuwUy/1m1B4O+3qDdZwnSi8QJxVLKeFqIHnkqv+XOTfOJIGPZel8WJMjYsC0uLg6FChXCunXrUKfO398Yyps3L6ZNm4Z27dpd0FqXLl0QGxurbhnNmDEDxYoVQ79+/fDEE09k2Gt8fDyuvPJKvPrqq2qlJ6NDbm3JSRk4BGSkf1n9sQpLgbrSzpw5c9C+fXtERUVlqsRjX6zDnA0H8Xrn2rizbhkbqrlX1M543IsqtJ5MG5Np4xF3OabQctyN2pl5JNfQmJgYJCUl2b6GuhE7+8haAYJM1hqllZBVF4ESWWGRVZPAUaZMGYwePRoCLumPli1bYuHChXjrrbcUwKxfv17tqRkzZgy6du16Qdnk5GR07twZJ06cwIIFC5AjR44MIxs2bBhefvnlS342ffr0y9axMcQMi55PBV5ckx1nkrPhn/WSUVCfrxKEOjTWpwJUIMIVCFx7CTL+TQSCjA3vBDJkX4zVFRm5TbR69Wrs27cvrZcBAwZg//79mDp1atq/yQkkEHTkyBG1ETh//vyXjcqLFZn1++Jwx/s/o2qJfGqjr64HfzPW1Zm/46JH+ntk4ioTV2T8kXfBRkmQsamc7JEZOnQoevbsqWrKk0fVqlXLcI+MrJxMmDBB/SxwCMgcOHAAX375pfqns2fP4s4771TLmt9++626TWTncGOPzHuLtmHUvM3oeVMlDOl4tZ3wXC1r2n39wIQya9YstQk8q9t/roodZGf0KEjhXK5mmk/cI+NyArncHUHGpuDy1NLkyZMxd+5ctTrTo0cP9Vi1PF598bF7927UqFEDo0aNQt++fbFx40bI7aZ3330X9957L06fPo0OHTogd+7cag+N3Ke1e7gBMoHPEnz6j/q4pVoJuyG6Vt60iy9BxrXUCakj5l1I8rlSmSDjisyedUKQsSm93NoZOHCgeo9MYmIiWrduDXk6Sd4jM2XKFDz88MMKUALH4sWL1bthZOVG3h0jKzL9+/dXP5bHuAWEBGTS/7bdrVs39W4aK0e4QebwyQQ0+NdC5M+VA2teaqndZwnSa8QJxUrGeFuGHnmrv9XeTfOJIGPVeX+WI8j407e0qMMNMpNX7MJL3/yBO+qUxltdrtNaLdMuvlyR0Trd0oJj3unvE0FGf49CiZAgE4p6GtQNN8jcP+EXLN92DOO61UWbWqU0GPHlQ+CEorU9Kjh6pL9HJvpEkPFH3gUbJUEmWOU0qRdOkImNT8L1IxYgOns2rHvpVuTOqdfXri+2gJOkJkmZSRj0SH+PCDL+8IhR/q0AQcbn2RBOkJm6Zi+em74ebWqWxLgH6mmvFCdJ7S3iioz+Fhm5csYVGZ8kXpBhEmSCFE6XauEEmYcmrcH8Pw/h7S51cHsdPd/mm94HgowuWcnbf/o7kXmEpp1LBBm/Z2Tm8RNkfO5vuEAmNTVVPa105FQiVr3YAiXy23803G1pTbv4RtoSv9v54lR/zDunlAxfOwSZ8GmrQ8sEGR1cCCGGcIHMwbgENBq5EKUKxmDF8y1CiNC9qpxQ3NM62J7oUbDKuVvPNJ8IMu7mj9u9EWTcVtzh/sIFMj/8cRB9Jv+KW6++Ah88eL3DUYenOdMuvlyRCU+eON0q885pRZ1vjyDjvKY6tUiQ0cmNIGIJF8iM/mEzxvy4Dc/cWhWPNr8qiMjcr8IJxX3N7fZIj+wq5k1503wiyHiTR271SpBxS+kw9RMukOn+8Sos2XIEE3s2QNOqxcMUvbPNmnbx5YqMs/kRrtaYd+FS1rl2CTLOaaljSwQZHV2xEVM4QEY2+tYbvgDH45Ow9qVWKJI3p42IvCvKCcU77a32TI+sKuVtOdN8Ish4m0/h7p0gE26Fw9x+OEBmX+wZNH5tEcoWzo1lA5uHeQTONW/axZcrMs7lRjhbYt6FU11n2ibIOKOjrq0QZHR1xmJc4QCZ7zccQL8pa9HumpJ4/379X4QXkIoTisWk8bAYPfJQfBtdm+YTQcaG+T4sSpDxoWnpQw4HyLw2978Yu3g7Brapjn63VPGNQqZdfLki44/UY97p7xNBRn+PQomQIBOKehrUDQfIdJuwEsu2HcVnvRqi8VXFNBiltRA4oVjTyctS9MhL9a33bZpPBBnr3vuxJEHGj66li9lpkElKPo86//wBCedSsG7IrSiYO9o3Cpl28eWKjD9Sj3mnv08EGf09CiVCgkwo6mlQ12mQ+WXHMXT54BdcV74QZjxykwYjtB4CJxTrWnlVkh55pby9fk3ziSBjz3+/lSbI+M2xi+J1GmRGzfsv3lu0HY83vxJP3VrNV+qYdvHliow/0o95p79PBBn9PQolQoJMKOppUNdpkLn93WX4fV8cpvW9AfUrFtFghNZD4IRiXSuvStIjr5S3169pPhFk7Pnvt9IEGb85FsYVmdj4JNQdPh95c+bAuiGtEJ09ylfqmHbx5YqMP9KPeae/TwQZ/T0KJUKCTCjqaVDXyRWZOesPoP/na9GyxhWY0N0fH4pMbwEnFA0SMosQ6JH+HkUaQIdyDfWHm+ZHSZDxucehnIQXTyqDvlqP/6zei5dvq4nuN1b0nTKcJPW3jB7p7xFBxh8eMcq/FSDI+DwbnASZxq/9iH2xZ/Hj001RuXg+3ynDSVJ/y+iR/h4RZPzhEaMkyBiTA06BTOyZc+pDkUXz5sSawS2RLVs232nESVJ/y+iR/h4RZPzhEaMkyBiTA06BzNJtx9D941VoWrU4JvZs4Et9OEnqbxs90t8jgow/PGKUBBljcsApkBm7ZAdGzduM/s2q4NnW1X2pDydJ/W2jR/p7RJDxh0eMkiBjTA44BTKPfrEO3204iLH310Xba0r5Uh9OkvrbRo/094gg4w+PGCVBxpgccApkbnl9CfYcP4OlzzVDuSJ5fKkPJ0n9baNH+ntEkPGHR4ySIGNMDjgBMk1atsF1ryxQH4j8bUgrX270NfHia+KYCDL+uPSY5hNfiOePvAs2Sj5+HaxymtRzAmSKXn0jun20CjddWRRTejfSZGT2wzDt4kuQsZ8DXtRg3nmhur0+CTL29PJbaYKM3xy7KF4nQOZgoZoY+f1mPNykMp5vV8O3inBC0d86eqS/R5EG0KFcQ/3hpvlREmR87nEoJ2FgUlkQXw6z1h/AO12vw23XlvatIpwk9beOHunvEUHGHx4xyr8VIMj4PBtCBZlvvp2Ft7cWwK5jZ3z7Rt+AhZwk9U9meqS/RwQZf3jEKAkyxuRAsCATd+Yc/rN6Nz748b84lpgN+XPlwO9Db0VUlP/e6EuQ8U86E2T84ZVpPnGPjD/yLtgouSITrHKa1AsWZH7fewK3v7dcjaLqFfnw9K3V0LpmSU1GFVwYpl18I+034+Bc974W8857D7KKgCCTlUL+/jlBxt/+IViQkWG/+v0mRB/ZggH3d0D27Nl9rgTACUV/C+mR/h5FGkCHcg31h5vmR0mQ8bnHoZyEpk0qpo0n0iYUv56KzDv9neOKjP4ehRIhQSYU9TSoS5D52wROKBokZBYh0CP9PYo0gA7lGuoPN82PkiDjc49DOQlNm1RMG0+kTSh+PRWZd/o7xxUZ/T0KJUKCTCjqaVCXIMMVGQ3S0HIInPQtS+VpQdN8Ish4mk5h75wgE3aJw9sBQYYgE94Mc7Z10yZIE1fNTBwTQcbZ81i31ggyujliMx6CDEHGZsp4Wpwg46n8ljs3zSeCjGXrfVmQIONL2/4OmiBDkPFTCps2QZq4emHimAgyfrpK2I+VIGNfM61qEGQIMlolZBbBEGT84ZZpPhFk/JF3wUZJkAlWOU3qEWQIMpqkoqUwTJsgTVy9MHFMBBlLp6dvCxFkfGvd/wInyBBk/JTCBBl/uGWaTwQZf+RdsFESZIJVTpN6SUlJyJUrF+Lj4xEdHW0rKjm5Z8+ejQ4dOiAqKspWXR0LmzaewG/G9EjHbLsQoE3yKNLyTn4ZzJs3LxITE5EzZ069k43RZagAQcbniXHmzBl1EvKgAlSAClCB4BWQXwbz5MkTfAOs6ZkCBBnPpHemY1mFSEhIQI4cOZAtWzZbjQZ+EwlmNcdWRy4VNm08IptpYzJtPCZ6ZOKYMsu71NRUJCcnIyYmxoiVaZcut1p1Q5DRyg53gwllf427kVrrzbTxBCYUWe6WW4h2bx1aU83dUvTIXb2D7c00n0wbT7C+mlqPIGOqsxbGZdrJbdp4CDIWkliDIsw7DUzIIgQTPdJfdfciJMi4p7V2PZl2cps2HoKMdqdMhgEx7/T3yUSP9FfdvQgJMu5prV1PKSkpeOWVV/DSSy8he/bs2sVnNyDTxiPjN21Mpo3HRI9MHJOJeWf3+mhyeYKMye5ybFSAClABKkAFDFeAIGO4wRweFaACVIAKUAGTFSDImOwux0YFqAAVoAJUwHAFCDKGG8zhUQEqQAWoABUwWQGCjMnuZjI22fw2aNAgfPrpp+qFem3atMG4ceNQtGhR7RUZOHCg+rTCnj17UKBAAbRr1w6vvfYaihQpomKXMfXs2fOCt3R27NgRX3zxhbZj69GjB6ZMmaI+NxE4/v3vf+ORRx5J+/ukSZPw8ssv48CBA6hdu7byq06dOlqOqWbNmti9e3dabJJvkme//vorTp48iWbNml3wRmoZz88//6zVWP7zn//gvffew++//w55g7a8NC39MXfuXDz99NPYsWMHqlSpgrfffhstWrRIK7Jt2zb07dsXK1asQOHChfHMM89gwIABno4xszF99913eP3119V45UWb11xzDUaMGIGbb745LWZ56Wbu3LkveHHcX3/9hYIFC3oyrszGs3jx4izzTEePPBHS550SZHxuYLDhywVq4sSJmDdvnrrIdu/eXV28Zs2aFWyTrtV74YUXcPfdd6NWrVqIjY1Ft27d1KQ4Y8aMNJAZPnw45CLll0NARt7OPGHChAxDXrZsGVq3bo1vvvlGTSyjR4/GmDFjsHXrVuTLl0/7Yb744ouYOXMm/vjjD8gE07Jly0vAQLdByLlx/PhxnD17Fn369LkgXoEXyb8PP/xQ5aJMqAKdmzZtQrly5dTTZvLzVq1a4dVXX8Wff/6pflkYP3487rrrLs+GmtmYBKTlFf3NmzdX55OAsvyys3nzZpQpU0bFLCCzdOlSNG7c2LMxpO84s/FklWe6eqSFsD4LgiDjM8OcCrdChQoYMmQIevXqpZqUi1X16tWxd+9elC1b1qluXGlHJvd//OMfatKRQ1ZkTAOZAGhOnjxZjVGgUyZMWbW5//77XdE52E5kJUNiff755/H444/7BmQC481oQhw6dCh+/PFHNakHjhtuuEF9gFWgbdGiRWjfvj0OHz6cBpoy/jVr1mD+/PnBSulYvawm+UBH8kuO/MJz2223aQkymXmU1Rh198gxsyOgIYJMBJh88RDj4uJQqFAhrFu37oJbE/Jb2LRp09StGj8dMjlu2LBBTR4BkHn44YfVSpO81v+mm27CyJEjUalSJW2HJSsyAmTyG2+xYsVw++23QybLwGqL3EKSMulvTchEKbdwBGZ0PqZPn44HH3wQ+/fvV3kXWPIXYJYXldWrVw//+te/cO2112o5jIwmxDvuuAMVK1bEW2+9lRZz//79ceTIEUydOlX9uwD1b7/9lvZzObekjMCN10dWk7zEt3btWtSvX1+t+lWuXDkNZEqWLKl8k9tpcpv3zjvv9Ho4GcJxVnmmu0eei+qjAAgyPjLLqVBl1aV8+fLq3n76yV2Wj+WWRZcuXZzqKuztfPnll3jooYfUb8aBiVDGJasAV155pZo0ZHlcbs3IvX9dvxQue0dkYi9evLi6PSErTDJRBPb1yP8PHjxY/XvgkJWY/Pnzq1sAOh9ye0XG9sknn6gwDx48iEOHDikIO336tNrf9MEHHygYLV26tHZDyWjSl70wcntF9iwFDlmJER9l74y8aHLBggVYsmRJ2s9lJUb2asleIa+PrEBGPJLxybVAVjcDx8KFC9UvBnIIeAtcyy1duW3m5ZHReLLKM9098lJPv/VNkPGbYw7Ee+LECbVa4fcVGZnk5Tdc2XvRpEmTyyojvz3KZkTZ/5N+M6YDUoatieXLl+OWW25RE71sAPbrisz27dtx1VVXqQ2vDRs2vKxeUkaAM3CrM2zCBtFwpK3I7Nu3T+1hEjhJv+KUkXTyS4SAWeCWZxDyOlIlKzALdJI+z7gi44j0WjRCkNHCBveDkD0ycutCnu6RY8uWLahWrZpv9sh89NFHeO655zBnzhw0atQoUwFldUZARn6DlAu0Hw6Z+AXOTp06hZiYGLUZOzU1FfLkkhzy/7LvRFYzdN4jIx7JSoRAc2aH5N6zzz6L3r17a2fP5fbIyK3Mn376KS3eG2+8Ue2LSb9HRm41BVYBZZP66tWrtd4jI6uZco7cc889apNyVofcwo2Pj8dnn32WVdGw/twqyKTPs8AeGV09CqtghjVOkDHMUKvDkaeW5LcoWQaX1RlZIpaVC3msWffjnXfewT//+U/1xJXsr7j4ELiR20xyq0yeapJNljJOeWJG1yd85KkX+Q1Y9pDIngQBl1KlSuGrr75Sw5NbY/Lzb7/9Vi3tv/nmm+pxX52fWkpKSlK3lGQJXya8wCGbZOXWpuy7kMea5ZFf+e1Ybi0JnOlyyFMtck4IrMi+MVkdk0NWyGTCl8eTP/74Y/UUktzilEet5ekkGVvgiRh50kz2Z8ntQvn/sWPHonPnzp4NMbMxyYZ/gRhZFUt/yywQ7MaNG5Vfsjooe7nkPLvvvvvUE1uBzcBuDyyz8QioZJZnunrktoYm9EeQMcHFIMYgJ7Fs1JMNiYmJieoiK4+G+uE9MnIRlUeV079zRSQITDTym708SiqbmuU9MzLxy2bSqlWrBqGUO1XkNtL69euVFyVKlECnTp0wbNgwFX/gkNUY+bf075G57rrr3AkwiF5kgpNbDxJveoAUCBNwOXr0qFqtqFu3roId2Viq0yHnRvo9SYHYdu7cqTb6XvweGRlT+hU/efxfAC79e2SefPJJT4eY2ZgEXuTnF+8jk+uCrPoJGDz66KPYtWsXcubMqfZwybtxvNxTl9l4ZO9OVnmmo0eeJohPOyfI+NQ4hk0FqAAVoAJUgAoABBlmARWgAlSAClABKuBbBQgyvrWOgVMBKkAFqAAVoAIEGeYAFaACVIAKUAEq4FsFCDK+tY6BUwEqQAWoABWgAgQZ5gAVoAJUgApQASrgWwUIMr61joFTASpABagAFaACBBnmABWgAlSAClABKuBbBQgyvrWOgVMBKkAFqAAVoAIEGeYAFTBEAfnMhLzxeMKECZ6OSD5N8MADD+CHH35A9uzZ1Rt8rRzyin+J/91337VSnGWoABWgAkoBggwTgQoYooAuICNfJZcPJMq3eS5+3X1AannF//Dhw9GtWzct1Lf60UEtgmUQVIAKXKAAQYYJQQUMUcBpkJEPJkZHR9tWRwBFwGDBggWXrUuQsS0rK1ABKnAZBQgyTA0qEAYFZKLu06cPFi5ciJUrV6JChQoYN24cbr75ZtVbRtBx5ZVXYvDgwepn8jE8AQL5SJ98HVo+gCkfIJQvecuHGAUS5OvYH330ERo3bpzWpsBHVFQUvvnmGxQvXhwvvfSSai9wLF26VLUhX2mWr54/8sgjeOqpp9TXjAOrEtL3kCFDcOjQIcTHx1+ijnwBWdr4+uuvcfbsWdW/fJFc+VK6SAAACPFJREFUvjQst4fki9Dnz59HTEyM+tKztJf+6Nixo/pysnx4UG4l3Xjjjeo21MWaSExym+mTTz5RX4+WL5rLV6anT5+ON954Q8Um/ckHQQOHrAI9/fTT+PXXX5EnTx71sUP5UroAmdzyEj1nzpyJhIQElCxZUtWV/uUDiPJvgRWk9957T32BfM+ePUqf5cuXqy4k9tGjRyN//vzq7xKjfARTxrh9+3Zcf/31+PDDDyFeyiEfzpSPMe7bt0/F07Zt20v0CEP6sUkqEFEKEGQiym4O1i0FBGQCQHH11VerL41/9dVXkC8nWwUZARapJ1Dxxx9/oGHDhrjmmmswZswY9f8vvviianPr1q1pbcpXv2Xily8S//jjj7jtttvUf2WyljYaNWqEzz77DB06dFD1ZGKVifbBBx9UINOsWTN07doVY8eOVZO/TL4XHwJUv/32mwKZQoUK4YknnsDq1auxdu1atSdGvtC9bNky2ysyGYFMgwYNFLgUKVIE7du3V0AgYxNAExgTHSRuGd/hw4dRo0YNBSfy1eojR47g9ttvVxqIhh988IEal0CgfOV97969OHXqFMSfjG4tCdjUqlUL9913nwI3+buAkQCQwFoAZKTPb7/9FmXKlFHQs2TJEmzYsEF9ybxgwYKYN28emjdvrsBLNArArFu5yH6ogOkKEGRMd5jj80QBARlZ7XjuuedU/5s3b0b16tXVxleZRK2syDz++OOIjY1VcCCHTOr169eHrBbIIRN5zZo1ceLECTVhSpuyKiCrLoFDJl5ZZZBJXFYjZDUlMAlLGVld+P7779XkHgAZWYUoV65chrrJSou0JxN3q1atVJnTp08r0JAJ/IYbbnAUZKZOnYq7775b9fP+++9j0KBBl2giYxSYkpWr7777ToFb4BDQExjctm2bWgkZMWKEGr/EKatBgSMjkBGAkrqiaeCQlR6BJtFRfJEVGdlc3atXL1VEYEVWuqS9OnXqoFixYiougS/RiAcVoALOK0CQcV5TtkgFcPEeEFlJEDiQFRn5mRWQkVtLMgEHjltuuQUtW7ZUt5/k2LVrFypVqqRWFsqWLavaTElJweTJk9PqSFlZBZAJXlY0ZJLPlStX2s8FTCQuWa2RybdFixaqjcsdcrtJViQkLrkdEzikf7ndc8899zgKMgJlgVtngdttl9Okf//+Cipy586dFldqaqoaj8BWcnKyArdp06ap1SgZ67///W91GygjkBk1apTatHzxhmVZmRG4kRUYARmBQGkrIy2kXdFFxlG5cmV120tWeHhQASrgnAIEGee0ZEtUIE2BrEBGVkeOHTsGecJHDpls5TaN3DZKv0fGLshktiIjE70cgRWdi+2y8uSOgI/cbpo9e7aCKjmCWZGRSV32rqR/aimjW0t2QEbAQ8Yg+2+yOmQVSzyQ1aeffvpJ/ZHbPwI7gUOAR26TCeRd7shsRUZWbgKH+CurWHfddZeCqPQQmFWs/DkVoAKZK0CQYYZQgTAokBXIyOqC3HaSjcClS5dWk7qsDshG0VBARvbITJo0Sd2OkUld9sLIioGsashG2KZNm6pbLG3atFGrCVu2bFF7SeTfrYCMSCWbmGUPiNy2Efh68sknsWLFCqxbt87yHhmZ5OXWlOzPCRyhgszBgwfVhuCRI0eqVQ/ZTCyrVjJGGa+sRkm8ss9IgExu3QlUyL9LmWrVqmHHjh1qlUsOuX0kt4ckrsceewz58uXD/v37sWrVKnTq1EmVEQ3l9p5srhYfn3nmGdWeaC23EWWvkIyzQIECWLRokVq5kT4kP3hQASrgjAIEGWd0ZCtU4AIFsgIZebqoX79+CgZkhUP2YsiTPxc/tWR3RSb9U0uyF0c2xfbs2TMtNgEO6eP3339Xk7ncVhGgkqeLrIKM7AORvSqy2Vc2tAqUSOyBydnKZl+51SVwIKtSsl9F9umECjIySNk3JLEJbMgTVRKTbE6W/Uqy+vXKK6+oVRiBHNlzJCtgV111ldJHVqxkT45oKP8uL/WT23ay0VcgRDYGC6zce++9aQAWeGpJNlgLoNStW1fBaNWqVXHgwAG1OVgAT1Z65BaetCXt8qACVMA5BQgyzmnJlqgAFYgwBQRk0t/+irDhc7hUQAsFCDJa2MAgqAAV8KMCBBk/usaYTVOAIGOaoxwPFaACrilAkHFNanZEBS6rAEGGyUEFqAAVoAJUgAr4VgGCjG+tY+BU4P/arYMaAAAYCGH+XaODpA6W7h4QIECAAAEhYwMECBAgQIDAVkDIbF/ncAIECBAgQEDI2AABAgQIECCwFRAy29c5nAABAgQIEBAyNkCAAAECBAhsBYTM9nUOJ0CAAAECBISMDRAgQIAAAQJbASGzfZ3DCRAgQIAAASFjAwQIECBAgMBWQMhsX+dwAgQIECBAQMjYAAECBAgQILAVEDLb1zmcAAECBAgQEDI2QIAAAQIECGwFhMz2dQ4nQIAAAQIEhIwNECBAgAABAlsBIbN9ncMJECBAgAABIWMDBAgQIECAwFZAyGxf53ACBAgQIEBAyNgAAQIECBAgsBUQMtvXOZwAAQIECBAQMjZAgAABAgQIbAWEzPZ1DidAgAABAgSEjA0QIECAAAECWwEhs32dwwkQIECAAAEhYwMECBAgQIDAVkDIbF/ncAIECBAgQEDI2AABAgQIECCwFRAy29c5nAABAgQIEBAyNkCAAAECBAhsBYTM9nUOJ0CAAAECBISMDRAgQIAAAQJbASGzfZ3DCRAgQIAAASFjAwQIECBAgMBWQMhsX+dwAgQIECBAQMjYAAECBAgQILAVEDLb1zmcAAECBAgQEDI2QIAAAQIECGwFhMz2dQ4nQIAAAQIEhIwNECBAgAABAlsBIbN9ncMJECBAgAABIWMDBAgQIECAwFZAyGxf53ACBAgQIEBAyNgAAQIECBAgsBUQMtvXOZwAAQIECBAQMjZAgAABAgQIbAWEzPZ1DidAgAABAgSEjA0QIECAAAECWwEhs32dwwkQIECAAAEhYwMECBAgQIDAVkDIbF/ncAIECBAgQEDI2AABAgQIECCwFQgz3CnpN/91MwAAAABJRU5ErkJggg==\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f8d56896fd0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f8d080cc080>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.603      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 60         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 42         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00735421 |\n",
      "|    clip_fraction        | 0.382      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0661     |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0316    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.0042     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 80 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.603       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009233216 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0942      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0928      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.603       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039166592 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.4        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0824      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0367      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 63 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.607       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036538605 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.421      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0842      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0237      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.611       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028216159 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0536      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0157      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.612       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020301925 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.477       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.035       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0121      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.613       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016120002 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00947     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014462548 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0466      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00882     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011021134 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.711       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0519      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00863     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010327557 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0653      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00804     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.624       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009739417 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0634      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0075      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.625       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008182868 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0668      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00719     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.627        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075162947 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0643       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0069       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.631       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011902288 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.007       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.634       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011345756 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0568      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00695     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 62 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.636        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033960373 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.808        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0476       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00659      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.639        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068844557 |\n",
      "|    clip_fraction        | 0.331        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0316       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00662      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.642       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008942166 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0738      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00646     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.646       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007042864 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0398      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0065      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.646        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071283714 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0614       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00614      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.648        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011383683 |\n",
      "|    clip_fraction        | 0.318        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.041        |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00602      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.65         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047222553 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.822        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.064        |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00611      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008404577 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0056      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 66 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.655       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006806654 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0494      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0058      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 67 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.658      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 95         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00940643 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.823      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0835     |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0284    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00596    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 65 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008808491 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0658      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00569     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006496078 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0055      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006552848 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0056      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.665        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063127116 |\n",
      "|    clip_fraction        | 0.332        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0362       |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00544      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.665        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074937316 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0894       |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00528      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010952175 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0496      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00561     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071539045 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0321       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00527      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060836645 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0752       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00558      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066732853 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0426       |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00521      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004844174 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0515      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00542     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008579296 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0466      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00523     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041467724 |\n",
      "|    clip_fraction        | 0.331        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0525       |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00572      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 62 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.679      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00716002 |\n",
      "|    clip_fraction        | 0.332      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.844      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0616     |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0283    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00534    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008499099 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0552      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00513     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007848236 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0779      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00545     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012138143 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0624      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00488     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.682       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008143226 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0547      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00529     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007858997 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0454      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.005       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008023304 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0345      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00507     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009611433 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0559      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00521     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057073026 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0581       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00514      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025123388 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0502       |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00498      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003993976 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0533      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00509     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.689      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 93         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 27         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00682818 |\n",
      "|    clip_fraction        | 0.345      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.853      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0465     |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0292    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00499    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050835935 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0637       |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00499      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007778385 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.041       |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00485     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007981452 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0403      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00502     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003519124 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0711      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057195066 |\n",
      "|    clip_fraction        | 0.331        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0395       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00496      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007243344 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0945      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00482     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022376657 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0798       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00474      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005947006 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0593      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0049      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066509186 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0372       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00473      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040112613 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0757       |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00495      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005996847 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0834      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00485     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014098388 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0428      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00455     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009627977 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00473     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.692      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 95         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00695827 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.873      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0739     |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0311    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00456    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007524231 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0534      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00463     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.693        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039437087 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0755       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00474      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.693        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063982815 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.048        |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0047       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.693       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009057549 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0438      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00482     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008185375 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0568      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00458     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009651616 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0047      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064718304 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0615       |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00481      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.694        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060895355 |\n",
      "|    clip_fraction        | 0.319        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0626       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0046       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063274354 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0334       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00473      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030025751 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0581       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00452      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044499934 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0522       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00445      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056927563 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.12         |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00473      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010114414 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0543      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00464     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007543704 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0536      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00445     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.695      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 94         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 27         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01147489 |\n",
      "|    clip_fraction        | 0.368      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.871      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0331     |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0307    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00449    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046399683 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0044       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 66 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006714797 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0834      |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00454     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008034607 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0712      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00469     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 67 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.694       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004049438 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0408      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00463     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.695      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 94         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 27         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00542098 |\n",
      "|    clip_fraction        | 0.351      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.874      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0495     |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0297    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00432    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 68 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007363835 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0477      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00465     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005573511 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0634      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055956007 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0598       |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00441      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073605357 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0541       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00434      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064196438 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0572       |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00433      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 65 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041481047 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0599       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00449      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007133956 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0489      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0041      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066481144 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0962       |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.0318      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00449      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 61 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006855884 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0346      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0043      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033981025 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0466       |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00421      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006065461 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0635      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00435     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007732308 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0506      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00436     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008697823 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0428      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00407     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008227495 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0727      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0043      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011406231 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0638      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00433     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007888702 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.055       |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075318543 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0537       |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00435      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 68 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067750392 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0574       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00407      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 65 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064496845 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0357       |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00409      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069506643 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0351       |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.032       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00393      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071276217 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0457       |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00414      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030363277 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0454       |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00387      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011044783 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.067       |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00409     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 61 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008734375 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0469      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00389     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009680266 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0538      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00409     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009797955 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0332      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00399     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009041068 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0903      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 61 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070595504 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0515       |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00405      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004602897 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.069       |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00401     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076729446 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.063        |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00397      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 94           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075923502 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0961       |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00413      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005852759 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0615      |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00395     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006155804 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0421      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030431538 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0499       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00404      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007221839 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0578      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003081727 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0468      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00393     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.699      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 98         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00749587 |\n",
      "|    clip_fraction        | 0.391      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.889      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0527     |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.032     |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0038     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005091116 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0365      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.004       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007171592 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.052       |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00376     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074973465 |\n",
      "|    clip_fraction        | 0.388        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0472       |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0325      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00384      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.699      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 96         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00963383 |\n",
      "|    clip_fraction        | 0.374      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0463     |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00392    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 63 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088912845 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0312       |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00386      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070697935 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0449       |\n",
      "|    n_updates            | 2500         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0039       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 5              |\n",
      "|    mean_reward          | 0.699          |\n",
      "| time/                   |                |\n",
      "|    fps                  | 96             |\n",
      "|    iterations           | 1              |\n",
      "|    time_elapsed         | 26             |\n",
      "|    total_timesteps      | 2560           |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00074237585 |\n",
      "|    clip_fraction        | 0.38           |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | 91.8           |\n",
      "|    explained_variance   | 0.894          |\n",
      "|    learning_rate        | 3e-06          |\n",
      "|    loss                 | 0.0571         |\n",
      "|    n_updates            | 2520           |\n",
      "|    policy_gradient_loss | -0.0303        |\n",
      "|    std                  | 0.055          |\n",
      "|    value_loss           | 0.0038         |\n",
      "--------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008366361 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.064       |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 92           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051849694 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0507       |\n",
      "|    n_updates            | 2560         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00378      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087815495 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0423       |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00385      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008262813 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0725      |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00377     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0084370645 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0683       |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00384      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 66 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 68           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059892177 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0591       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 65 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 67          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004036933 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00355     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 64 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 65          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008807972 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0814      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 62 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 65           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064522983 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0616       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00369      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 61 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 66           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055318684 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00349      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 65           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061871437 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0588       |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00347      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 65          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008664397 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0511      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00371     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 65 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 65           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036409348 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0497       |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00346      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 65 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 64           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053885607 |\n",
      "|    clip_fraction        | 0.38         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0532       |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00355      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008113941 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0615      |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 70 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008818892 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0597      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00359     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 61 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004387304 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0586      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 67 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 65           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066909045 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0742       |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00377      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 65 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006937775 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0665      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 64 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004267433 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0914      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00372     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[1.0],\n",
    "                                episode_limit_array=[75000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
