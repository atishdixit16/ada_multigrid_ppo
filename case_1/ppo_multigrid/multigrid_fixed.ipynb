{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_1_multigrid_fixed'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 3e-6,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.1,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[150,100,80], log_std_init=-2.9),\n",
    "                verbose = 1,\n",
    "                target_kl = 0.05,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/coarse_grid_functions.py:51: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1mFirst-class function type feature is experimental\u001b[0m\u001b[0m\n",
      "  for j in range(len(p_1)-1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f7c46c67be0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f7c4680e5f8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.59 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.594    |\n",
      "| time/              |          |\n",
      "|    fps             | 106      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.597       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015503553 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.236      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0926      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.598       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 259         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027701471 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.25       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.042       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.601      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 255        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03151405 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.308     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0617     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0244     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.604       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022714242 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0727      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0155      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019427568 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0616      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0115      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014360016 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0764      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00904     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 255         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013760949 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.038       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00822     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.611       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009902736 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0519      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0072      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.613       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008958871 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0713      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00705     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007303399 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0934      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0066      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009353575 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0451      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00661     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.621       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006472853 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0923      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00611     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.626        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046646623 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0553       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00591      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.63         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056614517 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0525       |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00583      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.633        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075015277 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0559       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00553      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009601568 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0705      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00538     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.639       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009925622 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0345      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007462728 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0642      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00497     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.645        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046749176 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0399       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00501      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.646        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 254          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052710087 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0601       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0049       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.649       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005696991 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0406      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00464     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007913312 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0556      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00486     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.653        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074240863 |\n",
      "|    clip_fraction        | 0.311        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0241      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0045       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 257         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005299613 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.654        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034764528 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0396       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00456      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.655        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046053766 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0441       |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0266      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00458      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.656       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005309331 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00422     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.657       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005764833 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0611      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 255         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006988463 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.062       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0041      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008306416 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0377      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.661     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 250       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0072301 |\n",
      "|    clip_fraction        | 0.351     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.878     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0676    |\n",
      "|    n_updates            | 620       |\n",
      "|    policy_gradient_loss | -0.0269   |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.00394   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006913173 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0791      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00377     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006545782 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0379      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008365149 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0378      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00402     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009714067 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00374     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.664        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053011654 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0696       |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006423101 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0973      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00365     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.666        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052858116 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0912       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0262      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00378      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007949164 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0531      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00374     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002187717 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0781      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006781551 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0703      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004051268 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0496      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00367     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 254          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062743486 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0691       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00359      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004979509 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0572      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058360607 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0565       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00355      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027345777 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.047        |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0258      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00338      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.673       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004990545 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0782      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00346     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 255          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046329917 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0614       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00341      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "seed 1: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f7c4680c080> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f7c468f89b0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004152441 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.065       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.684      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 194        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01260156 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.834      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0672     |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0301    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00495    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002791047 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0897      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00463     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 194          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0128581105 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0485       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00468      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008222347 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00465     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010010257 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0045      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 193          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058306186 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0693       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00439      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004788852 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0482      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006617397 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0411      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009856972 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0697      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00419     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068899454 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0587       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00411      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0100083705 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0429       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00414      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 192          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074029355 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0421       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00405      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006541741 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0782      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00419     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055289892 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0756       |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00385      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 193          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041852714 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0696       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00386      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 195          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052358597 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0282       |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070577264 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0954       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00403      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004865548 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.063       |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005265215 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0534      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.004       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007978404 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0304      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00392     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 193          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014226123 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0484       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00372      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007330939 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0881      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00388     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010227472 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0669      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00378     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007430819 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0605      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 193          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068831146 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0775       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00364      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061261654 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0497       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00372      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009201157 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0594      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00384     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009255168 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0368      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00384     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053725033 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.08         |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00376      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007555249 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0687      |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00409     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074279965 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0461       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0037       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003924498 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0521      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00373     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008955225 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.048       |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00372     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011991155 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00383     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007584804 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0719      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00389     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049439697 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0399       |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00372      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062008025 |\n",
      "|    clip_fraction        | 0.374        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0657       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00362      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006623539 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0792      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00384     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068919035 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.05         |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011865223 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0478      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00361     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076889307 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0425       |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00367      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 192          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050700186 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0708       |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00366      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007765481 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0718      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.687         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 190           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 13            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00096726714 |\n",
      "|    clip_fraction        | 0.357         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.894         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0576        |\n",
      "|    n_updates            | 1860          |\n",
      "|    policy_gradient_loss | -0.0295       |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.00372       |\n",
      "-------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 192          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063747168 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0698       |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00354      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008946439 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0684      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0038      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057938844 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0768       |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00331      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006534028 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00367     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "seed 1: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f7c46984208> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f7c4696cbe0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 63           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073551508 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0674       |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00399      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 69 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024288923 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.81         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0339       |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00599      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073745786 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0725       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00582      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043176236 |\n",
      "|    clip_fraction        | 0.374        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0444       |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.0316      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00579      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049214363 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.053        |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00507      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061264276 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0594       |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00553      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005292228 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0335      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00529     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008780321 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0495      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00553     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030741394 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0555       |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00562      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008698607 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0651      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00529     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008884805 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.048       |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00538     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011095941 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.108       |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00549     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072181164 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.049        |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.0316      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00524      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051781805 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.067        |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0056       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005156979 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0475      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00526     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008296031 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0988      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00517     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004943478 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0415      |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00527     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006272125 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0316      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00528     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007586348 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0511      |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00534     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008328292 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.061       |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00518     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007705507 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0496      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00535     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005536097 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.055       |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00563     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007413718 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0492      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00519     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008507863 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00542     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004147166 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0826      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00507     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 98         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00901342 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.85       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0469     |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0301    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0051     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 98         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00671452 |\n",
      "|    clip_fraction        | 0.374      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.843      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0538     |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.032     |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00529    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053327917 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0636       |\n",
      "|    n_updates            | 2500         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00513      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017665088 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0647       |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00518      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010129255 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.105       |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00499     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008541336 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0677      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00515     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006054881 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0496      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00488     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00717383 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.852      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0486     |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.032     |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00502    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009797215 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0395      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00513     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093501685 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0452       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00483      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009625217 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0598      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00509     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006147766 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0609      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00509     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007995928 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.053       |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00501     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008834446 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0464      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00468     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057306485 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0518       |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00493      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009552089 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.054       |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00474     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008026203 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.064       |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0048      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009642055 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.046       |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00464     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060814647 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0391       |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00484      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008824718 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0654      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00456     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038305582 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0531       |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00456      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011881039 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0833      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00467     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059150336 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0658       |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00448      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005275327 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0427      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0045      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f7c46968780> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f7c468c92b0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009514904 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0583      |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00473     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.601      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 243        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03261771 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.476     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0475     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0641     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.605       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034250923 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.18       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0708      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0378      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.606      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03983616 |\n",
      "|    clip_fraction        | 0.381      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.466     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0539     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0229     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.607       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029545229 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0502      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0144      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.614       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027530098 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0253      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0101      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.615      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 232        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01951104 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.683      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0783     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00806    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.619       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014926551 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0429      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00722     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.62       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 237        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01039381 |\n",
      "|    clip_fraction        | 0.336      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.799      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.049      |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0251    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00626    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.624     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 230       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0096928 |\n",
      "|    clip_fraction        | 0.339     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.81      |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0403    |\n",
      "|    n_updates            | 180       |\n",
      "|    policy_gradient_loss | -0.0264   |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00608   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.629       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010694569 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0714      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00589     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008935439 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00578     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.633       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010327518 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0663      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00572     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006420663 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00511     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.637       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 226         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009483946 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0614      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00522     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010879433 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00512     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.641        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 228          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070581674 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0589       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00514      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010783603 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00473     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.645        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 234          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082113175 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0502       |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00519      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.647       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008283702 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0047      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.65         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 232          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073689194 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0674       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00484      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.652      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 236        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00844332 |\n",
      "|    clip_fraction        | 0.34       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.857      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.066      |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00475    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.654     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 238       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0096127 |\n",
      "|    clip_fraction        | 0.346     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.862     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0901    |\n",
      "|    n_updates            | 440       |\n",
      "|    policy_gradient_loss | -0.027    |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00468   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.656        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 241          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029226064 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0687       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0245      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00458      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007250157 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.065       |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0044      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 240          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056521357 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0729       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00453      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009899834 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0928      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00457     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006917706 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.041       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00445     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.662       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005110082 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0042      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.662        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 234          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048077675 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0552       |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00413      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.663        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 231          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069179265 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0559       |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00414      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.663         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 238           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059212744 |\n",
      "|    clip_fraction        | 0.337         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.871         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0571        |\n",
      "|    n_updates            | 620           |\n",
      "|    policy_gradient_loss | -0.0272       |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.00419       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007168728 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0417      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00414     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007922521 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.063       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008942589 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0364      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004092327 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0511      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005412662 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0543      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00367     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006316277 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0589      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0038      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 237          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059427978 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0903       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00377      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005376813 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0709      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006884399 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.078       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0037      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005267979 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0539      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 235          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049629183 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0511       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00367      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 234          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050691008 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0521       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00351      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002076575 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0928      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 232          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044919522 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0481       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00381      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007311416 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0612      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0037      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 237          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070066154 |\n",
      "|    clip_fraction        | 0.329        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0457       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0256      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0035       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 240          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053995075 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0741       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00342      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "seed 2: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f7c470d1b00> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f7c468f85c0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005432823 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.055       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014175693 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0643      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00527     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005531612 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0267      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00462     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003973526 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0481      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00452     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006879178 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0497      |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00442     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003920513 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0609      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00457     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003918928 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0544      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 184          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044926764 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.072        |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0046       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004346752 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0951      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00452     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005609378 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00427     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008024106 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0535      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00421     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 184          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062348903 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0466       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00433      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039530573 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0375       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00417      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007655156 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0583      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00427     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008820432 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0281      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00443     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005879608 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0368      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011078197 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.059       |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00395     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061403424 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0535       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00421      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074849636 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0552       |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00405      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009063688 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0387      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00383     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032162995 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0556       |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00388      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053257733 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0943       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00382      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028727292 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0611       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00376      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065648435 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0824       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00385      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007947365 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0655      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00366     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 185          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069647045 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0528       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00385      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090523865 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0619       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00389      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006924033 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.033       |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00383     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069529624 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0533       |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00386      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006476271 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0724      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 185          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045274138 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0427       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00371      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045120567 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0499       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00363      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.69          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 183           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 13            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046337844 |\n",
      "|    clip_fraction        | 0.36          |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.893         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0397        |\n",
      "|    n_updates            | 1620          |\n",
      "|    policy_gradient_loss | -0.0289       |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.00377       |\n",
      "-------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009776312 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0644      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0037      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 185          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055813044 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0456       |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00369      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051915855 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0373       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00351      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007331255 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0375      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00371     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012266248 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0483      |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00371     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007964807 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0632      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00344     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006278828 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0822      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00348     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 187        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00638701 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.898      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0673     |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00351    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 183          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061170934 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0483       |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.0319      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00348      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011708203 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0534      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008814618 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0677      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00346     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 184          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036108494 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0607       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00338      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 185          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041496726 |\n",
      "|    clip_fraction        | 0.387        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0455       |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.0317      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00353      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009634569 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0467      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00366     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0089867115 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0455       |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00351      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010209605 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0993      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00351     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "seed 2: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f7c468f85c0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f7c140e0eb8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005984756 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00355     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 74 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045134844 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0321       |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00596      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040752054 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0603       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0318      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00588      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006940761 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0642      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00565     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062828227 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.832        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.103        |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00558      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011458245 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0488      |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0057      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077620326 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0648       |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00578      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024410635 |\n",
      "|    clip_fraction        | 0.336        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0372       |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0056       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035756498 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0665       |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0055       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008296949 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00562     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0084946845 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0658       |\n",
      "|    n_updates            | 2160         |\n",
      "|    policy_gradient_loss | -0.0319      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00573      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072871475 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0402       |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00604      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060093254 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0362       |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0056       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059189736 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.065        |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0058       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064280657 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.823        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0361       |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00578      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038301914 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0802       |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00533      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010969537 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.07        |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00557     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029029013 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0442       |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00567      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009670016 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0362      |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00582     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009669324 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0535      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00549     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00512304 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.828      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0398     |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00557    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072720917 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.837        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0409       |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00552      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008006093 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.06        |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00542     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065320404 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0618       |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0054       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004477772 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0561      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00536     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009423328 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0449      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00529     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049129934 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.053        |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00517      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004745397 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0412      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00537     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007858833 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0526      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00503     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056096166 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0878       |\n",
      "|    n_updates            | 2540         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00499      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008560568 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.038       |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00543     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009274319 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0725      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00515     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049616545 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.847        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0653       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00519      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006495282 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0569      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0048      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007197255 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0589      |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008834729 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.056       |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00549     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008672329 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0805      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003386253 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0694      |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00531     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008096198 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00496     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.698      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01112504 |\n",
      "|    clip_fraction        | 0.368      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.852      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0447     |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0308    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00501    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071435184 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0497       |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00519      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057926984 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0826       |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.0322      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0048       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010191971 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0529      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00468     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044134916 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0488       |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00479      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007895377 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0494      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00488     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009956067 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0876      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00475     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053568394 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0764       |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0048       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012000799 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.033        |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00494      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073684095 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0294       |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00502      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f7c140e0208> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f7c14092828>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.599        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 106          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033539473 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.083        |\n",
      "|    n_updates            | 2940         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00504      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008441448 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0678      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0827      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.603       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036776222 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.4        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0878      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0336      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.605       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037886694 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.371      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0884      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0203      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.609       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031634964 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.084       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0126      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.609       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023138948 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.575       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0328      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00905     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.612      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 243        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01902612 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.723      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.052      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00683    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.615       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014740577 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.072       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00615     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011999324 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0322      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00588     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009492916 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0525      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00543     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.623       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010967347 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0583      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00508     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.623       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007900062 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.089       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00504     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.623        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 240          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061360775 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0397       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.026       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00504      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.625       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011871862 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0628      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00499     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.626       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014109999 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0648      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.627      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 239        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00510236 |\n",
      "|    clip_fraction        | 0.334      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.866      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.042      |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00438    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.63         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060845404 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0552       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0245      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00456      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.633       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012400851 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0664      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00404     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.637       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 248         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012367246 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0336      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00426     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.639        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058953343 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0582       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0262      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00415      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.641       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003532982 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0329      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00401     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.641       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008809748 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0711      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.642        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064934194 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0512       |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00381      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.645       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 254         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008403584 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0407      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.646       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007718435 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0752      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00406     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.647        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0111936275 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.081        |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0245      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00384      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.649       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006145814 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0589      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005348435 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.059       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004923704 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0414      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00333     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.654        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023266436 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0618       |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00346      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.656       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011344736 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0548      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00362     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042720973 |\n",
      "|    clip_fraction        | 0.332        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0439       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0242      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00353      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 5              |\n",
      "|    mean_reward          | 0.659          |\n",
      "| time/                   |                |\n",
      "|    fps                  | 247            |\n",
      "|    iterations           | 1              |\n",
      "|    time_elapsed         | 10             |\n",
      "|    total_timesteps      | 2560           |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -0.00017822385 |\n",
      "|    clip_fraction        | 0.35           |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | 91.8           |\n",
      "|    explained_variance   | 0.9            |\n",
      "|    learning_rate        | 3e-06          |\n",
      "|    loss                 | 0.0868         |\n",
      "|    n_updates            | 640            |\n",
      "|    policy_gradient_loss | -0.0261        |\n",
      "|    std                  | 0.0551         |\n",
      "|    value_loss           | 0.00332        |\n",
      "--------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083399415 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0557       |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.661        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047372817 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0375       |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.0259      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00346      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.664        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036762594 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0498       |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00309      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.665        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028889328 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0542       |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0255      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00334      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.666        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064543905 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0357       |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.0256      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00329      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011148497 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00318     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008196855 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0698      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011829317 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069169314 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.056        |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00337      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005862546 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0452      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0032      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050961403 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0745       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00314      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 256          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058733253 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0381       |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00323      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017754287 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0585       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00326      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.67          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 250           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0002821833 |\n",
      "|    clip_fraction        | 0.363         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.912         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0664        |\n",
      "|    n_updates            | 920           |\n",
      "|    policy_gradient_loss | -0.028        |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.00301       |\n",
      "-------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072675883 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0342       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 254          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061362833 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0765       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00316      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "seed 3: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f7c140acb70> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f7c140ef080>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065400093 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0587       |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0031       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011188051 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0403      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00465     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076557905 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.049        |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00456      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064540775 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0418       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00438      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032833158 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0629       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00424      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041320445 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0491       |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00436      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055781305 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0335       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00399      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036110312 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0444       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0041       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.685      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 187        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00806571 |\n",
      "|    clip_fraction        | 0.351      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.881      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0456     |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00404    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059013306 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0433       |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00386      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014952391 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0563       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00379      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.686      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 186        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00583016 |\n",
      "|    clip_fraction        | 0.321      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.894      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0452     |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00363    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068862615 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0265       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00386      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010893014 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0552      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009802771 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0825      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00383     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 192          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076512485 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0592       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00356      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005053279 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00348     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 192          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077535273 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0702       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00391      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009983768 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00373     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.687     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 190       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 13        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0092116 |\n",
      "|    clip_fraction        | 0.365     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.896     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0409    |\n",
      "|    n_updates            | 1360      |\n",
      "|    policy_gradient_loss | -0.0297   |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.00363   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008580878 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0643      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00348     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007439351 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0569      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00345     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004522818 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0595      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00366     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005281174 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.687      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00402942 |\n",
      "|    clip_fraction        | 0.353      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.899      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0561     |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00361    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067441673 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0619       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00351      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033643246 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0767       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00358      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008299989 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0552      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00343     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004456776 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0704      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00349     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069779577 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.051        |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00342      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076191216 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0686       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00356      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004993701 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0531      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00337     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038785995 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0797       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00347      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075728535 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0491       |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010699287 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0472      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00338     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008190095 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007055402 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0623      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00351     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085377395 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0796       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00309      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034511506 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0618       |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00317      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049242647 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0746       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00321      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009721969 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0487      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00322     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006478071 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0827      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00343     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 185          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038086206 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0371       |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00322      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039487393 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.051        |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00303      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010807818 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00331     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007817453 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0705      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0033      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011436236 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0591      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00323     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006596011 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0521      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00323     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 185          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042210966 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.068        |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00325      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "seed 3: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f7bfc1d2dd8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f7bfc1d9208>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008315107 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0541      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00334     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 71 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010250369 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00537     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005962199 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0352      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005046657 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0409      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.005       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003758335 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.061       |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00481     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008915436 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.048       |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00492     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066800686 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0409       |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00464      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036913038 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0801       |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00507      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008907447 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0857      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00494     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035735904 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.065        |\n",
      "|    n_updates            | 2140         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00496      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010935461 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.075       |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0046      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012386319 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0758      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00482     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062915892 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0666       |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00463      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075620683 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0424       |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00469      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055493237 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0633       |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00491      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008507768 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0713      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00482     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007975864 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0537      |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00461     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028469623 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0646       |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00472      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.698      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 96         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00404554 |\n",
      "|    clip_fraction        | 0.355      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.861      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0698     |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0283    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00468    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031263381 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0561       |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00488      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036702692 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0638       |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00459      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028421432 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0586       |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0046       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027992695 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0496       |\n",
      "|    n_updates            | 2400         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00471      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046955915 |\n",
      "|    clip_fraction        | 0.397        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0497       |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.0333      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00463      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058136196 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0884       |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00447      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052307905 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0354       |\n",
      "|    n_updates            | 2460         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0044       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038508743 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0263       |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00456      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.699      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 96         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01064719 |\n",
      "|    clip_fraction        | 0.355      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.872      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0501     |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00445    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053767785 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0463       |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00422      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005900347 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0914      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0045      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010570526 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0931       |\n",
      "|    n_updates            | 2560         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00461      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007716328 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0462      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00412     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066993446 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0434       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00416      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008967993 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0515      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00429     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007268259 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0822      |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00437     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.699      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 95         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00613406 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.877      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.111      |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0309    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00425    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004684341 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0589      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00425     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075568645 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0531       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00438      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003156361 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0648      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00405     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008836037 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0715      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033142031 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0598       |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00426      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007234377 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0495      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00413     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.7          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042805136 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0524       |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00384      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043169707 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0534       |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00398      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041004657 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0456       |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0041       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075528054 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0448       |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00423      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.699      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 96         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00894781 |\n",
      "|    clip_fraction        | 0.377      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.881      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0733     |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0298    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00415    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007888198 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0519      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00404     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052112043 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0938       |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00424      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[0.25, 0.5, 1.0],\n",
    "                                episode_limit_array=[25000, 25000, 25000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
