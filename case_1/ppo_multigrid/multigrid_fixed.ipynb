{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_1_multigrid_fixed'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 3e-6,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.1,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[150,100,80], log_std_init=-2.9),\n",
    "                verbose = 1,\n",
    "                target_kl = 0.05,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f4787a58438> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f4787a3e978>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.59 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.594    |\n",
      "| time/              |          |\n",
      "|    fps             | 204      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.597       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015503636 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.236      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.113       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0926      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.598       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027700674 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.25       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.042       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.601      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 205        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03151442 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.308     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0617     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0244     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.603       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022731388 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0769      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0155      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.609      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 204        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02047947 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.504      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0564     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0271    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0115     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014205614 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0738      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00904     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014776167 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0347      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00822     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.611       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009310255 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0572      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0072      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.614       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008558616 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0711      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00705     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007448086 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0936      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00659     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009115359 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0507      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0066      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.621        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067782523 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0935       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0253      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00611      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.625        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037248284 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0557       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00591      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005490628 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0515      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00583     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.632        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065607997 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0635       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0258      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00553      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.637       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009056427 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0688      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00539     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009073767 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0319      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.644       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007067797 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0598      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00497     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.647        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052206665 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0394       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00499      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.647        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052545993 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0596       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00489      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004952848 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0499      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00463     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.652        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 204          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075270445 |\n",
      "|    clip_fraction        | 0.335        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0535       |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0253      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00485      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.654        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073877336 |\n",
      "|    clip_fraction        | 0.311        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0432       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0242      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00448      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.654        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060952753 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0674       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00436      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.655        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019960166 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0468       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00455      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.656        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052233576 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.037        |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00457      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.657       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006204811 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0678      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0042      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.658        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062398524 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0516       |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00407      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.66       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00753358 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.877      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0548     |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00408    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008065457 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.037       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00393     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008026439 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0636      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00392     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007908275 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0739      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00376     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.661      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00624949 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0381     |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00386    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008521202 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0341      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.004       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009663308 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00372     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005313185 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0708      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00371     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005771786 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0991      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00363     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005723181 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0959      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071375193 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0464       |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022626133 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0776       |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063076317 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0548       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039473595 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0501       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00365      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006813541 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0724      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005300158 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0597      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00355     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 210          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056967945 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0591       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00353      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030656694 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0258      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00336      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 207          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058521954 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.079        |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00344      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055377274 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.059        |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00339      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "seed 1: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f4787a3e978> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f4785e1d208>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042487443 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0587       |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00314      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013235169 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0625      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00494     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029845804 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0911       |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00463      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012733981 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0467      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00466     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066221626 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0442       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00462      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010792184 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0474      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00448     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054006665 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0709       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00438      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005490288 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0468      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005322394 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0361      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00426     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010062948 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0706      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00418     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.684      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00727576 |\n",
      "|    clip_fraction        | 0.351      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.061      |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00409    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010082498 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0385      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00413     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007220918 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0414      |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00403     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066599725 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0755       |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00416      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058379113 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0774       |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00383      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038633526 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0687       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00384      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.686      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00399805 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.894      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0317     |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0293    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00372    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006710267 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0924      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00401     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.687     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 166       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 15        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0048051 |\n",
      "|    clip_fraction        | 0.338     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.89      |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.059     |\n",
      "|    n_updates            | 1340      |\n",
      "|    policy_gradient_loss | -0.0282   |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.00379   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004210937 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0552      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00398     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008582848 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0342      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0039      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022564917 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0501       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00371      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006430435 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0794      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00386     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008837161 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0693      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00376     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075751273 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0499       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00379      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074083866 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0786       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00362      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057114484 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0432       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00369      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097735375 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0516       |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00383      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009282386 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0422      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049558403 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0715       |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071644066 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0614       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00406      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070396364 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0507       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00367      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039733974 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0446       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00371      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008873579 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0482      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011272711 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007997808 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0759      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00386     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056462646 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0395       |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00369      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066866367 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0548       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0036       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007119444 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0769      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067030936 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0447       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00371      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.688      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01186063 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.896      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0419     |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0302    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00359    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069877566 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.057        |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00364      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047917725 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0761       |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00364      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007830408 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0759      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00378     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.688         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 165           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 15            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020078421 |\n",
      "|    clip_fraction        | 0.356         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.895         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0659        |\n",
      "|    n_updates            | 1860          |\n",
      "|    policy_gradient_loss | -0.0295       |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.00369       |\n",
      "-------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059074285 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0682       |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0035       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008445928 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0604      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00377     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059617134 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0778       |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00327      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066706864 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0356       |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00363      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "seed 1: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f4788467e80> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f4780094240>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.695        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072880806 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0613       |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00396      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032159686 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0339       |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00593      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006565237 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0728      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00575     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004862058 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0424      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00574     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005222595 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0465      |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00502     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005816403 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00546     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061247884 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0319       |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00523      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008937535 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0438      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0055      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028803244 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0558       |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00559      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.696      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00765101 |\n",
      "|    clip_fraction        | 0.363      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.847      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.055      |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0305    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00523    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.696      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00904763 |\n",
      "|    clip_fraction        | 0.345      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.844      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0415     |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00534    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010735741 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.105       |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00545     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007850895 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0479      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00519     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 88           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051549436 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0661       |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00554      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005994126 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0414      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00523     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071587265 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.111        |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0051       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044819685 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0419       |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00518      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054695727 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0336       |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00521      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070357174 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0522       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00527      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008766135 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0603      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00512     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007749477 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0541      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00527     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054119765 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0549       |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.031       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00555      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007082778 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.045       |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008514458 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0337      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00534     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00422945 |\n",
      "|    clip_fraction        | 0.368      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.851      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0849     |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0306    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00499    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008816903 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0514      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00501     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063161403 |\n",
      "|    clip_fraction        | 0.374        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0572       |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.0318      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00521      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005670622 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0556      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00506     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013252854 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0577       |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00508      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009685421 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00491     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074727414 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0669       |\n",
      "|    n_updates            | 2560         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00506      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005939713 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.054       |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0048      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076387497 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0466       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.0321      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00495      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00851653 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.847      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0435     |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0298    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00501    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008705238 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0402      |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00475     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00908618 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.848      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0519     |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0302    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00501    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00651941 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.855      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0541     |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0313    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00502    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.697     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 84        |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 30        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0084981 |\n",
      "|    clip_fraction        | 0.37      |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.85      |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.051     |\n",
      "|    n_updates            | 2700      |\n",
      "|    policy_gradient_loss | -0.03     |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00491   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009117857 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0508      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00461     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006283088 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0491      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00485     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008846989 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0526      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00464     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008751487 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0632      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00469     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010006228 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0527      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00456     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005532959 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0447      |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00471     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008421913 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0713      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00447     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003987706 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0548      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00447     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011895707 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0752      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00459     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056338953 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0644       |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0044       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005953178 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0409      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00445     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f47540d3748> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f47540d3588>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009355739 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0558      |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00467     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032633513 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.476      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0641      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.604       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034693487 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.17       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0698      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0378      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.606      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 197        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04002426 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.465     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0603     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0229     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.607       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029365933 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0538      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0144      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.613       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027726144 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.505       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0259      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0101      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019373255 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0772      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00806     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015355974 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00722     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009740907 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00626     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.624       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009541893 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0371      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00609     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.629       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009406346 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0778      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00591     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009039444 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0502      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0058      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.633       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009475094 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0641      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00575     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.636        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063322126 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0413       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0246      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00512      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008937001 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0591      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00524     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009291222 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0626      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.64         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072204424 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0607       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00515      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011285245 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.065       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00474     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.646      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00805054 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.838      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0491     |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0052     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.648        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074118166 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0553       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00471      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.651        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 204          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068108765 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0654       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00486      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007865369 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0672      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00477     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.655       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009314701 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0887      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00469     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027735978 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.071        |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0244      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00459      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.659        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075268834 |\n",
      "|    clip_fraction        | 0.332        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0676       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0254      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00442      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055933953 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0719       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00455      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009925294 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.091       |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00459     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 213          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077251345 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0371       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0256      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00446      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.662       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004445684 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.056       |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00421     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.663        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050566616 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.059        |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00414      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007718733 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.051       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00416     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.663         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 209           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013876855 |\n",
      "|    clip_fraction        | 0.339         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.871         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0604        |\n",
      "|    n_updates            | 620           |\n",
      "|    policy_gradient_loss | -0.0272       |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.0042        |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.664        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064074188 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0431       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00417      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007404676 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0592      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00395     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008397022 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0344      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00409     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 217         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004732102 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0519      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006041235 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00368     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007111603 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0592      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054844827 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0877       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0266      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00377      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050089536 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0787       |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00383      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063805105 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0779       |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00371      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 215          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054797963 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.049        |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00352      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005155808 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0586      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00368     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005201557 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 212          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025882989 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.085        |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00358      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005924487 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008378643 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00372     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007880524 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0552      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004336402 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0759      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00344     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "seed 2: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f47540c8be0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f47540d3128>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 150          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064620106 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0518       |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00376      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013300797 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0718      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0053      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004888651 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0254      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00465     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033572495 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0523       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00455      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061625643 |\n",
      "|    clip_fraction        | 0.335        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0534       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00443      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042661964 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0599       |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00459      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005239102 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0044      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004542336 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0729      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00463     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004183191 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0978      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00455     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005896914 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0508      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007797292 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0508      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00424     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070629627 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0458       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00437      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004228103 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.037       |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00419     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007447398 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0557      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00429     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093474295 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0242       |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.0322      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00445      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057852594 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0368       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00411      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010791781 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0586      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00399     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056168735 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0618       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00425      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006389572 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00409     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008675342 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0416      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033726036 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0572       |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00392      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005786815 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0917      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00386     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029380382 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0664       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00381      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006341234 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0815      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075651556 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0507       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00371      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068704723 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0486       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0039       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009452152 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0602      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006155126 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0324      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.689      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00722568 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.889      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0549     |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00391    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005753809 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0721      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043509007 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0382       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00375      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047324626 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0523       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00366      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008118391 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0389       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0038       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008906734 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.059       |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00373     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055352836 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0477       |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046314686 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0401       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00356      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008612829 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0338      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00374     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011892244 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00376     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007993504 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0579      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00348     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065233232 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0831       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00351      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00640575 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.897      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0662     |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00354    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059281616 |\n",
      "|    clip_fraction        | 0.374        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0489       |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.032       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0035       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0117737055 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0462       |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0034       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008015585 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0621      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0035      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047370223 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.058        |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00341      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004389918 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.045       |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00356     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010839921 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0567      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0037      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009176376 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0403      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00354     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093998285 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0987       |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00354      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "seed 2: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f47300492e8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f471c345c50>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 80           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062811314 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0507       |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00359      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.697      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00537653 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.796      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0349     |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0307    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00604    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042171865 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.822        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0615       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0318      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00595      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063409745 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0603       |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00572      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063894736 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.1          |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00563      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010909587 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0461      |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00576     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008106291 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0561      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00584     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029004067 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0392       |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00564      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042289854 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0771       |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00555      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008689028 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0495      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00565     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008215824 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0608      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00579     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068319947 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.044        |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00607      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006840485 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0395      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00564     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006041533 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0572      |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00583     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006818029 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0318      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00582     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003670317 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.084       |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00539     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011060322 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0593      |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00563     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028697043 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0406       |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.0308      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00571      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010227209 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0466      |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00589     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009540188 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0469      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00554     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053830952 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0413       |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00561      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007284552 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0417      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00555     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009419146 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0603      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00546     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069137365 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0593       |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00545      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.696        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056864023 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0595       |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0054       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011614022 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0488      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00534     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048487186 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0527       |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00523      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004307085 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0418      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00542     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007374498 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0498      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00506     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063917814 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0884       |\n",
      "|    n_updates            | 2540         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00504      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008804267 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0363      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00548     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009306121 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0724      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00519     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053860904 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0702       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.0312      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00525      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006666401 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0634      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00483     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072228285 |\n",
      "|    clip_fraction        | 0.335        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0521       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00517      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008464724 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0546      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00553     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008595479 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0686      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00519     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046074153 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0571       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00536      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075814663 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0574       |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.005        |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011569673 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00506     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075137676 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0542       |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00524      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006597939 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0783      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00482     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010796532 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.051       |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00468     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033269108 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0542       |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00482      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008722732 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.855       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.049       |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00491     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009289485 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.093       |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00477     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052544116 |\n",
      "|    clip_fraction        | 0.374        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0777       |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.0315      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00482      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005920559 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0337       |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00495      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.697        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072075934 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0336       |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00501      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 15 x 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f471c30bfd0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f4787a55f28>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.599        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035937845 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0746       |\n",
      "|    n_updates            | 2940         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00503      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008611363 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0645      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0827      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.603       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036471147 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.39       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0906      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0336      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.605     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 206       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 12        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0380391 |\n",
      "|    clip_fraction        | 0.38      |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | -0.375    |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0909    |\n",
      "|    n_updates            | 60        |\n",
      "|    policy_gradient_loss | -0.0214   |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.0203    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.609      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 203        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03127714 |\n",
      "|    clip_fraction        | 0.378      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.335      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0802     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0126     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.609       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023681339 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.574       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0301      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00905     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.611     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 204       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 12        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0186771 |\n",
      "|    clip_fraction        | 0.343     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.725     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0562    |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | -0.023    |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00684   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.615       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014332309 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00616     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011970257 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0357      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00588     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.618      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 204        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00867331 |\n",
      "|    clip_fraction        | 0.345      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.83       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0539     |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00542    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.622        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112716975 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0608       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0231      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00508      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.622       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008531893 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0873      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00503     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.623        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 204          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048460127 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.848        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0378       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0259      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00503      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.624       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011932664 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00498     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.625        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 205          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0128508685 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0674       |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.0269      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0044       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.626       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005663827 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00438     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.629        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 203          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067400276 |\n",
      "|    clip_fraction        | 0.334        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0621       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0246      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00455      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.632       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012890858 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0638      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00404     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011975783 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00426     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005647713 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.05        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00416     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.64         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 199          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044877767 |\n",
      "|    clip_fraction        | 0.327        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0331       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0244      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00401      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009833497 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0757      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.641      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 201        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00793908 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0493     |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0264    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00382    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.643       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008342582 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0405      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.644        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075665624 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0725       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0253      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00406      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.646      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 205        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01218692 |\n",
      "|    clip_fraction        | 0.338      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0854     |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00384    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.647       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005613193 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0626      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.649        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057478277 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0617       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00356      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005415781 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0453      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00333     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.652       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003361559 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0661      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00346     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.655       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010581171 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0535      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00362     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.656       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005057818 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0447      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00353     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 203          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007322341 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0822       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00333      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007673341 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0606      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051002414 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0404       |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.0259      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00347      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.662        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 206          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038924187 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0474       |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0031       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.664       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002762404 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00334     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007065469 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.045       |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00329     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.665      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 202        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01055531 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.903      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0535     |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00318    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009018359 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0694      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011550871 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0637      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.666       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007347724 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0561      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00337     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063210875 |\n",
      "|    clip_fraction        | 0.329        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0419       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.024       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056840717 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0728       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00315      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 208          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063381107 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.031        |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00323      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023716062 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0624       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00326      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.669         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 207           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039167405 |\n",
      "|    clip_fraction        | 0.36          |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.912         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0605        |\n",
      "|    n_updates            | 920           |\n",
      "|    policy_gradient_loss | -0.0278       |\n",
      "|    std                  | 0.0551        |\n",
      "|    value_loss           | 0.003         |\n",
      "-------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 207         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008696845 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0382      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005884564 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.074       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "seed 3: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f4787a55f28> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f4787a8ff60>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006482959 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.682       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011201417 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0402      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00464     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069970964 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.052        |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00454      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073840143 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0424       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00435      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039914576 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0582       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00423      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004259902 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0424      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00435     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063505173 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0266       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00399      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039018423 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0463       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00408      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007733971 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0439      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00402     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006734091 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00385     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010906875 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0569       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00378      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054474473 |\n",
      "|    clip_fraction        | 0.323        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0533       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00361      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066095768 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0251       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00383      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011676857 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0602      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00377     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010357303 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.073       |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007724309 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0666      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00354     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048668953 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0555       |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00346      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008758736 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0712      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00392     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009891232 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0459      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00371     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008837387 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0461      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00362     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008392924 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.063       |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00347     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007732168 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0547      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00345     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004297477 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00365     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005652383 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0459      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031028837 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0542       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0036       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006304562 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0674      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0035      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003141269 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0774      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008647087 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0555      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00341     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004237035 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0627      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00347     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008108029 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0544      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00341     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007595992 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0707      |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00354     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005520618 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003638485 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0664      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00346     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006885207 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0417      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011226219 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0459      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00337     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009502405 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0507      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00335     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072713257 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0621       |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00351      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.688      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00898948 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.907      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0817     |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00309    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024910837 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0583       |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00318      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005313656 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0857      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009528692 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0458      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00323     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048367144 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0832       |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00343      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036969422 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0374       |\n",
      "|    n_updates            | 1820         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00323      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039370237 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.049        |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00303      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011394268 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0441      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00331     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008114967 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0734      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0033      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114900265 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0556       |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00323      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008062175 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0556      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00323     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034299463 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0736       |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00326      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "seed 3: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 61 x 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f4780070ef0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f471c2d4da0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008223429 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0526      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00335     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.698      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00883179 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.822      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0458     |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.03      |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00537    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056001423 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0443       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00515      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006197861 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.046       |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00501     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004389307 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0582      |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00482     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009696657 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.05        |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00494     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 87           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077125593 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.044        |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00466      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003905472 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0849      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0051      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010331136 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0821      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00495     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.698        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028462142 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0717       |\n",
      "|    n_updates            | 2140         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00498      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010266384 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0737      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00461     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012681889 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.074       |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00483     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006136638 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0478      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00466     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006443283 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0368      |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00472     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005615613 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0602      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00494     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008772266 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0632      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00483     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008444381 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0546      |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00463     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 85           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036572577 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0618       |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00474      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043646335 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0762       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00469      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027069629 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0561       |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00488      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034169883 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0693       |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00461      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003490959 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0607      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00464     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020981252 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0582       |\n",
      "|    n_updates            | 2400         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00475      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 57 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005505398 |\n",
      "|    clip_fraction        | 0.396       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0503      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0333     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00467     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056663128 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.087        |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00451      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 84           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051648794 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0328       |\n",
      "|    n_updates            | 2460         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00443      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030546368 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0294       |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00459      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010230621 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0441      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00449     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005934405 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00426     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004983711 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0852      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00452     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 5             |\n",
      "|    mean_reward          | 0.699         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 83            |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 30            |\n",
      "|    total_timesteps      | 2560          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -5.004704e-05 |\n",
      "|    clip_fraction        | 0.343         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | 91.8          |\n",
      "|    explained_variance   | 0.862         |\n",
      "|    learning_rate        | 3e-06         |\n",
      "|    loss                 | 0.0919        |\n",
      "|    n_updates            | 2560          |\n",
      "|    policy_gradient_loss | -0.0283       |\n",
      "|    std                  | 0.055         |\n",
      "|    value_loss           | 0.00461       |\n",
      "-------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.699       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007897762 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0415      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00414     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.7          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066292137 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0497       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00418      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0091600595 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0622       |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00429      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.699      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00694074 |\n",
      "|    clip_fraction        | 0.349      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.873      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0764     |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0287    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00437    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076199444 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.106        |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.0309      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00425      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.699        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 83           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047797235 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0598       |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00425      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 51 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008717743 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.046       |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00442     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003500852 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008554685 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0653      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0043      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004571265 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0628      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007886598 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0496      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00416     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.7          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034356625 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0535       |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00387      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 52 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004850045 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0574      |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00401     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 53 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.7          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 81           |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039544227 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0431       |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00413      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 81         |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 31         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00802418 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.871      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.052      |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00425    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009334207 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0749      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00416     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007986206 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0413      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00407     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004910135 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0907      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00426     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 59 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[0.25, 0.5, 1.0],\n",
    "                                episode_limit_array=[25000, 25000, 25000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
