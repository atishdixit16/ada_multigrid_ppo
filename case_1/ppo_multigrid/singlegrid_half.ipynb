{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_1_singlegrid_half'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 3e-6,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.1,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[150,100,80], log_std_init=-2.9),\n",
    "                verbose = 1,\n",
    "                target_kl = 0.05,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/coarse_grid_functions.py:51: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1mFirst-class function type feature is experimental\u001b[0m\u001b[0m\n",
      "  for j in range(len(p_1)-1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f3c2fddfcc0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f3c2fec18d0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.59 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.594    |\n",
      "| time/              |          |\n",
      "|    fps             | 83       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 61 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.598       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016035045 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.207      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.085       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0894      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030130435 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.33       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0993      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.039       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.603      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 192        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03296359 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.166     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0798     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0241     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.604       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023675045 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.287       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0908      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0162      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.612       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022041166 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.505       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0489      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.012       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.611       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015893986 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0865      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00965     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015791496 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.043       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00836     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.614       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011646261 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0383      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00771     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011162159 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0672      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00721     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.622       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009404319 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.109       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00706     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.625       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007467136 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0539      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00715     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008790458 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0804      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00663     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.634      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 189        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01035735 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.817      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0586     |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0287    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00625    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.639        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052428814 |\n",
      "|    clip_fraction        | 0.336        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0584       |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0062       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.642        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 192          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076252697 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0462       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00565      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.645        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 192          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073430836 |\n",
      "|    clip_fraction        | 0.324        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0658       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0252      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00564      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.645       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008712289 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0647      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00581     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.646       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007875329 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0992      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00524     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.648        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054706484 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0312       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00522      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006912166 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0438      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00515     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.651        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063952506 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0648       |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00492      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005735448 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0713      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00522     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.655       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008981374 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0461      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00489     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 192          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058068275 |\n",
      "|    clip_fraction        | 0.336        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.041        |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00469      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.658        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060798465 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0627       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00467      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.659        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074940263 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00463      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005507782 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0816      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00473     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005508578 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0699      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00452     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.663      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 187        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00891695 |\n",
      "|    clip_fraction        | 0.345      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.87       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0461     |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00438    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008073117 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.056       |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00443     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058835624 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0706       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00429      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071253986 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0406       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00416      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070369183 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0489       |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00427      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008548048 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0263      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00421     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009114778 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0753      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00456     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006087008 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0792      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00426     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007943654 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0438      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00372     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008518621 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0602      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00414     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010357025 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0556      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00413     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.674        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010160893 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0831       |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00409      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.674        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060708644 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0554       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00412      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006330845 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0484      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00386     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.676       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008082184 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0734      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005222586 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0681      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00383     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062951385 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0468       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00386      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006445399 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0386      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005395055 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0695      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006152821 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0996      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00378     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009069431 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0592      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00363     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 194          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062584938 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0582       |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00377      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050432296 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.104        |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00377      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053839744 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0434       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00396      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008165699 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.061       |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00362     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007838771 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0294      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005845758 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0469      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0036      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068367184 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0467       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00388      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059021264 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0337       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00381      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008614245 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0311      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00362     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.686      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 185        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01159499 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.897      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0282     |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00353    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071948855 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0496       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00348      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.686      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 187        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00716981 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.893      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0532     |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0311    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.0037     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023067445 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0323       |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008292317 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0832      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00349     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006540558 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0609      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00353     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006666851 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0808      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043562083 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.061        |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00337      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009074291 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0758      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00346     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045678793 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0343       |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00365      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010406044 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0738      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007927631 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0525      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 184          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066406904 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0467       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00355      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 174          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075164796 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.032        |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00361      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 177         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005177149 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0583      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00324     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 179         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009897148 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0743      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0035      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 178          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070150136 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0638       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0036       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 175         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009959003 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0274      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00348     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 173         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005710998 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0391      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00359     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.687      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 14         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00964888 |\n",
      "|    clip_fraction        | 0.377      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.898      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0584     |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.031     |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00345    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 175          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076519162 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0563       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00348      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 174         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010334715 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0508      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00364     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 172         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008625189 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00328     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007767263 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0555      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00339     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009277204 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0471      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004726964 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0371      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0034      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086839525 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0561       |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00329      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.688      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00644902 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.904      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0423     |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0277    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00336    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009598801 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0952      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00332     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008136829 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.045       |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00328     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009428081 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0338      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.688      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00718365 |\n",
      "|    clip_fraction        | 0.355      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.906      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0448     |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00324    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009517223 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0455      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00341     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009734911 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00351     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008090812 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0544      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0034      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008717599 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0401      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00323     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011101881 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0604      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00327     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008221805 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0555      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00323     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004807064 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00343     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047653764 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.049        |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00328      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008405268 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0818      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00335     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071199834 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0426       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00318      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069934726 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0696       |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071190177 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0459       |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00313      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054897876 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0411       |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00321      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007020208 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0501      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00318     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012097624 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0724      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00314     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008260027 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0799      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.689      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01105811 |\n",
      "|    clip_fraction        | 0.367      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.912      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.078      |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0301    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00313    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008020446 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0586      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076231537 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0584       |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00302      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009179848 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0564      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008171225 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0422      |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00334     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006313148 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0885      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00303     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.689      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00840348 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.903      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0632     |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.028     |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00319    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006466347 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0545      |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009997946 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0527      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00293     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074604005 |\n",
      "|    clip_fraction        | 0.382        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0708       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00308      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075083347 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0632       |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00302      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008917481 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0518      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00802367 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.915      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.05       |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00294    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010126556 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0791      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00305     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008247999 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0507      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006978071 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0568      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00306     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010830519 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0579      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075285197 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.043        |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00318      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012002262 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0647      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00296     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005860609 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0853      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00306     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006104082 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0448      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00294     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007662675 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0445      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00315     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051503656 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0783       |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00313      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057065515 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0623       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00322      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055766194 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.105        |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00302      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007286939 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0522      |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007211381 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0521      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043183924 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0734       |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037804246 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0639       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00284      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00841926 |\n",
      "|    clip_fraction        | 0.355      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.91       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0487     |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    std                  | 0.0549     |\n",
      "|    value_loss           | 0.00308    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073775603 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0352       |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00299      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004630047 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.05        |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00278     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008786231 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0625      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050347834 |\n",
      "|    clip_fraction        | 0.389        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0604       |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00317      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003948274 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0587      |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00305     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062009664 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0581       |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.0267      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00302      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004340604 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.076       |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00294     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007625359 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0913      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00308     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008399159 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0766      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00306     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 184          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060659973 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0356       |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f3c2fde8c50> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f3c2ff72ac8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.599       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005635646 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 60 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.598       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033992883 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.513      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0529      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0646      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.601       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039102152 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.991      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.083       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0373      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.605       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039867155 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.352      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0738      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0236      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030508399 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0735      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0146      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.615      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02791207 |\n",
      "|    clip_fraction        | 0.38       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.534      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0469     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.028     |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0101     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.617       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018966805 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0475      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00819     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016028723 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0646      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00749     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.623       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013901937 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.048       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00651     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.625       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012507698 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0495      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0061      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.629        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065912483 |\n",
      "|    clip_fraction        | 0.334        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.816        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.06         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.006        |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.631       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007509765 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0511      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00546     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.635        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073112966 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.831        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0385       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00567      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.637       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005474487 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0557      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00563     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009321695 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0707      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00545     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.642       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010751697 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0449      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00517     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.645        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075450866 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.084        |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00514      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.649       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008544767 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0414      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00502     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007238096 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0471      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00504     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007147989 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0396      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00476     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074676247 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0669       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00491      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011668593 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0609      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00482     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011805898 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0631      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00474     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.662        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037433535 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.082        |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00466      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008559877 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0705      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00467     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.666        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 156          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070061386 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0501       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00454      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011009756 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0813      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00457     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 156          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059667705 |\n",
      "|    clip_fraction        | 0.336        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0507       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00471      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001755175 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0591      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 152          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040223775 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.869        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0529       |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0044       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 155          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0098762335 |\n",
      "|    clip_fraction        | 0.326        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0429       |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00442      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 155          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034033328 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.077        |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00439      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 156          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037962527 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0597       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00461      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.672      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 155        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00754579 |\n",
      "|    clip_fraction        | 0.341      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.877      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0549     |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00419    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.674        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045154006 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0521       |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00435      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070444793 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0814       |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00414      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.676       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006697327 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0502      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00405     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057267966 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0696       |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00406      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061925678 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.88         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.094        |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00397      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058085946 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0877       |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00405      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007780501 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.047       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00396     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.678      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00706006 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.892      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.051      |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0302    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00372    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008004266 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0481      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00398     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.679        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041854917 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0508       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00397      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004392138 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.087       |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 156          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069758324 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0372       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00398      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063218563 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0673       |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00405      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007218316 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0605      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00388     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068244725 |\n",
      "|    clip_fraction        | 0.331        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.101        |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00385      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.682      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 155        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00791209 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.88       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0512     |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0307    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.0041     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.682       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010713911 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0769      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00397     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007900107 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0369      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00378     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007354787 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0404      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 156          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063572126 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0548       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00365      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053682686 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0476       |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00355      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.686     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 154       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 16        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0083164 |\n",
      "|    clip_fraction        | 0.344     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.896     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0697    |\n",
      "|    n_updates            | 1100      |\n",
      "|    policy_gradient_loss | -0.0291   |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.0037    |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.686      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 155        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00623084 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.89       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0273     |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0287    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00376    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072468845 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0372       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00376      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005871892 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0569      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075025917 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0501       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00358      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 155          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053595067 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0349       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00351      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077976407 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0691       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00354      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106135635 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0398       |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00339      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007906601 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0496      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00361     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029577166 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00338      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072225453 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.051        |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00361      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069551854 |\n",
      "|    clip_fraction        | 0.328        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0472       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00347      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010441581 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0479      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00345     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005968827 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0626      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00345     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007316458 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0365      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00342     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007549152 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0802      |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00353     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.688      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 153        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00516659 |\n",
      "|    clip_fraction        | 0.381      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.902      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0787     |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00347    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039580883 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0535       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00355      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070203366 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0544       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0303      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00314      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008859629 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0716      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00361     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008131349 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0572      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00345     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 155          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059063076 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0446       |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0033       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006260267 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0732      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00342     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007268235 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0461      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041957377 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0558       |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060477853 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0686       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00321      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050026984 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0557       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0032       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005444637 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0309      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0032      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018335789 |\n",
      "|    clip_fraction        | 0.38         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0743       |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00329      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009040391 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0439      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00305     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008445626 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0734      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00313     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 152          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045244517 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0554       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00317      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071133645 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0838       |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00319      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006537816 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0489      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00306     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070441305 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0287       |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00315      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007695538 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.035       |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0031      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007860279 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0928      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005157131 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00322     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 151          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071554543 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0666       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00307      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006777972 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0686      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008054575 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0332      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009165248 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0725      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00299     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009832022 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007127321 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0526      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00295     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041354597 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0599       |\n",
      "|    n_updates            | 1980         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00311      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 151          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060848384 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0398       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00302      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009434715 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0472      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044134543 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0529       |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00289      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 151          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053173723 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0491       |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008489278 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.049       |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007906536 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0648      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065662386 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0433       |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00289      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007844609 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.039       |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009904062 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0537      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008345356 |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0529      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057160556 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0434       |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006190172 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0398      |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.692      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 153        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00788745 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.915      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0619     |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00297    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 152          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054133562 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0838       |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00296      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 152          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083321575 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0796       |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00295      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010664066 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0529      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010833996 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0437      |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00281     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.692      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 152        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00957444 |\n",
      "|    clip_fraction        | 0.368      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.917      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0587     |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0283    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00291    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 151         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002739969 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0552      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00294     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 151         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005254969 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0834      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00289     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007850418 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0484      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081319185 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0544       |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00293      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027344017 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0907       |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00288      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010060447 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0627      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00312     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010365191 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0648      |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00298     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078107743 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0522       |\n",
      "|    n_updates            | 2500         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008242684 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0726      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010432178 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00288     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004474717 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.103       |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00285     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 152          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066423714 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0704       |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00295      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 152          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074077398 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0402       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00295      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 150          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067196847 |\n",
      "|    clip_fraction        | 0.386        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0589       |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00286      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070980997 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0668       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00296      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051864358 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0902       |\n",
      "|    n_updates            | 2660         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00282      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055027856 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0473       |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00305      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 150          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063857078 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0738       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00263      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 49 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 152          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077756853 |\n",
      "|    clip_fraction        | 0.385        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.069        |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00291      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071812035 |\n",
      "|    clip_fraction        | 0.389        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0447       |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00301      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 152          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075735776 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0536       |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00292      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001539734 |\n",
      "|    clip_fraction        | 0.391       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0718      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00286     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.691      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 150        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00558182 |\n",
      "|    clip_fraction        | 0.358      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.916      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0512     |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00299    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 151          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052030445 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0499       |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00279      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063536735 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0602       |\n",
      "|    n_updates            | 2840         |\n",
      "|    policy_gradient_loss | -0.0268      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0027       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009533415 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0491      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00286     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.691      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 154        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00817815 |\n",
      "|    clip_fraction        | 0.379      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.92       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0574     |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00278    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 148          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076143593 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0537       |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007542935 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0392      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00294     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f3c306b0828> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f3bcc042128>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.598       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008705894 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0629      |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00281     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 56 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.599       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009385499 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0855      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0881      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039988212 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.34       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0802      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.034       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.603      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 180        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 14         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03806997 |\n",
      "|    clip_fraction        | 0.369      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.312     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.087      |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.021      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.607       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 174         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031719904 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0578      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0131      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.609       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 177         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022306494 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0364      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00971     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.611      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 14         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01652424 |\n",
      "|    clip_fraction        | 0.342      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.711      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0536     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00724    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.614       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 177         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014663994 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00653     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009261787 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0546      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00627     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010711762 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00587     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.622       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 178         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011587948 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00538     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.623       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008246174 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.06        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00516     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.624       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007616815 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.055       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00511     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.629       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011376751 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00515     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.631       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013865143 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0608      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00501     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.632        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051175593 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0546       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00472      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.636        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 183          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057842196 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0518       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0255      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00475      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.639        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090448465 |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.861        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0759       |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00461      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.643        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067388653 |\n",
      "|    clip_fraction        | 0.335        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0398       |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00459      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.643        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070085553 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0508       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00433      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.646        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027611137 |\n",
      "|    clip_fraction        | 0.314        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0362       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00426      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.648        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062493654 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.063        |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00448      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.65         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077333627 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0503       |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00405      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.652       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008698732 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0346      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00414     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.654       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010271912 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0637      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00436     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.655       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011808878 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0622      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00406     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.656        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066550313 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0377       |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00392      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008083051 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0663      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00395     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.661        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 184          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067751454 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0355       |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.662        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 185          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074910102 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0592       |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00375      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010408929 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0503      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00403     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.665        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051616817 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.032        |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0259      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0037       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 192          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056271167 |\n",
      "|    clip_fraction        | 0.332        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.884        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0779       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00386      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.668        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064661563 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0406       |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00369      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051592467 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0482       |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.673    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 189      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 13       |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.00704  |\n",
      "|    clip_fraction        | 0.363    |\n",
      "|    clip_range           | 0.1      |\n",
      "|    entropy_loss         | 91.7     |\n",
      "|    explained_variance   | 0.894    |\n",
      "|    learning_rate        | 3e-06    |\n",
      "|    loss                 | 0.0662   |\n",
      "|    n_updates            | 700      |\n",
      "|    policy_gradient_loss | -0.0294  |\n",
      "|    std                  | 0.0551   |\n",
      "|    value_loss           | 0.00371  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 184          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047884495 |\n",
      "|    clip_fraction        | 0.327        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.882        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0507       |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0262      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0041       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.673       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007204461 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0587      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007917859 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0605      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00364     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008424568 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0746      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00386     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011052039 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0536      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00344     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.675        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070216656 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0381       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00377      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.676       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009117377 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009055749 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0416      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007952958 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.05        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050913543 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0456       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00362      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002830121 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0471      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0036      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070721386 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0573       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00365      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.682       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007625714 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0467      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00359     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.682       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006290567 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0579      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00364     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009373802 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0233      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00345     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072119264 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0407       |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00361      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004774141 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0689      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00365     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004944661 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0422      |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00344     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077002407 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.077        |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00346      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024148882 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0945       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00324      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072420924 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0613       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00339      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010185364 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0254      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0034      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048845797 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0631       |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00346      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004847282 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0694      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00339     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013214001 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0328      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00326     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074935197 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0594       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0034       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007240683 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0618      |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009327481 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0559      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0033      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023272038 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0685       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0033       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047665834 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0542       |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00336      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008773824 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0406      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0034      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061276914 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0534       |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00325      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010184365 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0936      |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00329     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 193          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052459417 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0617       |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00335      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063283863 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0679       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00323      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006997651 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0333      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00333     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043924274 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0645       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00319      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004987827 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0674      |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00304     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007457209 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0033      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009634805 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0593      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00323     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007846335 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0672      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00308     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010870499 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0368      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0031      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006124565 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0459      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00312     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008205777 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0766      |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00312     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068179844 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0744       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00326      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 192          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049948604 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0545       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00319      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005380717 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0637      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009756735 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0481      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00327     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006242612 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0889      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00308     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077309785 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0709       |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00309      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006120056 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0529      |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008570117 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0549      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068039806 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0646       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00307      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066914395 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0609       |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00286      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008041546 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0996      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007834425 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0537      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005799937 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0465      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00299     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062779575 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0497       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00301      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008510917 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0468      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00304     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009166056 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0336      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0091776755 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0525       |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00293      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106023345 |\n",
      "|    clip_fraction        | 0.382        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0514       |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00306      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072253672 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0658       |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0031       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.689      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 192        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00879853 |\n",
      "|    clip_fraction        | 0.359      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.911      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.057      |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.028     |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00301    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 186          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053869723 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0409       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0272      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00282      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007883579 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0545      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078047602 |\n",
      "|    clip_fraction        | 0.395        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0624       |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0324      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00277      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 188          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072445897 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.054        |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00286      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056258785 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0578       |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00278      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00881773 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.915      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0729     |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0289    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00283    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009941936 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0511      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00274     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011308658 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0483      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009463539 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0487      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00284     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011815915 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0943      |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0029      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 195          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073773386 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0496       |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00287      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044817505 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.049        |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00287      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006730479 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.081       |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00277     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070420178 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0949       |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00284      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 193          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061635016 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.058        |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00279      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 190          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049993633 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0484       |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00284      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050410987 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0423       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00281      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010632491 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0404      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00258     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045801164 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.043        |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00274      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007609056 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0544      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00267     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 193          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063899728 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.037        |\n",
      "|    n_updates            | 2400         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00278      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079586655 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0596       |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00263      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009427136 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0464      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00271     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011190179 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0427      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00278     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008514998 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.044       |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00261     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007784265 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0397      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0027      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010471359 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0582       |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0026       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 47 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009898713 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0438      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00266     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005468175 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.066       |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00264     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 190         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010572383 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0343      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0027      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010476625 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0832      |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00262     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.691      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 191        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01027213 |\n",
      "|    clip_fraction        | 0.354      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.923      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0786     |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00269    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.691     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 187       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 13        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0077162 |\n",
      "|    clip_fraction        | 0.376     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.924     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0503    |\n",
      "|    n_updates            | 2640      |\n",
      "|    policy_gradient_loss | -0.0291   |\n",
      "|    std                  | 0.055     |\n",
      "|    value_loss           | 0.00264   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004231253 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0869      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00247     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012509796 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0799      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0026      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076578767 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0567       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00265      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 191         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005894497 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0446      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00244     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.691      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 190        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00565902 |\n",
      "|    clip_fraction        | 0.363      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.932      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0479     |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.028     |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00239    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 192         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008369515 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0633      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00266     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022986382 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.93         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0606       |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00244      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060755997 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0485       |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0025       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007851884 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0437      |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00263     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 189         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009487862 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0706      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00244     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 194         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005566126 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 191          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062473593 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0602       |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00264      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006758988 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0508      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00257     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 196         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006172332 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[0.5],\n",
    "                                episode_limit_array=[75000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
