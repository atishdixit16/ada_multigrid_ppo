{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_1_singlegrid_half'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 3e-6,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.1,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[150,100,80], log_std_init=-2.9),\n",
    "                verbose = 1,\n",
    "                target_kl = 0.05,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fe01d2c2390> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fe01d2b5390>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.59 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.594    |\n",
      "| time/              |          |\n",
      "|    fps             | 155      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.598       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016803026 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.207      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0877      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0894      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030996263 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.34       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.1         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.039       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.602      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03370878 |\n",
      "|    clip_fraction        | 0.371      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.162     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0868     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0241     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.604       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023064896 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0935      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0163      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.612       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021543453 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0521      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0121      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.612       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016541803 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0774      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00968     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014627079 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.728       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0432      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00838     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.614       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009969029 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0364      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00772     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010554666 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0627      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00722     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.622       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008978307 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00709     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.626       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008035764 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0553      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00717     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.631       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009051713 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0783      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00665     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.635       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009970456 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0586      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00627     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.639       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005455467 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0492      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00622     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.641       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007556242 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0486      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00567     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.644       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008199622 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0596      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00566     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.645       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009105894 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0677      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00583     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.646       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008401829 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00527     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.648        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054484666 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0367       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00523      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008639827 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00516     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.651        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059804292 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.856        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0685       |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00494      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005499807 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.847       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0708      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00523     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.656       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009087536 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0469      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00491     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.658       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005467144 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00471     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.659        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050147446 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.864        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0687       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0047       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006875777 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0465      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00465     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.66         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053827465 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.862        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0793       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00475      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.662       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005890414 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0648      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00453     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009082454 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0541      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00439     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007817517 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0533      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00444     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058685867 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0714       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00429      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065790624 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0329       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00417      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007847264 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0452      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.669       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008670723 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.028       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00421     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008999726 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0692      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00457     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057612658 |\n",
      "|    clip_fraction        | 0.337        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0713       |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00427      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.671        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072865365 |\n",
      "|    clip_fraction        | 0.336        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0405       |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008328321 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0695      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00416     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.673      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01025064 |\n",
      "|    clip_fraction        | 0.328      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.879      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0539     |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00414    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007109821 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0779       |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00409      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.674        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049529015 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0641       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00412      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006799534 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0403      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007912284 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0696      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00408     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051248847 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0732       |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00385      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063152937 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0525       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00387      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007190123 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005237758 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0761      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00377     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007179135 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00379     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008797228 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.056       |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00364     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064452263 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0534       |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00378      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055233776 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0889       |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00379      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.682       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005249566 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0484      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00398     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008015486 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0637      |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00364     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.683      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00816072 |\n",
      "|    clip_fraction        | 0.342      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.89       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0326     |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00388    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055715293 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0626       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00361      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071579157 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0523       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00388      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056047114 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0319       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0313      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00381      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009446842 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0339      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00362     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010916975 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0276      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00353     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075134276 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0458       |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00349      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007084283 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0524      |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0037      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015480578 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0371       |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008864487 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0715      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00349     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071309865 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0585       |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00353      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 169          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067436337 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0719       |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00352      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041047363 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0633       |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00338      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010263202 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0829      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00346     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041454495 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0327       |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.0314      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00365      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010463121 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0634      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 170          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070177065 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0537       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00334      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006610745 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0564      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00355     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075538666 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0308       |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00362      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.687      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00449155 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.908      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0573     |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0296    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00325    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0084825065 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0744       |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0035       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060604243 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0658       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00359      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010854307 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0358      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00348     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055154203 |\n",
      "|    clip_fraction        | 0.319        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0382       |\n",
      "|    n_updates            | 1540         |\n",
      "|    policy_gradient_loss | -0.0261      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00359      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008584869 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0521      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00346     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008022455 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0488      |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00349     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102783535 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0505       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00364      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008302296 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0566      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00328     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006988445 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0539      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00339     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009589207 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0369      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040266276 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0385       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0034       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008316132 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0514      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00329     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057223677 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0468       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00337      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009810698 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0867      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00334     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008507895 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0478      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0033      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010451761 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.034       |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008141011 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0433      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00327     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009488681 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0489      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00344     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008862635 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0741      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00353     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006983665 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0566      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0034      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008904633 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0338      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00324     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108245285 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0615       |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.0298      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00328      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008343494 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0607      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00323     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050486354 |\n",
      "|    clip_fraction        | 0.384        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0394       |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | -0.0305      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00344      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005420506 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00329     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008473677 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0801      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073096813 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0398       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00317      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073794215 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0631       |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00321      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074748425 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0446       |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00314      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066852598 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0437       |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00324      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064679384 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0442       |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00319      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011815962 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0677      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008054172 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0814      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011471003 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0794      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00313     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007898259 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0615      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0032      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070589036 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0611       |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00303      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009043458 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0544      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00303     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008290723 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0483      |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00337     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005685991 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0949      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00306     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009032726 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0532      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00322     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061057033 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0501       |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00318      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009379407 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0541      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00295     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006683427 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0843      |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006728542 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0666      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00305     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094510075 |\n",
      "|    clip_fraction        | 0.383        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0537       |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00319      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008417746 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0595      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009927022 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0783      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008233341 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0415      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0032      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007237786 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0511      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 168          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110478345 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0464       |\n",
      "|    n_updates            | 2460         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00325      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007847336 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00322     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011325559 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0596      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.689      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00611757 |\n",
      "|    clip_fraction        | 0.362      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.909      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0892     |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00311    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065849214 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0532       |\n",
      "|    n_updates            | 2540         |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00299      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007988071 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004904789 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0814      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066883117 |\n",
      "|    clip_fraction        | 0.37         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0561       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00327      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006330782 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00307     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066272914 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0455       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00314      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006649864 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0454      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00321     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043940367 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0767       |\n",
      "|    n_updates            | 2680         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00323      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046520173 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0615       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00288      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009199038 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0477      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00312     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007346821 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0408      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045381696 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.055        |\n",
      "|    n_updates            | 2760         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00282      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009101856 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0593      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00307     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003890428 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0599      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00322     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042462884 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.061        |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00308      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005940932 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0653      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00307     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004680523 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0859      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00299     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007322663 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0883      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00312     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00780427 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.9       |\n",
      "|    explained_variance   | 0.909      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0753     |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.0292    |\n",
      "|    std                  | 0.0549     |\n",
      "|    value_loss           | 0.00312    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007460979 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.9        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0371      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.0549      |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fe0a9644c88> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fdfc41cbb00>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.599        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 153          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056955516 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 92           |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0546       |\n",
      "|    n_updates            | 2940         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0549       |\n",
      "|    value_loss           | 0.00315      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.598       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033991955 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.513      |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0529      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0646      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.601      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03962434 |\n",
      "|    clip_fraction        | 0.351      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.991     |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0826     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0374     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.605       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040520255 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.35       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0721      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0236      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.609       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030850654 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0759      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0146      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.614       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027425464 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0454      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0101      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.617       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018441848 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0571      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00819     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.619       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015783833 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0675      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00749     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.622       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014155095 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0533      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00651     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.624       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012528625 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0562      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0061      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.628       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007151353 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0614      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.006       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007763681 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0444      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00546     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.635       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007896396 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0451      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00567     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.637       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004859954 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0472      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00564     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010066149 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0688      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00546     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.642       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010632658 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0452      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00516     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.644      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00782196 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.844      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.072      |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0277    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00515    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.648        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077969343 |\n",
      "|    clip_fraction        | 0.334        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0439       |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00501      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006685528 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0498      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00504     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.653        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076791225 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0373       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00477      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.657        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075906785 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0738       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0049       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.659       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010073671 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0548      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00482     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011431021 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0717      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00473     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.663        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044117747 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.865        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0846       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00465      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009256092 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0694      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00466     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.667        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070581315 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0476       |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00453      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011180023 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0735      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00457     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006856459 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0513      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00471     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014162541 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0604       |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00441      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004188272 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0044      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009542632 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0405      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.672        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012222051 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0671       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00438      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.672       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004264629 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0578      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00461     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.673        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067319544 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0511       |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00418      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004320788 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0556      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00434     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073691844 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0846       |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00414      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.677       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007237649 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0435      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00404     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073858975 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0578       |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00405      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.678      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00740726 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.88       |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0897     |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0287    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00396    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005405155 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0859      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00404     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079580275 |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0515       |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00396      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064680367 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0493       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00373      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008603844 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00398     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004576558 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0466      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00397     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041801212 |\n",
      "|    clip_fraction        | 0.345        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0866       |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.0288      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0037       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.681       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006448522 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0397      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00398     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.682      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00599035 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0665     |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00405    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.682      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00751515 |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.889      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0594     |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0284    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00389    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075295717 |\n",
      "|    clip_fraction        | 0.331        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.108        |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0276      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00385      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008421344 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0634      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0041      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010558302 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0777      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00397     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007814819 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0339      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00378     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006689301 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0404      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007440853 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.047       |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00363     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005460128 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0453      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00354     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008234212 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0697      |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069987504 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0314       |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067128926 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0436       |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00375      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055579483 |\n",
      "|    clip_fraction        | 0.383        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0533       |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.0324      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00368      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070384117 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0539       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00357      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060833814 |\n",
      "|    clip_fraction        | 0.341        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.04         |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0035       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0084339855 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0708       |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00352      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010849339 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.04        |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00338     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008826586 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0539      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0036      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004442832 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0404      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00336     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007559827 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00358     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007144466 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0462      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00345     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009701518 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0444      |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00343     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062642186 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0585       |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00342      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006356376 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0452      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00339     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073248744 |\n",
      "|    clip_fraction        | 0.347        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0791       |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00349      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050670775 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0891       |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00345      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004542035 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0585      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00352     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060331435 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0496       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0304      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00311      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009340662 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0697      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075024636 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0588       |\n",
      "|    n_updates            | 1500         |\n",
      "|    policy_gradient_loss | -0.0311      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00342      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045263497 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0436       |\n",
      "|    n_updates            | 1520         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00328      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006543943 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.081       |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00338     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060239015 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0489       |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00315      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003971553 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0537      |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006269944 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0577      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00318     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042375624 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0558       |\n",
      "|    n_updates            | 1620         |\n",
      "|    policy_gradient_loss | -0.0278      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00317      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004748568 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0332      |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00318     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024525612 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.075        |\n",
      "|    n_updates            | 1660         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00325      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009074676 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0493      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008517057 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.064       |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003717193 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0535      |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066898614 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.91         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0757       |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00317      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.691      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00587464 |\n",
      "|    clip_fraction        | 0.356      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.914      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0498     |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00304    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005798912 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0267      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00312     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071712197 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0396       |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00307      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008506027 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0909      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004633361 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0615      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006648469 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0728      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00305     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071558207 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0654       |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.0291      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00298      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008933062 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0343      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00315     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008578593 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0625      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00297     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.692      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00914346 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.909      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0528     |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0302    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00316    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070717754 |\n",
      "|    clip_fraction        | 0.371        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0554       |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.029       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00291      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004479012 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0706      |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0031      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005840808 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0415      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008696052 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0577      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004799126 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0499      |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00288     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006040439 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0608      |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00295     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008962834 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0569      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069997846 |\n",
      "|    clip_fraction        | 0.384        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0692       |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.003        |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006242998 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0477      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00286     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008269492 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0377      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00295     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009658292 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0538      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00312     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075903595 |\n",
      "|    clip_fraction        | 0.4          |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0503       |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.032       |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005832988 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0503      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00293     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059761764 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0424       |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00298      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077198297 |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0758       |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.028       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00295      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054971664 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0793       |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00293      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008471593 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0758      |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00291     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.692      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01088281 |\n",
      "|    clip_fraction        | 0.374      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.917      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0485     |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00289    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011417726 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0424      |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00279     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010736671 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0632      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00289     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023413568 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.053        |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00291      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006200594 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0932      |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00285     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008388477 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.044       |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00294     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008032793 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0518      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0029      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034567192 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0854       |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00285      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010357541 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0605      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0031      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009244415 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0681      |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00296     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008708293 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0532      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00295     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008131954 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0705      |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00298     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104865525 |\n",
      "|    clip_fraction        | 0.356        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0594       |\n",
      "|    n_updates            | 2540         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00284      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003883162 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0923      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065984367 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.065        |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00293      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075446516 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0407       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00293      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072214245 |\n",
      "|    clip_fraction        | 0.388        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0525       |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00284      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073895813 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0648       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00294      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005087936 |\n",
      "|    clip_fraction        | 0.369       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0879      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00281     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006121956 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0463      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056426884 |\n",
      "|    clip_fraction        | 0.378        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.928        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0705       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.0026       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007713559 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0687      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00289     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006652531 |\n",
      "|    clip_fraction        | 0.388       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0419      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006347948 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0544      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0029      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001743421 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0725      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043819873 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0593       |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.0281      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053596767 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0502       |\n",
      "|    n_updates            | 2820         |\n",
      "|    policy_gradient_loss | -0.0264      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00275      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.691      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00765045 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.923      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0614     |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00268    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009736663 |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0503      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00284     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006607893 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0566      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00277     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068137227 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0551       |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00295      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072083445 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0435       |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00291      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 30 x 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fe01ef19be0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fdfc4028d68>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.598      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 156        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00878156 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.923      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0729     |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0301    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00279    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.599      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00938569 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.166      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0855     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0881     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.599       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040564656 |\n",
      "|    clip_fraction        | 0.384       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.34       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0815      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.034       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.602      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03725143 |\n",
      "|    clip_fraction        | 0.368      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | -0.31      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0887     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.021      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.605      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03179601 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.317      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0602     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.024     |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.0131     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.608       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021949966 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0408      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00971     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016829487 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0516      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00724     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.613       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015270892 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0526      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00653     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.614       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009046296 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0545      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00627     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011290813 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00585     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.619       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011750823 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0713      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00538     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.621       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007967254 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0652      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00515     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.621       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007945818 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0617      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0051      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.627       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011406526 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0546      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00514     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.629       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013657275 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0743      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.005       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.631        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050155846 |\n",
      "|    clip_fraction        | 0.342        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0464       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00472      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.635        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056693642 |\n",
      "|    clip_fraction        | 0.328        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0392       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0255      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00475      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.638       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008537253 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0797      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0046      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.641       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007035175 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0372      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0046      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.642        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061168983 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.872        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0406       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00433      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.645        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026560784 |\n",
      "|    clip_fraction        | 0.315        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0365       |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00426      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.646        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067916783 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0579       |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00448      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.649       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007924998 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0441      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00404     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.651       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008407494 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0373      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00414     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.653       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010885393 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0704      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00435     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.65 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.654       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011946616 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0657      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00405     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.655       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006534934 |\n",
      "|    clip_fraction        | 0.342       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0354      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0039      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.658       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008085394 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.06        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00393     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006920296 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0356      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00372     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.661       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007868591 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0608      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00374     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.66 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.663      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01133658 |\n",
      "|    clip_fraction        | 0.337      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0494     |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00401    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005151293 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0321      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0037      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.668       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005089405 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0706      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00385     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.669        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057280213 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0415       |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00368      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.671       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005456579 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0496      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00372     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.673       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007902267 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0608      |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0037      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.674     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 162       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 15        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0041612 |\n",
      "|    clip_fraction        | 0.328     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.7      |\n",
      "|    explained_variance   | 0.883     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0508    |\n",
      "|    n_updates            | 720       |\n",
      "|    policy_gradient_loss | -0.026    |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.00409   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007487816 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0563      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.674       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007898711 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0624      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00363     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008316842 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.077       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00384     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.675       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010316816 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0546      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00342     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.676        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076738866 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.04         |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00375      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.677        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0084224995 |\n",
      "|    clip_fraction        | 0.33         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0466       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0262      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00355      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.678        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083542075 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0442       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00367      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009164432 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0543      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00381     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.68         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067739068 |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0459       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0036       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.681        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027476072 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0518       |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.0297      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00359      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053629936 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0559       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00363      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.682        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073146373 |\n",
      "|    clip_fraction        | 0.349        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0468       |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00356      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.682       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006306401 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0603      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00362     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009264648 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.03        |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00342     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.683        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072136996 |\n",
      "|    clip_fraction        | 0.357        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.894        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0374       |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00361      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055508553 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0745       |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00363      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051284702 |\n",
      "|    clip_fraction        | 0.338        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0408       |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.0266      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00341      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006721428 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0842      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00344     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.684        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028899491 |\n",
      "|    clip_fraction        | 0.344        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0988       |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00321      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006783375 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0607      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00337     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008846214 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.029       |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00338     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042642085 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0653       |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00343      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040789125 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0777       |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.03        |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00337      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012084598 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0342      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00323     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008049568 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0614      |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00338     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.685        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072203847 |\n",
      "|    clip_fraction        | 0.36         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0638       |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00314      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.685       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010715815 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0454      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00328     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002202645 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0687      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00327     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.686        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050019682 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0477       |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -0.0286      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00334      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010173082 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0413      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00338     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058772787 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.908        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0575       |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00324      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010428691 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00328     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048116473 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0555       |\n",
      "|    n_updates            | 1380         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00334      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.687      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00703319 |\n",
      "|    clip_fraction        | 0.339      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.7       |\n",
      "|    explained_variance   | 0.907      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0723     |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00323    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007377252 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0334      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00332     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004782924 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0621      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00318     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.687     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 163       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 15        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0054497 |\n",
      "|    clip_fraction        | 0.344     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | 91.8      |\n",
      "|    explained_variance   | 0.912     |\n",
      "|    learning_rate        | 3e-06     |\n",
      "|    loss                 | 0.0618    |\n",
      "|    n_updates            | 1460      |\n",
      "|    policy_gradient_loss | -0.0274   |\n",
      "|    std                  | 0.0551    |\n",
      "|    value_loss           | 0.00304   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077263685 |\n",
      "|    clip_fraction        | 0.368        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.126        |\n",
      "|    n_updates            | 1480         |\n",
      "|    policy_gradient_loss | -0.0295      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.0033       |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008969625 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0614      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00322     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008002755 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0575      |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009823227 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0405      |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.0031      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005680138 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0483      |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00312     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007911322 |\n",
      "|    clip_fraction        | 0.37        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0799      |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00311     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.687        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067678364 |\n",
      "|    clip_fraction        | 0.339        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0659       |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -0.0273      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00325      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005172178 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0475      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00318     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053661885 |\n",
      "|    clip_fraction        | 0.348        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0596       |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.0279      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00309      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009149417 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.7        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0462      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00326     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055365534 |\n",
      "|    clip_fraction        | 0.364        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0927       |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.0302      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00308      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069871126 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0699       |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00309      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067921905 |\n",
      "|    clip_fraction        | 0.35         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.7         |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0582       |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00297      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.688       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009735489 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.003       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.688        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068669706 |\n",
      "|    clip_fraction        | 0.361        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0692       |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -0.0292      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00306      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 159          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069392594 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0501       |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00285      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008505667 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0946      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008549106 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0473      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052511664 |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0465       |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00298      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.689        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073887734 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0514       |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.003        |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008524132 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0406      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00305     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00982978 |\n",
      "|    clip_fraction        | 0.375      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.915      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.0355     |\n",
      "|    n_updates            | 1900       |\n",
      "|    policy_gradient_loss | -0.03      |\n",
      "|    std                  | 0.0551     |\n",
      "|    value_loss           | 0.00293    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065716268 |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0488       |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00293      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011022079 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0505      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00306     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008006376 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0729      |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009101972 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.057       |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055632116 |\n",
      "|    clip_fraction        | 0.351        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0464       |\n",
      "|    n_updates            | 2000         |\n",
      "|    policy_gradient_loss | -0.0271      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00282      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008211908 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0504      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00285     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008265728 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0501      |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    std                  | 0.0551      |\n",
      "|    value_loss           | 0.00278     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073177693 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0594       |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00285      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045992597 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0599       |\n",
      "|    n_updates            | 2080         |\n",
      "|    policy_gradient_loss | -0.0294      |\n",
      "|    std                  | 0.0551       |\n",
      "|    value_loss           | 0.00277      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009695053 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0737      |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008314902 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0519      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00274     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009862152 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0461      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009240219 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0387      |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00284     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01096355 |\n",
      "|    clip_fraction        | 0.365      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | 91.8       |\n",
      "|    explained_variance   | 0.917      |\n",
      "|    learning_rate        | 3e-06      |\n",
      "|    loss                 | 0.095      |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    std                  | 0.055      |\n",
      "|    value_loss           | 0.00288    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.69         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067667007 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0505       |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00287      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035345673 |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0519       |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -0.0263      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00286      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060405345 |\n",
      "|    clip_fraction        | 0.358        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0742       |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00276      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007529831 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0952      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059761973 |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0527       |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00278      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005369869 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0513      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00283     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048601716 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0447       |\n",
      "|    n_updates            | 2320         |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00281      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009839356 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.042       |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052018585 |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.92         |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0427       |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00276      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083000045 |\n",
      "|    clip_fraction        | 0.388        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0487       |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00267      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006696117 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0373      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00277     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008204127 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0536      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00262     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009174967 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0397      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00273     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011737829 |\n",
      "|    clip_fraction        | 0.382       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0456      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00277     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008115411 |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.045       |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0026      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006860909 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00269     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008258835 |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0548       |\n",
      "|    n_updates            | 2520         |\n",
      "|    policy_gradient_loss | -0.0274      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00261      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009765697 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0453      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00266     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005603215 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0681      |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00265     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010293657 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.035       |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00271     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108747985 |\n",
      "|    clip_fraction        | 0.365        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0881       |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -0.027       |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00262      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 164          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093974145 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0792       |\n",
      "|    n_updates            | 2620         |\n",
      "|    policy_gradient_loss | -0.0265      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00269      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 166          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070141554 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0499       |\n",
      "|    n_updates            | 2640         |\n",
      "|    policy_gradient_loss | -0.0293      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00264      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004707915 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0858      |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00247     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012348598 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0863      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00259     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066858083 |\n",
      "|    clip_fraction        | 0.373        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0642       |\n",
      "|    n_updates            | 2700         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00264      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065349163 |\n",
      "|    clip_fraction        | 0.362        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.9         |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0485       |\n",
      "|    n_updates            | 2720         |\n",
      "|    policy_gradient_loss | -0.0277      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00244      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057422193 |\n",
      "|    clip_fraction        | 0.363        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.932        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0517       |\n",
      "|    n_updates            | 2740         |\n",
      "|    policy_gradient_loss | -0.0283      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00239      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007632652 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0645      |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00266     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 163          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018925488 |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0562       |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.0275      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00244      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.691        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052368967 |\n",
      "|    clip_fraction        | 0.381        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0439       |\n",
      "|    n_updates            | 2800         |\n",
      "|    policy_gradient_loss | -0.0289      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00251      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007307255 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0456      |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00262     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009983927 |\n",
      "|    clip_fraction        | 0.36        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 3e-06       |\n",
      "|    loss                 | 0.0644      |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.00245     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 161          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060747536 |\n",
      "|    clip_fraction        | 0.376        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.928        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0588       |\n",
      "|    n_updates            | 2860         |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00256      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 160          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070709335 |\n",
      "|    clip_fraction        | 0.379        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0609       |\n",
      "|    n_updates            | 2880         |\n",
      "|    policy_gradient_loss | -0.0287      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00266      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 165          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070689083 |\n",
      "|    clip_fraction        | 0.377        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.0504       |\n",
      "|    n_updates            | 2900         |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00257      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5            |\n",
      "|    mean_reward          | 0.692        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 162          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2560         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073122084 |\n",
      "|    clip_fraction        | 0.375        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | 91.8         |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 3e-06        |\n",
      "|    loss                 | 0.102        |\n",
      "|    n_updates            | 2920         |\n",
      "|    policy_gradient_loss | -0.0282      |\n",
      "|    std                  | 0.055        |\n",
      "|    value_loss           | 0.00257      |\n",
      "------------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[0.5],\n",
    "                                episode_limit_array=[75000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
